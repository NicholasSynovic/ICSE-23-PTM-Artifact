{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69853\n"
     ]
    }
   ],
   "source": [
    "dic = {'None': 56933,\n",
    " 'common_voice': 328,\n",
    " 'conll2003': 580,\n",
    " 'emotion': 477,\n",
    " 'glue': 355,\n",
    " 'mozilla-foundation/common_voice_7_0': 218,\n",
    " 'mozilla-foundation/common_voice_8_0': 344,\n",
    " 'other': 3263,\n",
    " 'speech-recognition-community-v2/dev_data': 340,\n",
    " 'universal_dependencies': 6834,\n",
    " 'xtreme': 181}\n",
    "\n",
    "count = 0\n",
    "for name in dic:\n",
    "    count += dic[name]\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"modelinfo.json\")\n",
    "df.to_csv(\"modelinfo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ndjson\n",
      "  Using cached ndjson-0.3.1-py2.py3-none-any.whl (5.3 kB)\n",
      "Installing collected packages: ndjson\n",
      "Successfully installed ndjson-0.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ndjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ndjson\n",
    "\n",
    "with open(\"modelinfo.json\", \"r\") as file:\n",
    "\n",
    "    data = json.load(file)\n",
    "    output = ndjson.dumps(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('modelinfo.ndjson', 'w') as f:\n",
    "    ndjson.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../datamodelinfo.json\", \"r\") as file:\n",
    "    # with open(\"./reproducibility/results.json\", \"r\") as file:\n",
    "        data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {m[\"id\"]: m for m in data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert-base-uncased': {'modelId': 'bert-base-uncased',\n",
       "  'sha': '418430c3b5df7ace92f2aede75700d22c78a0f95',\n",
       "  'lastModified': '2022-06-06T11:41:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-base-uncased',\n",
       "  'downloads': 21607939,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 183,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/chinese-macbert-base': {'modelId': 'hfl/chinese-macbert-base',\n",
       "  'sha': 'a986e004d2a7f2a1c2f5a3edef4e20604a974ed1',\n",
       "  'lastModified': '2021-05-19T19:09:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'hfl/chinese-macbert-base',\n",
       "  'downloads': 11988756,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '巴黎是[MASK]国的首都。'}, {'text': '生活的真谛是[MASK]。'}],\n",
       "  'likes': 20,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'tags': ['bert'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'distilbert-base-uncased': {'modelId': 'distilbert-base-uncased',\n",
       "  'sha': '043235d6088ecd3dd5fb5ca3592b6913fd516027',\n",
       "  'lastModified': '2022-05-31T19:08:36.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'distilbert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1910.01108',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['DistilBertForMaskedLM'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'distilbert-base-uncased',\n",
       "  'downloads': 9046961,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 63,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'gpt2': {'modelId': 'gpt2',\n",
       "  'sha': '6c0e6080953db56375760c0471a8c5f2929baf11',\n",
       "  'lastModified': '2021-05-19T16:25:59.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'tflite',\n",
       "   'rust',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'gpt2',\n",
       "  'downloads': 9042237,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 141,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'tags': ['exbert'], 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'distilbert-base-uncased-finetuned-sst-2-english': {'modelId': 'distilbert-base-uncased-finetuned-sst-2-english',\n",
       "  'sha': '83dbcb26ba08b01dd9dc2ce4eac7670b5c69e255',\n",
       "  'lastModified': '2022-06-27T19:44:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rust',\n",
       "   'distilbert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:sst2',\n",
       "   'dataset:glue',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['DistilBertForSequenceClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'distilbert-base-uncased-finetuned-sst-2-english',\n",
       "  'downloads': 8359004,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 69,\n",
       "  'model-index': [{'name': 'distilbert-base-uncased-finetuned-sst-2-english',\n",
       "    'results': [{'task': {'type': 'text-classification',\n",
       "       'name': 'Text Classification'},\n",
       "      'dataset': {'name': 'glue',\n",
       "       'type': 'glue',\n",
       "       'config': 'sst2',\n",
       "       'split': 'validation'},\n",
       "      'metrics': [{'name': 'Accuracy',\n",
       "        'type': 'accuracy',\n",
       "        'value': 0.9105504587155964,\n",
       "        'verified': True},\n",
       "       {'name': 'Precision',\n",
       "        'type': 'precision',\n",
       "        'value': 0.8978260869565218,\n",
       "        'verified': True},\n",
       "       {'name': 'Recall',\n",
       "        'type': 'recall',\n",
       "        'value': 0.9301801801801802,\n",
       "        'verified': True},\n",
       "       {'name': 'AUC',\n",
       "        'type': 'auc',\n",
       "        'value': 0.9716626673402374,\n",
       "        'verified': True},\n",
       "       {'name': 'F1',\n",
       "        'type': 'f1',\n",
       "        'value': 0.9137168141592922,\n",
       "        'verified': True},\n",
       "       {'name': 'loss',\n",
       "        'type': 'loss',\n",
       "        'value': 0.39013850688934326,\n",
       "        'verified': True}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['sst2', 'glue'],\n",
       "   'model-index': [{'name': 'distilbert-base-uncased-finetuned-sst-2-english',\n",
       "     'results': [{'task': {'type': 'text-classification',\n",
       "        'name': 'Text Classification'},\n",
       "       'dataset': {'name': 'glue',\n",
       "        'type': 'glue',\n",
       "        'config': 'sst2',\n",
       "        'split': 'validation'},\n",
       "       'metrics': [{'name': 'Accuracy',\n",
       "         'type': 'accuracy',\n",
       "         'value': 0.9105504587155964,\n",
       "         'verified': True},\n",
       "        {'name': 'Precision',\n",
       "         'type': 'precision',\n",
       "         'value': 0.8978260869565218,\n",
       "         'verified': True},\n",
       "        {'name': 'Recall',\n",
       "         'type': 'recall',\n",
       "         'value': 0.9301801801801802,\n",
       "         'verified': True},\n",
       "        {'name': 'AUC',\n",
       "         'type': 'auc',\n",
       "         'value': 0.9716626673402374,\n",
       "         'verified': True},\n",
       "        {'name': 'F1',\n",
       "         'type': 'f1',\n",
       "         'value': 0.9137168141592922,\n",
       "         'verified': True},\n",
       "        {'name': 'loss',\n",
       "         'type': 'loss',\n",
       "         'value': 0.39013850688934326,\n",
       "         'verified': True}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'distilgpt2': {'modelId': 'distilgpt2',\n",
       "  'sha': 'b754fe8f9410238372bd6adc0a6c17282c4bb9c0',\n",
       "  'lastModified': '2022-06-02T08:10:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'tflite',\n",
       "   'rust',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'dataset:openwebtext',\n",
       "   'arxiv:1910.01108',\n",
       "   'arxiv:2201.08542',\n",
       "   'arxiv:2203.12574',\n",
       "   'arxiv:1910.09700',\n",
       "   'arxiv:1503.02531',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:apache-2.0',\n",
       "   'model-index',\n",
       "   'co2_eq_emissions'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'distilgpt2',\n",
       "  'downloads': 5800175,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 73,\n",
       "  'model-index': [{'name': 'distilgpt2',\n",
       "    'results': [{'task': {'type': 'text-generation',\n",
       "       'name': 'Text Generation'},\n",
       "      'dataset': {'type': 'wikitext', 'name': 'WikiText-103'},\n",
       "      'metrics': [{'type': 'perplexity',\n",
       "        'name': 'Perplexity',\n",
       "        'value': 21.1}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['openwebtext'],\n",
       "   'model-index': [{'name': 'distilgpt2',\n",
       "     'results': [{'task': {'type': 'text-generation',\n",
       "        'name': 'Text Generation'},\n",
       "       'dataset': {'type': 'wikitext', 'name': 'WikiText-103'},\n",
       "       'metrics': [{'type': 'perplexity',\n",
       "         'name': 'Perplexity',\n",
       "         'value': 21.1}]}]}],\n",
       "   'co2_eq_emissions': '149200 g'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'xlm-roberta-base': {'modelId': 'xlm-roberta-base',\n",
       "  'sha': 'f6d161e8f5f6f2ed433fb4023d6cb34146506b3f',\n",
       "  'lastModified': '2022-06-06T11:40:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'multilingual',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'as',\n",
       "   'az',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'br',\n",
       "   'bs',\n",
       "   'ca',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'eo',\n",
       "   'es',\n",
       "   'et',\n",
       "   'eu',\n",
       "   'fa',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'he',\n",
       "   'hi',\n",
       "   'hr',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'id',\n",
       "   'is',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'ku',\n",
       "   'ky',\n",
       "   'la',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'om',\n",
       "   'or',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sa',\n",
       "   'sd',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'th',\n",
       "   'tl',\n",
       "   'tr',\n",
       "   'ug',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'xh',\n",
       "   'yi',\n",
       "   'zh',\n",
       "   'arxiv:1911.02116',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['XLMRobertaForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'xlm-roberta-base',\n",
       "  'downloads': 5710750,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 34,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['exbert'],\n",
       "   'language': ['multilingual',\n",
       "    'af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'as',\n",
       "    'az',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'br',\n",
       "    'bs',\n",
       "    'ca',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'eo',\n",
       "    'es',\n",
       "    'et',\n",
       "    'eu',\n",
       "    'fa',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'he',\n",
       "    'hi',\n",
       "    'hr',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'id',\n",
       "    'is',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'ku',\n",
       "    'ky',\n",
       "    'la',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'om',\n",
       "    'or',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sa',\n",
       "    'sd',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'th',\n",
       "    'tl',\n",
       "    'tr',\n",
       "    'ug',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'xh',\n",
       "    'yi',\n",
       "    'zh'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'roberta-base': {'modelId': 'roberta-base',\n",
       "  'sha': '251c3c36356d3ad6845eb0554fdb9703d632c6cc',\n",
       "  'lastModified': '2021-07-06T10:34:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1907.11692',\n",
       "   'arxiv:1806.02847',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'roberta-base',\n",
       "  'downloads': 5301889,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 42,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-base-cased': {'modelId': 'bert-base-cased',\n",
       "  'sha': 'a8d257ba9925ef39f3036bfc338acf5283c512d9',\n",
       "  'lastModified': '2021-09-06T08:07:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-base-cased',\n",
       "  'downloads': 5060964,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 26,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-base-multilingual-cased': {'modelId': 'bert-base-multilingual-cased',\n",
       "  'sha': 'aff660c4522e466f4d0de19eaf94f91e4e2e7375',\n",
       "  'lastModified': '2021-05-18T16:18:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'multilingual',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-base-multilingual-cased',\n",
       "  'downloads': 3281257,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 34,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/chinese-roberta-wwm-ext': {'modelId': 'hfl/chinese-roberta-wwm-ext',\n",
       "  'sha': '5c58d0b8ec1d9014354d691c538661bf00bfdb44',\n",
       "  'lastModified': '2022-03-01T09:13:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'arxiv:1906.08101',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'hfl/chinese-roberta-wwm-ext',\n",
       "  'downloads': 3270783,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '巴黎是[MASK]国的首都。'}, {'text': '生活的真谛是[MASK]。'}],\n",
       "  'likes': 43,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'tags': ['bert'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'distilbert-base-multilingual-cased': {'modelId': 'distilbert-base-multilingual-cased',\n",
       "  'sha': '1a01b38498875d45f69b2a6721bf6fe87425da39',\n",
       "  'lastModified': '2020-12-11T21:24:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'fill-mask',\n",
       "   'multilingual',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['DistilBertForMaskedLM'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'distilbert-base-multilingual-cased',\n",
       "  'downloads': 3042085,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-base-chinese': {'modelId': 'bert-base-chinese',\n",
       "  'sha': '4b1f5fb6deac3583018fcf351473024a3d65b2d4',\n",
       "  'lastModified': '2021-05-18T16:13:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-base-chinese',\n",
       "  'downloads': 2998046,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '巴黎是[MASK]国的首都。'}, {'text': '生活的真谛是[MASK]。'}],\n",
       "  'likes': 99,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'zh'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'SpanBERT/spanbert-large-cased': {'modelId': 'SpanBERT/spanbert-large-cased',\n",
       "  'sha': 'a49cba45de9565a5d3e7b089a94dbae679e64e79',\n",
       "  'lastModified': '2021-05-19T11:31:33.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'SpanBERT',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'SpanBERT/spanbert-large-cased',\n",
       "  'downloads': 2980751,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'openai/clip-vit-base-patch32': {'modelId': 'openai/clip-vit-base-patch32',\n",
       "  'sha': 'f4881ba48ee4d21b7ed5602603b9e3e92eb1b346',\n",
       "  'lastModified': '2022-03-14T17:58:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'clip',\n",
       "   'feature-extraction',\n",
       "   'arxiv:2103.00020',\n",
       "   'arxiv:1908.04913',\n",
       "   'transformers',\n",
       "   'vision'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'openai',\n",
       "  'config': {'architectures': ['CLIPModel'], 'model_type': 'clip'},\n",
       "  'id': 'openai/clip-vit-base-patch32',\n",
       "  'downloads': 2931985,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 41,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['vision']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'albert-base-v2': {'modelId': 'albert-base-v2',\n",
       "  'sha': '51dbd9db43a0c6eba97f74b91ce26fface509e0b',\n",
       "  'lastModified': '2021-08-30T12:04:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1909.11942',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'albert-base-v2',\n",
       "  'downloads': 2221973,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/stsb-distilbert-base': {'modelId': 'sentence-transformers/stsb-distilbert-base',\n",
       "  'sha': '815bb3e4dbcba340b5f8c0c0489800230880e06e',\n",
       "  'lastModified': '2022-06-15T19:43:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/stsb-distilbert-base',\n",
       "  'downloads': 2127464,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-zh-en': {'modelId': 'Helsinki-NLP/opus-mt-zh-en',\n",
       "  'sha': 'b7d1e2a349f4e50df9fbde0f8a07280899331cb1',\n",
       "  'lastModified': '2021-02-26T18:53:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'zh',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-zh-en',\n",
       "  'downloads': 2107900,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '我叫沃尔夫冈，我住在柏林。'}, {'text': '我叫萨拉，我住在伦敦。'}],\n",
       "  'likes': 31,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh', 'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'roberta-large': {'modelId': 'roberta-large',\n",
       "  'sha': '619fd8c2ca2bc7ac3959b7f71b6c426c897ba407',\n",
       "  'lastModified': '2021-05-21T08:57:02.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1907.11692',\n",
       "   'arxiv:1806.02847',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'roberta-large',\n",
       "  'downloads': 2041188,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 36,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/ms-marco-MiniLM-L-12-v2': {'modelId': 'cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
       "  'sha': '97f7dcbdd6ab58fe7f44368c795fc5200b48fcbe',\n",
       "  'lastModified': '2021-08-05T08:39:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'cross-encoder/ms-marco-MiniLM-L-12-v2',\n",
       "  'downloads': 1779635,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/all-MiniLM-L6-v2': {'modelId': 'sentence-transformers/all-MiniLM-L6-v2',\n",
       "  'sha': '3746fd5f4cfd46ae64fc781df53e7cbb7849eb62',\n",
       "  'lastModified': '2022-06-15T21:28:20.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'arxiv:1904.06472',\n",
       "   'arxiv:2102.07033',\n",
       "   'arxiv:2104.08727',\n",
       "   'arxiv:1704.05179',\n",
       "   'arxiv:1810.09305',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/all-MiniLM-L6-v2',\n",
       "  'downloads': 1723581,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 43,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'xlm-roberta-large-finetuned-conll03-english': {'modelId': 'xlm-roberta-large-finetuned-conll03-english',\n",
       "  'sha': '3daa989491c928b218c78023b6dafd57b5dba966',\n",
       "  'lastModified': '2020-10-12T12:57:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'xlm-roberta',\n",
       "   'token-classification',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['XLMRobertaForTokenClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'xlm-roberta-large-finetuned-conll03-english',\n",
       "  'downloads': 1712460,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 19,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-large-uncased': {'modelId': 'bert-large-uncased',\n",
       "  'sha': '3835a195d41f7ddc47d5ecab84b64f71d6f144e9',\n",
       "  'lastModified': '2021-05-18T16:40:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-large-uncased',\n",
       "  'downloads': 1666413,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/paraphrase-MiniLM-L6-v2': {'modelId': 'sentence-transformers/paraphrase-MiniLM-L6-v2',\n",
       "  'sha': '68b97aaedb0c72be3c88c1af64296b3bbb8001fa',\n",
       "  'lastModified': '2022-06-15T18:39:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/paraphrase-MiniLM-L6-v2',\n",
       "  'downloads': 1654328,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 16,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'DeepPavlov/rubert-base-cased-conversational': {'modelId': 'DeepPavlov/rubert-base-cased-conversational',\n",
       "  'sha': '645946ce91842a52eaacb2705c77e59194145ffa',\n",
       "  'lastModified': '2021-11-08T13:06:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'ru',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'DeepPavlov',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'DeepPavlov/rubert-base-cased-conversational',\n",
       "  'downloads': 1394793,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'EleutherAI/gpt-j-6B': {'modelId': 'EleutherAI/gpt-j-6B',\n",
       "  'sha': '918ad376364058dee23512629bc385380c98e57d',\n",
       "  'lastModified': '2022-03-15T13:34:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gptj',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'dataset:The Pile',\n",
       "   'arxiv:2104.09864',\n",
       "   'arxiv:2101.00027',\n",
       "   'transformers',\n",
       "   'causal-lm',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'EleutherAI',\n",
       "  'config': {'architectures': ['GPTJForCausalLM'],\n",
       "   'model_type': 'gptj',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50,\n",
       "     'temperature': 1}}},\n",
       "  'id': 'EleutherAI/gpt-j-6B',\n",
       "  'downloads': 1359238,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 228,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['pytorch', 'causal-lm'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['The Pile']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cardiffnlp/twitter-roberta-base-sentiment': {'modelId': 'cardiffnlp/twitter-roberta-base-sentiment',\n",
       "  'sha': 'b636d90b2ed53d7ba6006cefd76f29cd354dd9da',\n",
       "  'lastModified': '2022-04-06T08:10:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'arxiv:2010.12421',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cardiffnlp',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cardiffnlp/twitter-roberta-base-sentiment',\n",
       "  'downloads': 1214346,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 55,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cardiffnlp/twitter-xlm-roberta-base-sentiment': {'modelId': 'cardiffnlp/twitter-xlm-roberta-base-sentiment',\n",
       "  'sha': 'f3e34b6c30bf27b6649f72eca85d0bbe79df1e55',\n",
       "  'lastModified': '2022-06-22T19:15:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'text-classification',\n",
       "   'multilingual',\n",
       "   'arxiv:2104.12250',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cardiffnlp',\n",
       "  'config': {'architectures': ['XLMRobertaForSequenceClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'cardiffnlp/twitter-xlm-roberta-base-sentiment',\n",
       "  'downloads': 1180933,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': '🤗'},\n",
       "   {'text': \"T'estimo! ❤️\"},\n",
       "   {'text': 'I love you!'},\n",
       "   {'text': 'I hate you 🤮'},\n",
       "   {'text': 'Mahal kita!'},\n",
       "   {'text': '사랑해!'},\n",
       "   {'text': '난 너가 싫어'},\n",
       "   {'text': '😍😍😍'}],\n",
       "  'likes': 20,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'widget': [{'text': '🤗'},\n",
       "    {'text': \"T'estimo! ❤️\"},\n",
       "    {'text': 'I love you!'},\n",
       "    {'text': 'I hate you 🤮'},\n",
       "    {'text': 'Mahal kita!'},\n",
       "    {'text': '사랑해!'},\n",
       "    {'text': '난 너가 싫어'},\n",
       "    {'text': '😍😍😍'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'unc-nlp/lxmert-base-uncased': {'modelId': 'unc-nlp/lxmert-base-uncased',\n",
       "  'sha': '628572c96242d1496147beec1c13a1bb7869605d',\n",
       "  'lastModified': '2021-03-10T02:39:25.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'lxmert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'unc-nlp',\n",
       "  'config': {'architectures': ['LxmertModel'], 'model_type': 'lxmert'},\n",
       "  'id': 'unc-nlp/lxmert-base-uncased',\n",
       "  'downloads': 1160943,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-en-de': {'modelId': 'Helsinki-NLP/opus-mt-en-de',\n",
       "  'sha': '6c00b328d3da7183582a4928b638b24a4a14a79f',\n",
       "  'lastModified': '2021-09-09T21:34:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-de',\n",
       "  'downloads': 1125890,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/bert-base-nli-mean-tokens': {'modelId': 'sentence-transformers/bert-base-nli-mean-tokens',\n",
       "  'sha': '18fc720063106176044380e71bad038d01e821d1',\n",
       "  'lastModified': '2022-06-09T12:34:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/bert-base-nli-mean-tokens',\n",
       "  'downloads': 1112113,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'xlm-roberta-large': {'modelId': 'xlm-roberta-large',\n",
       "  'sha': 'b2a6150f8be56457baf80c74342cc424080260f0',\n",
       "  'lastModified': '2022-06-27T11:25:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'multilingual',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'as',\n",
       "   'az',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'br',\n",
       "   'bs',\n",
       "   'ca',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'eo',\n",
       "   'es',\n",
       "   'et',\n",
       "   'eu',\n",
       "   'fa',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'he',\n",
       "   'hi',\n",
       "   'hr',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'id',\n",
       "   'is',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'ku',\n",
       "   'ky',\n",
       "   'la',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'om',\n",
       "   'or',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sa',\n",
       "   'sd',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'th',\n",
       "   'tl',\n",
       "   'tr',\n",
       "   'ug',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'xh',\n",
       "   'yi',\n",
       "   'zh',\n",
       "   'arxiv:1911.02116',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['XLMRobertaForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'xlm-roberta-large',\n",
       "  'downloads': 1064428,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 19,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['exbert'],\n",
       "   'language': ['multilingual',\n",
       "    'af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'as',\n",
       "    'az',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'br',\n",
       "    'bs',\n",
       "    'ca',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'eo',\n",
       "    'es',\n",
       "    'et',\n",
       "    'eu',\n",
       "    'fa',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'he',\n",
       "    'hi',\n",
       "    'hr',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'id',\n",
       "    'is',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'ku',\n",
       "    'ky',\n",
       "    'la',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'om',\n",
       "    'or',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sa',\n",
       "    'sd',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'th',\n",
       "    'tl',\n",
       "    'tr',\n",
       "    'ug',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'xh',\n",
       "    'yi',\n",
       "    'zh'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/bart-large-mnli': {'modelId': 'facebook/bart-large-mnli',\n",
       "  'sha': 'c626438eeca63a93bd6024b0a0fbf8b3c0c30d7b',\n",
       "  'lastModified': '2021-08-09T08:25:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'bart',\n",
       "   'text-classification',\n",
       "   'dataset:multi_nli',\n",
       "   'arxiv:1910.13461',\n",
       "   'arxiv:1909.00161',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['BartForSequenceClassification'],\n",
       "   'model_type': 'bart'},\n",
       "  'id': 'facebook/bart-large-mnli',\n",
       "  'downloads': 1036212,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system’s largest world. Konstantin Batygin did not set out to solve one of the solar system’s most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system’s missing “Planet Nine,” spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn’t rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: “Oh! This is how Europa formed.” Europa is one of Jupiter’s four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the Côte d’Azur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system’s formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 136,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'mit',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png',\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'datasets': ['multi_nli']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 't5-small': {'modelId': 't5-small',\n",
       "  'sha': 'd769bbacbd41c68c0f603aeab871b82e1081ffd8',\n",
       "  'lastModified': '2022-03-18T17:23:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'fr',\n",
       "   'ro',\n",
       "   'de',\n",
       "   'dataset:c4',\n",
       "   'arxiv:1805.12471',\n",
       "   'arxiv:1708.00055',\n",
       "   'arxiv:1704.05426',\n",
       "   'arxiv:1606.05250',\n",
       "   'arxiv:1808.09121',\n",
       "   'arxiv:1810.12885',\n",
       "   'arxiv:1905.10044',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['T5WithLMHeadModel'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 't5-small',\n",
       "  'downloads': 1005992,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 17,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'fr', 'ro', 'de'],\n",
       "   'datasets': ['c4'],\n",
       "   'tags': ['summarization', 'translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelWithLMHead',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/roberta-base-squad2': {'modelId': 'deepset/roberta-base-squad2',\n",
       "  'sha': '4dc572bcd6c09de8738cded464ab9cd46a6b9c61',\n",
       "  'lastModified': '2022-06-08T10:38:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'roberta',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'dataset:squad_v2',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['RobertaForQuestionAnswering'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'deepset/roberta-base-squad2',\n",
       "  'downloads': 991395,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 78,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['squad_v2'],\n",
       "   'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/codebert-base': {'modelId': 'microsoft/codebert-base',\n",
       "  'sha': '3b0952feddeffad0063f274080e3c23d75e7eb39',\n",
       "  'lastModified': '2022-02-11T19:59:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:2002.08155',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'microsoft/codebert-base',\n",
       "  'downloads': 872831,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 25,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ProsusAI/finbert': {'modelId': 'ProsusAI/finbert',\n",
       "  'sha': '5ea63b3d0c737ad6f06e061d9af36b1f7bbd1a4b',\n",
       "  'lastModified': '2022-06-03T06:34:37.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'arxiv:1908.10063',\n",
       "   'transformers',\n",
       "   'financial-sentiment-analysis',\n",
       "   'sentiment-analysis'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'ProsusAI',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'ProsusAI/finbert',\n",
       "  'downloads': 812206,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Stocks rallied and the British pound gained.'}],\n",
       "  'likes': 76,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['financial-sentiment-analysis', 'sentiment-analysis'],\n",
       "   'widget': [{'text': 'Stocks rallied and the British pound gained.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 't5-base': {'modelId': 't5-base',\n",
       "  'sha': '686f1db74a0daa55de154507afe13c68843311cf',\n",
       "  'lastModified': '2022-03-18T17:22:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'fr',\n",
       "   'ro',\n",
       "   'de',\n",
       "   'dataset:c4',\n",
       "   'arxiv:1805.12471',\n",
       "   'arxiv:1708.00055',\n",
       "   'arxiv:1704.05426',\n",
       "   'arxiv:1606.05250',\n",
       "   'arxiv:1808.09121',\n",
       "   'arxiv:1810.12885',\n",
       "   'arxiv:1905.10044',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['T5WithLMHeadModel'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 't5-base',\n",
       "  'downloads': 746393,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 50,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'fr', 'ro', 'de'],\n",
       "   'datasets': ['c4'],\n",
       "   'tags': ['summarization', 'translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelWithLMHead',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cl-tohoku/bert-base-japanese-whole-word-masking': {'modelId': 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
       "  'sha': 'ab68bf4a4d55e7772b1fbea6441bdab72aaf949c',\n",
       "  'lastModified': '2021-09-23T13:45:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ja',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'cl-tohoku',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
       "  'downloads': 684415,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '東北大学で[MASK]の研究をしています。'}],\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'datasets': ['wikipedia'],\n",
       "   'widget': [{'text': '東北大学で[MASK]の研究をしています。'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allenai/scibert_scivocab_uncased': {'modelId': 'allenai/scibert_scivocab_uncased',\n",
       "  'sha': '2ab156b969f2dbbd7ecc0080b78bc2cd272c4092',\n",
       "  'lastModified': '2021-05-19T11:41:40.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'allenai/scibert_scivocab_uncased',\n",
       "  'downloads': 672403,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 19,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'facebook/bart-large-cnn': {'modelId': 'facebook/bart-large-cnn',\n",
       "  'sha': '6d60e92a7a2bbf5051227d6fbeda503e53a26b68',\n",
       "  'lastModified': '2021-11-19T14:51:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'arxiv:1910.13461',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4}}},\n",
       "  'id': 'facebook/bart-large-cnn',\n",
       "  'downloads': 669575,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 63,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['summarization'],\n",
       "   'license': 'mit',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-en-fr': {'modelId': 'Helsinki-NLP/opus-mt-en-fr',\n",
       "  'sha': 'a8fbc1c711cb6263e8a20c5229b210cc05c57ff0',\n",
       "  'lastModified': '2021-09-09T21:35:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'fr',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-fr',\n",
       "  'downloads': 666422,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/all-mpnet-base-v2': {'modelId': 'sentence-transformers/all-mpnet-base-v2',\n",
       "  'sha': 'cd48db1124ae80604e59187546e6d7708a7745ee',\n",
       "  'lastModified': '2022-07-02T21:42:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mpnet',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:s2orc',\n",
       "   'arxiv:1904.06472',\n",
       "   'arxiv:2102.07033',\n",
       "   'arxiv:2104.08727',\n",
       "   'arxiv:1704.05179',\n",
       "   'arxiv:1810.09305',\n",
       "   'sentence-transformers',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['MPNetForMaskedLM'], 'model_type': 'mpnet'},\n",
       "  'id': 'sentence-transformers/all-mpnet-base-v2',\n",
       "  'downloads': 601937,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 36,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['s2orc']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2': {'modelId': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
       "  'sha': 'b8ef00830037f9868450f778081ea683e900fe39',\n",
       "  'lastModified': '2022-06-15T18:43:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'multilingual',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
       "  'downloads': 592144,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 41,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'language': 'multilingual',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/bart-base': {'modelId': 'facebook/bart-base',\n",
       "  'sha': '84358834e73de6a82c22cec1d90eb45ef4f6eba5',\n",
       "  'lastModified': '2022-06-03T09:43:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bart',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'arxiv:1910.13461',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['BartModel'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'length_penalty': 1,\n",
       "     'max_length': 128,\n",
       "     'min_length': 12,\n",
       "     'num_beams': 4},\n",
       "    'summarization_cnn': {'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'num_beams': 4},\n",
       "    'summarization_xsum': {'length_penalty': 1,\n",
       "     'max_length': 62,\n",
       "     'min_length': 11,\n",
       "     'num_beams': 6}}},\n",
       "  'id': 'facebook/bart-base',\n",
       "  'downloads': 591912,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 16,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0', 'language': 'en'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dslim/bert-base-NER': {'modelId': 'dslim/bert-base-NER',\n",
       "  'sha': 'f7c2808a659015eeb8828f3f809a2f1be67a2446',\n",
       "  'lastModified': '2021-09-05T12:00:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'en',\n",
       "   'dataset:conll2003',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'dslim',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'dslim/bert-base-NER',\n",
       "  'downloads': 579289,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 57,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'datasets': ['conll2003'], 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'distilbert-base-cased-distilled-squad': {'modelId': 'distilbert-base-cased-distilled-squad',\n",
       "  'sha': '626af3168b44d7bc30c62ce00f32b76b189137a5',\n",
       "  'lastModified': '2020-12-11T21:23:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rust',\n",
       "   'distilbert',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'dataset:squad',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['DistilBertForQuestionAnswering'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'distilbert-base-cased-distilled-squad',\n",
       "  'downloads': 577595,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 14,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['squad'],\n",
       "   'metrics': ['squad'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cardiffnlp/twitter-roberta-base-sentiment-latest': {'modelId': 'cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
       "  'sha': '5916057ce88cf0a408a195082b6c06d3dce12552',\n",
       "  'lastModified': '2022-03-31T09:47:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'english',\n",
       "   'arxiv:2202.03829',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cardiffnlp',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
       "  'downloads': 576895,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Covid cases are increasing fast!'}],\n",
       "  'likes': 27,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'english',\n",
       "   'widget': [{'text': 'Covid cases are increasing fast!'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sshleifer/distilbart-cnn-12-6': {'modelId': 'sshleifer/distilbart-cnn-12-6',\n",
       "  'sha': 'a4f8f3ea906ed274767e9906dbaede7531d660ff',\n",
       "  'lastModified': '2021-06-14T07:51:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:cnn_dailymail',\n",
       "   'dataset:xsum',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4}}},\n",
       "  'id': 'sshleifer/distilbart-cnn-12-6',\n",
       "  'downloads': 569966,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 51,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['summarization'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['cnn_dailymail', 'xsum'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/distilbart_medium.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/ms-marco-TinyBERT-L-2': {'modelId': 'cross-encoder/ms-marco-TinyBERT-L-2',\n",
       "  'sha': 'e9ed04745b2b19e8c4499360253ea5d5b41b5810',\n",
       "  'lastModified': '2021-08-05T08:39:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'cross-encoder/ms-marco-TinyBERT-L-2',\n",
       "  'downloads': 557872,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-en-es': {'modelId': 'Helsinki-NLP/opus-mt-en-es',\n",
       "  'sha': 'c8b63c83f30e46417ce423a585f9b9e20e1b877d',\n",
       "  'lastModified': '2021-07-13T16:24:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'es',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-es',\n",
       "  'downloads': 540882,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'es'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/minilm-uncased-squad2': {'modelId': 'deepset/minilm-uncased-squad2',\n",
       "  'sha': 'c28e1a1d3d7d077ee7f4e63387f1a140f251daea',\n",
       "  'lastModified': '2021-10-21T12:19:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'dataset:squad_v2',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'deepset/minilm-uncased-squad2',\n",
       "  'downloads': 522179,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['squad_v2'], 'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-base-multilingual-uncased': {'modelId': 'bert-base-multilingual-uncased',\n",
       "  'sha': '99406b9f2cfa046409626308a01da45a2a078f62',\n",
       "  'lastModified': '2021-05-18T16:19:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-base-multilingual-uncased',\n",
       "  'downloads': 508907,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-large-cased': {'modelId': 'bert-large-cased',\n",
       "  'sha': 'd9238236d8326ce4bc117132bb3b7e62e95f3a9a',\n",
       "  'lastModified': '2021-05-18T16:33:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-large-cased',\n",
       "  'downloads': 498013,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/distiluse-base-multilingual-cased-v2': {'modelId': 'sentence-transformers/distiluse-base-multilingual-cased-v2',\n",
       "  'sha': '896fbacdabde59de4cb8d75dea7b9bff6066015c',\n",
       "  'lastModified': '2022-06-15T19:24:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'multilingual',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/distiluse-base-multilingual-cased-v2',\n",
       "  'downloads': 496518,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 18,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'language': 'multilingual',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flair/ner-english-large': {'modelId': 'flair/ner-english-large',\n",
       "  'sha': 'e2b1caabf7f9bac1e7829db73eac734df7e6ad7b',\n",
       "  'lastModified': '2021-05-08T15:36:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'dataset:conll2003',\n",
       "   'arxiv:2011.06993',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/ner-english-large',\n",
       "  'downloads': 487895,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'George Washington went to Washington'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': 'en',\n",
       "   'datasets': ['conll2003'],\n",
       "   'widget': [{'text': 'George Washington went to Washington'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'microsoft/deberta-v3-base': {'modelId': 'microsoft/deberta-v3-base',\n",
       "  'sha': '559062ad13d311b87b2c455e67dcd5f1c8f65111',\n",
       "  'lastModified': '2022-02-06T09:55:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rust',\n",
       "   'deberta-v2',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'arxiv:2111.09543',\n",
       "   'transformers',\n",
       "   'deberta',\n",
       "   'deberta-v3',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'deberta-v2'},\n",
       "  'id': 'microsoft/deberta-v3-base',\n",
       "  'downloads': 486180,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 21,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['deberta', 'deberta-v3'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'gpt2-medium': {'modelId': 'gpt2-medium',\n",
       "  'sha': 'db57b2fa2912ce3edbf35caa7aa70cadbb1e91b5',\n",
       "  'lastModified': '2021-05-21T09:17:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'gpt2-medium',\n",
       "  'downloads': 470649,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mrm8488/t5-base-finetuned-question-generation-ap': {'modelId': 'mrm8488/t5-base-finetuned-question-generation-ap',\n",
       "  'sha': '7281097a2e51b1b57684b7de9999e32a0250dd83',\n",
       "  'lastModified': '2022-06-06T21:28:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:squad',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'mrm8488/t5-base-finetuned-question-generation-ap',\n",
       "  'downloads': 461427,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'answer: Manuel context: Manuel has created RuPERTa-base with the support of HF-Transformers and Google'}],\n",
       "  'likes': 23,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['squad'],\n",
       "   'widget': [{'text': 'answer: Manuel context: Manuel has created RuPERTa-base with the support of HF-Transformers and Google'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'distilbert-base-cased': {'modelId': 'distilbert-base-cased',\n",
       "  'sha': '935ac13b473164bb9d578640e33d9f21144c365e',\n",
       "  'lastModified': '2020-12-11T21:23:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1910.01108',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'model_type': 'distilbert'},\n",
       "  'id': 'distilbert-base-cased',\n",
       "  'downloads': 457738,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/all-MiniLM-L12-v2': {'modelId': 'sentence-transformers/all-MiniLM-L12-v2',\n",
       "  'sha': '2e5bea94bf8c0163aeb984db01be29cfb79e8e6b',\n",
       "  'lastModified': '2022-06-21T14:55:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'bert',\n",
       "   'en',\n",
       "   'arxiv:1904.06472',\n",
       "   'arxiv:2102.07033',\n",
       "   'arxiv:2104.08727',\n",
       "   'arxiv:1704.05179',\n",
       "   'arxiv:1810.09305',\n",
       "   'sentence-transformers',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/all-MiniLM-L12-v2',\n",
       "  'downloads': 457493,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'Helsinki-NLP/opus-mt-fr-en': {'modelId': 'Helsinki-NLP/opus-mt-fr-en',\n",
       "  'sha': '967b0840416a86ccf02573c8fedf9dd0e0b42fd6',\n",
       "  'lastModified': '2021-09-09T21:53:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'fr',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-fr-en',\n",
       "  'downloads': 440412,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Mon nom est Wolfgang et je vis à Berlin'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dslim/bert-large-NER': {'modelId': 'dslim/bert-large-NER',\n",
       "  'sha': '95c62bc0d4109bd97d0578e5ff482e6b84c2b8b9',\n",
       "  'lastModified': '2022-06-27T20:58:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'en',\n",
       "   'dataset:conll2003',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'dslim',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'dslim/bert-large-NER',\n",
       "  'downloads': 432664,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'datasets': ['conll2003'], 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bhadresh-savani/distilbert-base-uncased-emotion': {'modelId': 'bhadresh-savani/distilbert-base-uncased-emotion',\n",
       "  'sha': 'fff238f293a3a39108ff7345a1638e000be81b50',\n",
       "  'lastModified': '2021-09-15T17:40:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'distilbert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:emotion',\n",
       "   'arxiv:1910.01108',\n",
       "   'transformers',\n",
       "   'emotion',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'bhadresh-savani',\n",
       "  'config': {'architectures': ['DistilBertForSequenceClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'bhadresh-savani/distilbert-base-uncased-emotion',\n",
       "  'downloads': 430839,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 34,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'thumbnail': 'https://avatars3.githubusercontent.com/u/32437151?s=460&u=4ec59abc8d21d5feea3dab323d23a5860e6996a4&v=4',\n",
       "   'tags': ['text-classification', 'emotion', 'pytorch'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['emotion'],\n",
       "   'metrics': ['Accuracy, F1 Score']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'jplu/tf-xlm-roberta-base': {'modelId': 'jplu/tf-xlm-roberta-base',\n",
       "  'sha': 'bda462ca2c5c2b6851396df539f3eb01f08f3869',\n",
       "  'lastModified': '2020-12-11T21:48:00.000Z',\n",
       "  'tags': ['tf',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'jplu',\n",
       "  'config': {'architectures': ['XLMRobertaForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'jplu/tf-xlm-roberta-base',\n",
       "  'downloads': 428286,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wav2vec2-base-960h': {'modelId': 'facebook/wav2vec2-base-960h',\n",
       "  'sha': '706111756296bc76512407a11e69526cf4e22aae',\n",
       "  'lastModified': '2022-06-30T00:05:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'en',\n",
       "   'dataset:librispeech_asr',\n",
       "   'arxiv:2006.11477',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'hf-asr-leaderboard',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'facebook/wav2vec2-base-960h',\n",
       "  'downloads': 423146,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'example_title': 'Librispeech sample 1',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/sample1.flac'},\n",
       "   {'example_title': 'Librispeech sample 2',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/sample2.flac'}],\n",
       "  'likes': 50,\n",
       "  'model-index': [{'name': 'wav2vec2-base-960h',\n",
       "    'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'LibriSpeech (clean)',\n",
       "       'type': 'librispeech_asr',\n",
       "       'config': 'clean',\n",
       "       'split': 'test',\n",
       "       'args': {'language': 'en'}},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 3.4}]},\n",
       "     {'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'LibriSpeech (other)',\n",
       "       'type': 'librispeech_asr',\n",
       "       'config': 'other',\n",
       "       'split': 'test',\n",
       "       'args': {'language': 'en'}},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 8.6}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['librispeech_asr'],\n",
       "   'tags': ['audio', 'automatic-speech-recognition', 'hf-asr-leaderboard'],\n",
       "   'license': 'apache-2.0',\n",
       "   'widget': [{'example_title': 'Librispeech sample 1',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/sample1.flac'},\n",
       "    {'example_title': 'Librispeech sample 2',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/sample2.flac'}],\n",
       "   'model-index': [{'name': 'wav2vec2-base-960h',\n",
       "     'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'LibriSpeech (clean)',\n",
       "        'type': 'librispeech_asr',\n",
       "        'config': 'clean',\n",
       "        'split': 'test',\n",
       "        'args': {'language': 'en'}},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 3.4}]},\n",
       "      {'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'LibriSpeech (other)',\n",
       "        'type': 'librispeech_asr',\n",
       "        'config': 'other',\n",
       "        'split': 'test',\n",
       "        'args': {'language': 'en'}},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 8.6}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'klue/bert-base': {'modelId': 'klue/bert-base',\n",
       "  'sha': '812449f1a6bc736e693db7aa0e513e5e90795a62',\n",
       "  'lastModified': '2021-10-20T15:23:59.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ko',\n",
       "   'arxiv:2105.09680',\n",
       "   'transformers',\n",
       "   'korean',\n",
       "   'klue',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'klue',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'klue/bert-base',\n",
       "  'downloads': 418977,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '대한민국의 수도는 [MASK] 입니다.'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ko',\n",
       "   'tags': ['korean', 'klue'],\n",
       "   'mask_token': '[MASK]',\n",
       "   'widget': [{'text': '대한민국의 수도는 [MASK] 입니다.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/DialoGPT-large': {'modelId': 'microsoft/DialoGPT-large',\n",
       "  'sha': '06a70b2c3cecc1f56edc9fdc58d2e90641c9ae9e',\n",
       "  'lastModified': '2021-05-23T09:06:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'arxiv:1911.00536',\n",
       "   'transformers',\n",
       "   'conversational',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'microsoft/DialoGPT-large',\n",
       "  'downloads': 418526,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 28,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/dialogpt.png',\n",
       "   'tags': ['conversational'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'daigo/bert-base-japanese-sentiment': {'modelId': 'daigo/bert-base-japanese-sentiment',\n",
       "  'sha': '51ac2d2c0a5645d77ca26078fc5f02c349fbb93d',\n",
       "  'lastModified': '2021-05-19T14:36:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'ja',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'daigo',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert',\n",
       "   'task_specific_params': None},\n",
       "  'id': 'daigo/bert-base-japanese-sentiment',\n",
       "  'downloads': 418022,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ja']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'alvaroalon2/biobert_chemical_ner': {'modelId': 'alvaroalon2/biobert_chemical_ner',\n",
       "  'sha': 'f34bbf172059e8f6418c43efa925122feb46842b',\n",
       "  'lastModified': '2021-07-07T12:35:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'English',\n",
       "   'dataset:BC5CDR-chemicals',\n",
       "   'dataset:BC4CHEMD',\n",
       "   'transformers',\n",
       "   'NER',\n",
       "   'Biomedical',\n",
       "   'Chemicals',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'alvaroalon2',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'alvaroalon2/biobert_chemical_ner',\n",
       "  'downloads': 413385,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'English',\n",
       "   'tags': ['token-classification', 'NER', 'Biomedical', 'Chemicals'],\n",
       "   'datasets': ['BC5CDR-chemicals', 'BC4CHEMD'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'princeton-nlp/sup-simcse-bert-base-uncased': {'modelId': 'princeton-nlp/sup-simcse-bert-base-uncased',\n",
       "  'sha': '2d82fab19ac3a73a20dd20333d27eb8a52d6e97f',\n",
       "  'lastModified': '2021-05-20T02:54:31.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'princeton-nlp',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'princeton-nlp/sup-simcse-bert-base-uncased',\n",
       "  'downloads': 401184,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/nli-distilroberta-base': {'modelId': 'cross-encoder/nli-distilroberta-base',\n",
       "  'sha': '99f096e70ef1fb038b8f0aecabc5a0f491684084',\n",
       "  'lastModified': '2021-08-05T08:40:59.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:multi_nli',\n",
       "   'dataset:snli',\n",
       "   'transformers',\n",
       "   'distilroberta-base',\n",
       "   'license:apache-2.0',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cross-encoder/nli-distilroberta-base',\n",
       "  'downloads': 392951,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system’s largest world. Konstantin Batygin did not set out to solve one of the solar system’s most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system’s missing “Planet Nine,” spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn’t rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: “Oh! This is how Europa formed.” Europa is one of Jupiter’s four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the Côte d’Azur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system’s formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'tags': ['distilroberta-base'],\n",
       "   'datasets': ['multi_nli', 'snli'],\n",
       "   'metrics': ['accuracy'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-en-ROMANCE': {'modelId': 'Helsinki-NLP/opus-mt-en-ROMANCE',\n",
       "  'sha': '92870a2f094c444064c7a568c25eef6971e07b03',\n",
       "  'lastModified': '2021-09-09T21:34:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'roa',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-ROMANCE',\n",
       "  'downloads': 389575,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/chinese-bert-wwm-ext': {'modelId': 'hfl/chinese-bert-wwm-ext',\n",
       "  'sha': '2a995a880017c60e4683869e817130d8af548486',\n",
       "  'lastModified': '2021-05-19T19:06:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'arxiv:1906.08101',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'hfl/chinese-bert-wwm-ext',\n",
       "  'downloads': 386389,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '巴黎是[MASK]国的首都。'}, {'text': '生活的真谛是[MASK]。'}],\n",
       "  'likes': 25,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jean-Baptiste/camembert-ner-with-dates': {'modelId': 'Jean-Baptiste/camembert-ner-with-dates',\n",
       "  'sha': '8c2d77a331733d26e0ca95a8f525e0ca3aa8e909',\n",
       "  'lastModified': '2021-08-30T12:55:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'camembert',\n",
       "   'token-classification',\n",
       "   'fr',\n",
       "   'dataset:Jean-Baptiste/wikiner_fr',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jean-Baptiste',\n",
       "  'config': {'architectures': ['CamembertForTokenClassification'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'Jean-Baptiste/camembert-ner-with-dates',\n",
       "  'downloads': 383067,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': \"Je m'appelle jean-baptiste et j'habite à montréal depuis fevr 2012\"}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr',\n",
       "   'datasets': ['Jean-Baptiste/wikiner_fr'],\n",
       "   'widget': [{'text': \"Je m'appelle jean-baptiste et j'habite à montréal depuis fevr 2012\"}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dslim/bert-base-NER-uncased': {'modelId': 'dslim/bert-base-NER-uncased',\n",
       "  'sha': '1f52ebe0381dc9e285c0aa7c2971b350894f1efa',\n",
       "  'lastModified': '2021-05-19T16:10:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'dslim',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'dslim/bert-base-NER-uncased',\n",
       "  'downloads': 382315,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'camembert-base': {'modelId': 'camembert-base',\n",
       "  'sha': '482393b6198924f9da270b1aaf37d238aafca99b',\n",
       "  'lastModified': '2021-06-09T00:01:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'camembert',\n",
       "   'fill-mask',\n",
       "   'fr',\n",
       "   'dataset:oscar',\n",
       "   'arxiv:1911.03894',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['CamembertForMaskedLM'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'camembert-base',\n",
       "  'downloads': 379731,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris est la <mask> de la France.'}],\n",
       "  'likes': 16,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr', 'license': 'mit', 'datasets': ['oscar']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-large-uncased-whole-word-masking-finetuned-squad': {'modelId': 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
       "  'sha': '242d9dbb66bb5033025196d5678907307f8fb098',\n",
       "  'lastModified': '2021-05-18T16:35:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
       "  'downloads': 373885,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 23,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/layoutlmv2-base-uncased': {'modelId': 'microsoft/layoutlmv2-base-uncased',\n",
       "  'sha': '5c1ca07c23780c6dc123807def206ae9c4d59aca',\n",
       "  'lastModified': '2021-12-23T12:52:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'layoutlmv2',\n",
       "   'en',\n",
       "   'arxiv:2012.14740',\n",
       "   'transformers',\n",
       "   'license:cc-by-nc-sa-4.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'layoutlmv2'},\n",
       "  'id': 'microsoft/layoutlmv2-base-uncased',\n",
       "  'downloads': 362553,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 18,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'license': 'cc-by-nc-sa-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'microsoft/layoutlm-base-uncased': {'modelId': 'microsoft/layoutlm-base-uncased',\n",
       "  'sha': 'ca841ce8d2f46b13b0ac3f635b8eb7d2e1d758d5',\n",
       "  'lastModified': '2021-08-11T05:27:42.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'layoutlm', 'arxiv:1912.13318', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'layoutlm'},\n",
       "  'id': 'microsoft/layoutlm-base-uncased',\n",
       "  'downloads': 362286,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'absa/classifier-rest-0.2': {'modelId': 'absa/classifier-rest-0.2',\n",
       "  'sha': 'd1712a46a6725043e6bb3b720dc314d2a523977c',\n",
       "  'lastModified': '2021-05-19T11:37:54.000Z',\n",
       "  'tags': ['tf', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'absa',\n",
       "  'config': {'architectures': None, 'model_type': 'bert'},\n",
       "  'id': 'absa/classifier-rest-0.2',\n",
       "  'downloads': 350136,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'TFAutoModel'}},\n",
       " 'microsoft/deberta-v3-large': {'modelId': 'microsoft/deberta-v3-large',\n",
       "  'sha': '360b9940401fa4d3411a0ca9f796631ec36f287a',\n",
       "  'lastModified': '2022-01-13T17:50:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'deberta-v2',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'arxiv:2111.09543',\n",
       "   'transformers',\n",
       "   'deberta',\n",
       "   'deberta-v3',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'deberta-v2'},\n",
       "  'id': 'microsoft/deberta-v3-large',\n",
       "  'downloads': 342995,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 33,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['deberta', 'deberta-v3'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'gpt2-xl': {'modelId': 'gpt2-xl',\n",
       "  'sha': '82bb8104f524eef7f49c81a339b62f5866ef95b6',\n",
       "  'lastModified': '2022-03-08T09:48:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'gpt2-xl',\n",
       "  'downloads': 333346,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Rostlab/prot_bert': {'modelId': 'Rostlab/prot_bert',\n",
       "  'sha': '3d05bf06e79014892defacad82e0efd06e977ff6',\n",
       "  'lastModified': '2020-12-11T21:30:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'fill-mask',\n",
       "   'protein',\n",
       "   'dataset:Uniref100',\n",
       "   'transformers',\n",
       "   'protein language model',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Rostlab',\n",
       "  'config': {'architectures': ['BertForMaskedLM']},\n",
       "  'id': 'Rostlab/prot_bert',\n",
       "  'downloads': 322668,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 19,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'protein',\n",
       "   'tags': ['protein language model'],\n",
       "   'datasets': ['Uniref100']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask'}},\n",
       " 'emilyalsentzer/Bio_Discharge_Summary_BERT': {'modelId': 'emilyalsentzer/Bio_Discharge_Summary_BERT',\n",
       "  'sha': 'affde836a50e4d333f15dae9270f5a856d59540b',\n",
       "  'lastModified': '2022-02-27T13:59:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'en',\n",
       "   'arxiv:1904.03323',\n",
       "   'arxiv:1901.08746',\n",
       "   'transformers',\n",
       "   'fill-mask',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'emilyalsentzer',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'emilyalsentzer/Bio_Discharge_Summary_BERT',\n",
       "  'downloads': 321886,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'tags': ['fill-mask'], 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/paraphrase-mpnet-base-v2': {'modelId': 'sentence-transformers/paraphrase-mpnet-base-v2',\n",
       "  'sha': '18df4b22cd35517843308534d066190182ff39ef',\n",
       "  'lastModified': '2022-06-15T19:23:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'mpnet',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['MPNetModel'], 'model_type': 'mpnet'},\n",
       "  'id': 'sentence-transformers/paraphrase-mpnet-base-v2',\n",
       "  'downloads': 305105,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/bart-large': {'modelId': 'facebook/bart-large',\n",
       "  'sha': 'cb48c1365bd826bd521f650dc2e0940aee54720c',\n",
       "  'lastModified': '2022-06-03T10:00:20.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'bart',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'arxiv:1910.13461',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['BartModel'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'length_penalty': 1,\n",
       "     'max_length': 128,\n",
       "     'min_length': 12,\n",
       "     'num_beams': 4},\n",
       "    'summarization_cnn': {'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'num_beams': 4},\n",
       "    'summarization_xsum': {'length_penalty': 1,\n",
       "     'max_length': 62,\n",
       "     'min_length': 11,\n",
       "     'num_beams': 6}}},\n",
       "  'id': 'facebook/bart-large',\n",
       "  'downloads': 296934,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0', 'language': 'en'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'distilroberta-base': {'modelId': 'distilroberta-base',\n",
       "  'sha': 'ec58a5b7f760a8df0e81accb5abf0ab8368612f8',\n",
       "  'lastModified': '2021-05-20T22:47:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:openwebtext',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'distilroberta-base',\n",
       "  'downloads': 294170,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 21,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['openwebtext']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'yiyanghkust/finbert-tone': {'modelId': 'yiyanghkust/finbert-tone',\n",
       "  'sha': '69507fb7dad65fd5ee96679690e6336211edc7a5',\n",
       "  'lastModified': '2022-06-09T12:05:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'financial-sentiment-analysis',\n",
       "   'sentiment-analysis'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'yiyanghkust',\n",
       "  'config': {'architectures': ['BertForSequenceClassification']},\n",
       "  'id': 'yiyanghkust/finbert-tone',\n",
       "  'downloads': 287500,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'growth is strong and we have plenty of liquidity'}],\n",
       "  'likes': 20,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['financial-sentiment-analysis', 'sentiment-analysis'],\n",
       "   'widget': [{'text': 'growth is strong and we have plenty of liquidity'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification'}},\n",
       " 'joeddav/xlm-roberta-large-xnli': {'modelId': 'joeddav/xlm-roberta-large-xnli',\n",
       "  'sha': '9c1619b90a142cd2913190d80d5f488d6612f57e',\n",
       "  'lastModified': '2020-12-17T16:39:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'text-classification',\n",
       "   'multilingual',\n",
       "   'dataset:multi_nli',\n",
       "   'dataset:xnli',\n",
       "   'arxiv:1911.02116',\n",
       "   'transformers',\n",
       "   'tensorflow',\n",
       "   'license:mit',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'joeddav',\n",
       "  'config': {'architectures': ['XLMRobertaForSequenceClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'joeddav/xlm-roberta-large-xnli',\n",
       "  'downloads': 287482,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'За кого вы голосуете в 2020 году?',\n",
       "    'candidate_labels': 'politique étrangère, Europe, élections, affaires, politique',\n",
       "    'multi_class': True},\n",
       "   {'text': 'لمن تصوت في 2020؟',\n",
       "    'candidate_labels': 'السياسة الخارجية, أوروبا, الانتخابات, الأعمال, السياسة',\n",
       "    'multi_class': True},\n",
       "   {'text': \"2020'de kime oy vereceksiniz?\",\n",
       "    'candidate_labels': 'dış politika, Avrupa, seçimler, ticaret, siyaset',\n",
       "    'multi_class': True}],\n",
       "  'likes': 44,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'tags': ['text-classification', 'pytorch', 'tensorflow'],\n",
       "   'datasets': ['multi_nli', 'xnli'],\n",
       "   'license': 'mit',\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'widget': [{'text': 'За кого вы голосуете в 2020 году?',\n",
       "     'candidate_labels': 'politique étrangère, Europe, élections, affaires, politique',\n",
       "     'multi_class': True},\n",
       "    {'text': 'لمن تصوت في 2020؟',\n",
       "     'candidate_labels': 'السياسة الخارجية, أوروبا, الانتخابات, الأعمال, السياسة',\n",
       "     'multi_class': True},\n",
       "    {'text': \"2020'de kime oy vereceksiniz?\",\n",
       "     'candidate_labels': 'dış politika, Avrupa, seçimler, ticaret, siyaset',\n",
       "     'multi_class': True}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'gpt2-large': {'modelId': 'gpt2-large',\n",
       "  'sha': '897ad545cc0303ac6a2a8b7105c48aaca02f5294',\n",
       "  'lastModified': '2021-09-08T15:44:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'gpt2-large',\n",
       "  'downloads': 281109,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 14,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/m2m100_418M': {'modelId': 'facebook/m2m100_418M',\n",
       "  'sha': '441fd5182f1298d7e39f34013ac0b905f8ff4429',\n",
       "  'lastModified': '2022-05-26T22:26:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'm2m_100',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'ast',\n",
       "   'az',\n",
       "   'ba',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'br',\n",
       "   'bs',\n",
       "   'ca',\n",
       "   'ceb',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'es',\n",
       "   'et',\n",
       "   'fa',\n",
       "   'ff',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'he',\n",
       "   'hi',\n",
       "   'hr',\n",
       "   'ht',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'id',\n",
       "   'ig',\n",
       "   'ilo',\n",
       "   'is',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'lb',\n",
       "   'lg',\n",
       "   'ln',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'ns',\n",
       "   'oc',\n",
       "   'or',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sd',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'ss',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'th',\n",
       "   'tl',\n",
       "   'tn',\n",
       "   'tr',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'wo',\n",
       "   'xh',\n",
       "   'yi',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'zu',\n",
       "   'arxiv:2010.11125',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['M2M100ForConditionalGeneration'],\n",
       "   'model_type': 'm2m_100'},\n",
       "  'id': 'facebook/m2m100_418M',\n",
       "  'downloads': 278355,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 22,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'ast',\n",
       "    'az',\n",
       "    'ba',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'br',\n",
       "    'bs',\n",
       "    'ca',\n",
       "    'ceb',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'es',\n",
       "    'et',\n",
       "    'fa',\n",
       "    'ff',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'he',\n",
       "    'hi',\n",
       "    'hr',\n",
       "    'ht',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'id',\n",
       "    'ig',\n",
       "    'ilo',\n",
       "    'is',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'lb',\n",
       "    'lg',\n",
       "    'ln',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'ns',\n",
       "    'oc',\n",
       "    'or',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sd',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'ss',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'th',\n",
       "    'tl',\n",
       "    'tn',\n",
       "    'tr',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'wo',\n",
       "    'xh',\n",
       "    'yi',\n",
       "    'yo',\n",
       "    'zh',\n",
       "    'zu'],\n",
       "   'license': 'mit',\n",
       "   'tags': None},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/stsb-roberta-base': {'modelId': 'cross-encoder/stsb-roberta-base',\n",
       "  'sha': '90a6796bd3c504b63351dad78c76ffb40e3d6e5a',\n",
       "  'lastModified': '2021-08-05T08:41:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cross-encoder/stsb-roberta-base',\n",
       "  'downloads': 277780,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'finiteautomata/beto-sentiment-analysis': {'modelId': 'finiteautomata/beto-sentiment-analysis',\n",
       "  'sha': '2d232b7b937ca0f6940f6b32ce5aaaeb012d8b38',\n",
       "  'lastModified': '2022-06-22T13:46:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'es',\n",
       "   'arxiv:2106.09462',\n",
       "   'transformers',\n",
       "   'sentiment-analysis'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'finiteautomata',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'finiteautomata/beto-sentiment-analysis',\n",
       "  'downloads': 273730,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Te quiero. Te amo.'}],\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['es'], 'tags': ['sentiment-analysis']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'albert-base-v1': {'modelId': 'albert-base-v1',\n",
       "  'sha': 'aeffd769076a5c4f83b2546aea99ca45a15a5da4',\n",
       "  'lastModified': '2021-01-13T15:08:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1909.11942',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'albert-base-v1',\n",
       "  'downloads': 269242,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['exbert'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/electra-small-discriminator': {'modelId': 'google/electra-small-discriminator',\n",
       "  'sha': '153f486d928bcfc213932f8fc91fc2e3c41af769',\n",
       "  'lastModified': '2021-04-29T15:24:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'electra',\n",
       "   'pretraining',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['ElectraForPreTraining'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'google/electra-small-discriminator',\n",
       "  'downloads': 265244,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nlptown/bert-base-multilingual-uncased-sentiment': {'modelId': 'nlptown/bert-base-multilingual-uncased-sentiment',\n",
       "  'sha': 'e06857fdb0325a7798a8fc361b417dfeec3a3b98',\n",
       "  'lastModified': '2022-04-18T16:46:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'nl',\n",
       "   'de',\n",
       "   'fr',\n",
       "   'it',\n",
       "   'es',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'nlptown',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'nlptown/bert-base-multilingual-uncased-sentiment',\n",
       "  'downloads': 259805,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 54,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'nl', 'de', 'fr', 'it', 'es'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mrm8488/t5-base-finetuned-common_gen': {'modelId': 'mrm8488/t5-base-finetuned-common_gen',\n",
       "  'sha': '5c3010b4532b7834039c65580e688e9656626835',\n",
       "  'lastModified': '2021-09-24T08:52:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:common_gen',\n",
       "   'arxiv:1910.10683',\n",
       "   'arxiv:1911.03705',\n",
       "   'transformers',\n",
       "   'common sense',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'mrm8488/t5-base-finetuned-common_gen',\n",
       "  'downloads': 258993,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'tree plant ground hole dig'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['common sense'],\n",
       "   'datasets': ['common_gen'],\n",
       "   'widget': [{'text': 'tree plant ground hole dig'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'prajjwal1/bert-tiny': {'modelId': 'prajjwal1/bert-tiny',\n",
       "  'sha': '6f75de8b60a9f8a2fdf7b69cbd86d9e64bcb3837',\n",
       "  'lastModified': '2021-10-27T18:29:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'arxiv:1908.08962',\n",
       "   'arxiv:2110.01518',\n",
       "   'transformers',\n",
       "   'BERT',\n",
       "   'MNLI',\n",
       "   'NLI',\n",
       "   'transformer',\n",
       "   'pre-training',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'prajjwal1',\n",
       "  'config': {},\n",
       "  'id': 'prajjwal1/bert-tiny',\n",
       "  'downloads': 256147,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'license': ['mit'],\n",
       "   'tags': ['BERT', 'MNLI', 'NLI', 'transformer', 'pre-training']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'google/bert_uncased_L-2_H-128_A-2': {'modelId': 'google/bert_uncased_L-2_H-128_A-2',\n",
       "  'sha': '1ae49ff827beda5996998802695c4cac8e9932c6',\n",
       "  'lastModified': '2021-05-19T17:28:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'arxiv:1908.08962',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'google/bert_uncased_L-2_H-128_A-2',\n",
       "  'downloads': 254004,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'xlnet-base-cased': {'modelId': 'xlnet-base-cased',\n",
       "  'sha': '593a21e8b79948a7f952811aa44f37d76e23d586',\n",
       "  'lastModified': '2021-09-16T09:43:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rust',\n",
       "   'xlnet',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1906.08237',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['XLNetLMHeadModel'],\n",
       "   'model_type': 'xlnet',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 250}}},\n",
       "  'id': 'xlnet-base-cased',\n",
       "  'downloads': 249967,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dalle-mini/vqgan_imagenet_f16_16384': {'modelId': 'dalle-mini/vqgan_imagenet_f16_16384',\n",
       "  'sha': '9f990ec03ea054204706013e2176bbb498ebc387',\n",
       "  'lastModified': '2022-03-01T17:28:10.000Z',\n",
       "  'tags': ['jax', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dalle-mini',\n",
       "  'config': {'architectures': ['del']},\n",
       "  'id': 'dalle-mini/vqgan_imagenet_f16_16384',\n",
       "  'downloads': 248552,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 25,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'del'}},\n",
       " 'deepset/bert-large-uncased-whole-word-masking-squad2': {'modelId': 'deepset/bert-large-uncased-whole-word-masking-squad2',\n",
       "  'sha': '8ebfa4a417e72eb0307ef9d6b2b2bf742b5ca879',\n",
       "  'lastModified': '2021-05-19T15:28:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'deepset/bert-large-uncased-whole-word-masking-squad2',\n",
       "  'downloads': 234038,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/dpr-ctx_encoder-single-nq-base': {'modelId': 'facebook/dpr-ctx_encoder-single-nq-base',\n",
       "  'sha': 'aa2a11a692b50b73fa245d59e49be796eefd888f',\n",
       "  'lastModified': '2020-11-25T16:58:35.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'dpr', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['DPRContextEncoder'], 'model_type': 'dpr'},\n",
       "  'id': 'facebook/dpr-ctx_encoder-single-nq-base',\n",
       "  'downloads': 234037,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'DPRContextEncoder',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'seyonec/PubChem10M_SMILES_BPE_450k': {'modelId': 'seyonec/PubChem10M_SMILES_BPE_450k',\n",
       "  'sha': 'c18fccd09b3326bf2d4633412c256d7db872156d',\n",
       "  'lastModified': '2021-05-20T21:02:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'seyonec',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'seyonec/PubChem10M_SMILES_BPE_450k',\n",
       "  'downloads': 228396,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dccuchile/bert-base-spanish-wwm-uncased': {'modelId': 'dccuchile/bert-base-spanish-wwm-uncased',\n",
       "  'sha': '767afcc9ffdf900341128e9e0bfe44d522461c51',\n",
       "  'lastModified': '2022-05-31T15:02:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'es',\n",
       "   'arxiv:1904.09077',\n",
       "   'arxiv:1906.01502',\n",
       "   'arxiv:1812.10464',\n",
       "   'arxiv:1901.07291',\n",
       "   'arxiv:1904.02099',\n",
       "   'arxiv:1906.01569',\n",
       "   'arxiv:1908.11828',\n",
       "   'transformers',\n",
       "   'masked-lm',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'dccuchile',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'dccuchile/bert-base-spanish-wwm-uncased',\n",
       "  'downloads': 227818,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Mi nombre es [MASK] y vivo en Nueva York.'},\n",
       "   {'text': 'El español es un idioma muy [MASK] en el mundo.'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['es'], 'tags': ['masked-lm']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'openai/clip-vit-large-patch14': {'modelId': 'openai/clip-vit-large-patch14',\n",
       "  'sha': '0993c71e8ad62658387de2714a69f723ddfffacb',\n",
       "  'lastModified': '2022-03-14T18:01:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'clip',\n",
       "   'feature-extraction',\n",
       "   'arxiv:2103.00020',\n",
       "   'arxiv:1908.04913',\n",
       "   'transformers',\n",
       "   'vision'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'openai',\n",
       "  'config': {'architectures': ['CLIPModel'], 'model_type': 'clip'},\n",
       "  'id': 'openai/clip-vit-large-patch14',\n",
       "  'downloads': 226511,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['vision']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'roberta-large-mnli': {'modelId': 'roberta-large-mnli',\n",
       "  'sha': '130fb28e151bc632feba36b7417d01ad6dd6c424',\n",
       "  'lastModified': '2021-05-20T19:32:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'roberta-large-mnli',\n",
       "  'downloads': 223665,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. </s></s> I love you.'}],\n",
       "  'likes': 24,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'mit',\n",
       "   'widget': [{'text': 'I like you. </s></s> I love you.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'airesearch/wangchanberta-base-att-spm-uncased': {'modelId': 'airesearch/wangchanberta-base-att-spm-uncased',\n",
       "  'sha': 'abe46f39cf2c911a6ad5ec8299bdf7503edc95e4',\n",
       "  'lastModified': '2022-02-16T14:42:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'camembert',\n",
       "   'fill-mask',\n",
       "   'th',\n",
       "   'arxiv:1907.11692',\n",
       "   'arxiv:1801.06146',\n",
       "   'arxiv:1808.06226',\n",
       "   'arxiv:2101.09635',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'airesearch',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'airesearch/wangchanberta-base-att-spm-uncased',\n",
       "  'downloads': 223518,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'ผู้ใช้งานท่าอากาศยานนานาชาติ<mask>มีกว่าสามล้านคน<pad>'}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'th',\n",
       "   'widget': [{'text': 'ผู้ใช้งานท่าอากาศยานนานาชาติ<mask>มีกว่าสามล้านคน<pad>'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'aubmindlab/bert-base-arabertv02': {'modelId': 'aubmindlab/bert-base-arabertv02',\n",
       "  'sha': 'b214583e9b05a7bbc024d58daeb54a1b2a2997a0',\n",
       "  'lastModified': '2022-04-06T15:24:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ar',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:OSIAN',\n",
       "   'dataset:1.5B Arabic Corpus',\n",
       "   'dataset:OSCAR Arabic Unshuffled',\n",
       "   'arxiv:2003.00104',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'aubmindlab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'aubmindlab/bert-base-arabertv02',\n",
       "  'downloads': 220476,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': ' عاصمة لبنان هي [MASK] .'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ar',\n",
       "   'datasets': ['wikipedia',\n",
       "    'OSIAN',\n",
       "    '1.5B Arabic Corpus',\n",
       "    'OSCAR Arabic Unshuffled'],\n",
       "   'widget': [{'text': ' عاصمة لبنان هي [MASK] .'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-base-italian-cased': {'modelId': 'dbmdz/bert-base-italian-cased',\n",
       "  'sha': 'bcecdad25ce7cdd99c58c4e504ab97e6ff7222cf',\n",
       "  'lastModified': '2021-05-19T14:59:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'it',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-base-italian-cased',\n",
       "  'downloads': 220307,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': \"Roma è la [MASK] d'Italia.\"},\n",
       "   {'text': 'Lo scopo della vita è [MASK].'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'it', 'license': 'mit', 'datasets': ['wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/dpr-question_encoder-single-nq-base': {'modelId': 'facebook/dpr-question_encoder-single-nq-base',\n",
       "  'sha': '7fee7988e53c713da8323b184f6015d47861a1bf',\n",
       "  'lastModified': '2020-11-25T16:59:20.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'dpr', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['DPRQuestionEncoder'], 'model_type': 'dpr'},\n",
       "  'id': 'facebook/dpr-question_encoder-single-nq-base',\n",
       "  'downloads': 220096,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/paraphrase-distilroberta-base-v1': {'modelId': 'sentence-transformers/paraphrase-distilroberta-base-v1',\n",
       "  'sha': 'de91d53e03a544451c0e18312391a3f279f7f4ef',\n",
       "  'lastModified': '2022-06-15T20:03:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/paraphrase-distilroberta-base-v1',\n",
       "  'downloads': 218993,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/pegasus-xsum': {'modelId': 'google/pegasus-xsum',\n",
       "  'sha': 'a0aa5531c00f59a32a167b75130805098b046f9c',\n",
       "  'lastModified': '2021-09-14T07:25:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'pegasus',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:1912.08777',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['PegasusForConditionalGeneration'],\n",
       "   'model_type': 'pegasus'},\n",
       "  'id': 'google/pegasus-xsum',\n",
       "  'downloads': 218500,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 39,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'tags': ['summarization']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/deberta-v2-xlarge': {'modelId': 'microsoft/deberta-v2-xlarge',\n",
       "  'sha': '30597019711d3531f994d1e21defffd0d8cd55ab',\n",
       "  'lastModified': '2022-01-13T17:21:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'deberta-v2',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'deberta',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'deberta-v2'},\n",
       "  'id': 'microsoft/deberta-v2-xlarge',\n",
       "  'downloads': 217009,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': 'deberta',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'nlpaueb/legal-bert-small-uncased': {'modelId': 'nlpaueb/legal-bert-small-uncased',\n",
       "  'sha': '0e23f7a9a39f59768ea7e09766d8ee308580fb17',\n",
       "  'lastModified': '2022-04-28T14:43:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'legal',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'fill-mask'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'nlpaueb',\n",
       "  'config': {'architectures': None, 'model_type': 'bert'},\n",
       "  'id': 'nlpaueb/legal-bert-small-uncased',\n",
       "  'downloads': 216773,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'The applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of police.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'thumbnail': 'https://i.ibb.co/p3kQ7Rw/Screenshot-2020-10-06-at-12-16-36-PM.png',\n",
       "   'tags': ['legal'],\n",
       "   'widget': [{'text': 'The applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of police.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'imone/pangu_2_6B': {'modelId': 'imone/pangu_2_6B',\n",
       "  'sha': 'f6185c345ee59518384a463350bc834ace46e557',\n",
       "  'lastModified': '2021-12-13T06:34:22.000Z',\n",
       "  'tags': ['pytorch', 'gpt_pangu', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'imone',\n",
       "  'config': {'architectures': ['GPTPanguForCausalLM'],\n",
       "   'model_type': 'gpt_pangu'},\n",
       "  'id': 'imone/pangu_2_6B',\n",
       "  'downloads': 216405,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation'}},\n",
       " 'microsoft/deberta-base': {'modelId': 'microsoft/deberta-base',\n",
       "  'sha': '7d4c0126b06bd59dccd3e48e467ed11e37b77f3f',\n",
       "  'lastModified': '2022-01-13T13:56:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rust',\n",
       "   'deberta',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'deberta-v1',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'deberta'},\n",
       "  'id': 'microsoft/deberta-base',\n",
       "  'downloads': 212662,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': 'deberta-v1',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'uer/gpt2-chinese-cluecorpussmall': {'modelId': 'uer/gpt2-chinese-cluecorpussmall',\n",
       "  'sha': 'eac5b94f575ae754c1d39100da152369a7699e6c',\n",
       "  'lastModified': '2022-02-20T04:46:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'Chinese',\n",
       "   'dataset:CLUECorpusSmall',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'uer',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 320}}},\n",
       "  'id': 'uer/gpt2-chinese-cluecorpussmall',\n",
       "  'downloads': 209231,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '这是很久之前的事情了'}],\n",
       "  'likes': 17,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'Chinese',\n",
       "   'datasets': 'CLUECorpusSmall',\n",
       "   'widget': [{'text': '这是很久之前的事情了'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/paraphrase-xlm-r-multilingual-v1': {'modelId': 'sentence-transformers/paraphrase-xlm-r-multilingual-v1',\n",
       "  'sha': '50f7fa9e273db3db51beceacc1b111e4a1a31d34',\n",
       "  'lastModified': '2022-06-15T19:25:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['XLMRobertaModel'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'sentence-transformers/paraphrase-xlm-r-multilingual-v1',\n",
       "  'downloads': 207323,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 31,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cahya/t5-base-indonesian-summarization-cased': {'modelId': 'cahya/t5-base-indonesian-summarization-cased',\n",
       "  'sha': '3afd080677efb3978dfce95a19324d91caff3064',\n",
       "  'lastModified': '2021-06-23T12:05:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'id',\n",
       "   'dataset:id_liputan6',\n",
       "   'transformers',\n",
       "   'pipeline:summarization',\n",
       "   'summarization',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'cahya',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'cahya/t5-base-indonesian-summarization-cased',\n",
       "  'downloads': 207288,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'id',\n",
       "   'tags': ['pipeline:summarization', 'summarization', 't5'],\n",
       "   'datasets': ['id_liputan6']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ckiplab/albert-tiny-chinese': {'modelId': 'ckiplab/albert-tiny-chinese',\n",
       "  'sha': 'd1edf497761caf4fdd83d2c4488132a8c56f9e3c',\n",
       "  'lastModified': '2022-05-10T03:28:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'lm-head',\n",
       "   'license:gpl-3.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'ckiplab',\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'ckiplab/albert-tiny-chinese',\n",
       "  'downloads': 205977,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '巴黎是[MASK]国的首都。'}, {'text': '生活的真谛是[MASK]。'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'thumbnail': 'https://ckip.iis.sinica.edu.tw/files/ckip_logo.png',\n",
       "   'tags': ['pytorch', 'lm-head', 'albert', 'zh'],\n",
       "   'license': 'gpl-3.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/t5-v1_1-base': {'modelId': 'google/t5-v1_1-base',\n",
       "  'sha': '650d7745bf1e502d6949b22cc19155cd656d3d4e',\n",
       "  'lastModified': '2021-06-23T01:54:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:c4',\n",
       "   'arxiv:2002.05202',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/t5-v1_1-base',\n",
       "  'downloads': 205906,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'datasets': ['c4'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allenai/specter': {'modelId': 'allenai/specter',\n",
       "  'sha': 'c15597dc3bf1f00444f1c5a59c9bb80c93499635',\n",
       "  'lastModified': '2022-06-25T16:04:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'dataset:SciDocs',\n",
       "   'arxiv:2004.07180',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'allenai/specter',\n",
       "  'downloads': 204047,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://camo.githubusercontent.com/7d080b7a769f7fdf64ac0ebeb47b039cb50be35287e3071f9d633f0fe33e7596/68747470733a2f2f692e6962622e636f2f33544331576d472f737065637465722d6c6f676f2d63726f707065642e706e67',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['SciDocs'],\n",
       "   'metrics': ['F1', 'accuracy', 'map', 'ndcg']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'emilyalsentzer/Bio_ClinicalBERT': {'modelId': 'emilyalsentzer/Bio_ClinicalBERT',\n",
       "  'sha': '41943bf7f983007123c758373c5246305cc536ec',\n",
       "  'lastModified': '2022-02-27T13:59:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'en',\n",
       "   'arxiv:1904.03323',\n",
       "   'arxiv:1901.08746',\n",
       "   'transformers',\n",
       "   'fill-mask',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'emilyalsentzer',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'emilyalsentzer/Bio_ClinicalBERT',\n",
       "  'downloads': 199851,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 27,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'tags': ['fill-mask'], 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'flair/ner-english-ontonotes-large': {'modelId': 'flair/ner-english-ontonotes-large',\n",
       "  'sha': '4ffb3596f4359f0c8799ea15bbf5dbb3b0915a53',\n",
       "  'lastModified': '2021-05-08T15:35:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'dataset:ontonotes',\n",
       "   'arxiv:2011.06993',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/ner-english-ontonotes-large',\n",
       "  'downloads': 199677,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'On September 1st George won 1 dollar while watching Game of Thrones.'}],\n",
       "  'likes': 22,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': 'en',\n",
       "   'datasets': ['ontonotes'],\n",
       "   'widget': [{'text': 'On September 1st George won 1 dollar while watching Game of Thrones.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'Jean-Baptiste/roberta-large-ner-english': {'modelId': 'Jean-Baptiste/roberta-large-ner-english',\n",
       "  'sha': 'c272484a77f6bacd3569d32936fda04555fb4006',\n",
       "  'lastModified': '2022-07-04T15:02:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'token-classification',\n",
       "   'en',\n",
       "   'dataset:conll2003',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jean-Baptiste',\n",
       "  'config': {'architectures': ['RobertaForTokenClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'Jean-Baptiste/roberta-large-ner-english',\n",
       "  'downloads': 196068,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'My name is jean-baptiste and I live in montreal'},\n",
       "   {'text': 'My name is clara and I live in berkeley, california.'},\n",
       "   {'text': 'My name is wolfgang and I live in berlin'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['conll2003'],\n",
       "   'widget': [{'text': 'My name is jean-baptiste and I live in montreal'},\n",
       "    {'text': 'My name is clara and I live in berkeley, california.'},\n",
       "    {'text': 'My name is wolfgang and I live in berlin'}],\n",
       "   'train-eval-index': [{'config': 'conll2003',\n",
       "     'task': 'token-classification',\n",
       "     'task_id': 'entity_extraction',\n",
       "     'splits': {'eval_split': 'validation'},\n",
       "     'col_mapping': {'tokens': 'tokens', 'ner_tags': 'tags'}}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'T-Systems-onsite/cross-en-de-roberta-sentence-transformer': {'modelId': 'T-Systems-onsite/cross-en-de-roberta-sentence-transformer',\n",
       "  'sha': '17bdd111a862ec99279be149fc9efa4f9122bcc1',\n",
       "  'lastModified': '2022-06-16T18:13:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'feature-extraction',\n",
       "   'de',\n",
       "   'en',\n",
       "   'dataset:STSbenchmark',\n",
       "   'arxiv:1908.10084',\n",
       "   'transformers',\n",
       "   'sentence_embedding',\n",
       "   'search',\n",
       "   'roberta',\n",
       "   'xlm-r-distilroberta-base-paraphrase-v1',\n",
       "   'paraphrase',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'T-Systems-onsite',\n",
       "  'config': {'architectures': ['XLMRobertaModel'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'T-Systems-onsite/cross-en-de-roberta-sentence-transformer',\n",
       "  'downloads': 193158,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['de', 'en'],\n",
       "   'license': 'mit',\n",
       "   'tags': ['sentence_embedding',\n",
       "    'search',\n",
       "    'pytorch',\n",
       "    'xlm-roberta',\n",
       "    'roberta',\n",
       "    'xlm-r-distilroberta-base-paraphrase-v1',\n",
       "    'paraphrase'],\n",
       "   'datasets': ['STSbenchmark'],\n",
       "   'metrics': ['Spearman’s rank correlation', 'cosine similarity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 't5-11b': {'modelId': 't5-11b',\n",
       "  'sha': '952e4a1e7643d8e4dbebcae9b4d4f8f7507b523b',\n",
       "  'lastModified': '2022-03-18T17:24:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'fr',\n",
       "   'ro',\n",
       "   'de',\n",
       "   'dataset:c4',\n",
       "   'arxiv:1805.12471',\n",
       "   'arxiv:1708.00055',\n",
       "   'arxiv:1704.05426',\n",
       "   'arxiv:1606.05250',\n",
       "   'arxiv:1808.09121',\n",
       "   'arxiv:1810.12885',\n",
       "   'arxiv:1905.10044',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['T5WithLMHeadModel'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 't5-11b',\n",
       "  'downloads': 192660,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'fr', 'ro', 'de'],\n",
       "   'datasets': ['c4'],\n",
       "   'tags': ['summarization', 'translation'],\n",
       "   'license': 'apache-2.0',\n",
       "   'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelWithLMHead',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/blenderbot-400M-distill': {'modelId': 'facebook/blenderbot-400M-distill',\n",
       "  'sha': 'a2084cb58dd4810f45302724dd07c68051fe9ed3',\n",
       "  'lastModified': '2022-05-16T19:39:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'blenderbot',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:blended_skill_talk',\n",
       "   'arxiv:2004.13637',\n",
       "   'transformers',\n",
       "   'convAI',\n",
       "   'conversational',\n",
       "   'facebook',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['BlenderbotForConditionalGeneration'],\n",
       "   'model_type': 'blenderbot'},\n",
       "  'id': 'facebook/blenderbot-400M-distill',\n",
       "  'downloads': 190103,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 36,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'thumbnail': None,\n",
       "   'tags': ['convAI', 'conversational', 'facebook'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['blended_skill_talk'],\n",
       "   'metrics': ['perplexity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'princeton-nlp/unsup-simcse-bert-base-uncased': {'modelId': 'princeton-nlp/unsup-simcse-bert-base-uncased',\n",
       "  'sha': '6504ae026e02a1464538d443b15e36afc318e034',\n",
       "  'lastModified': '2021-05-20T02:57:45.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'princeton-nlp',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'princeton-nlp/unsup-simcse-bert-base-uncased',\n",
       "  'downloads': 188550,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'vinai/bertweet-base': {'modelId': 'vinai/bertweet-base',\n",
       "  'sha': 'f9365bd897a63399564af0859aa981deb6deb0f3',\n",
       "  'lastModified': '2022-06-08T04:43:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'vinai',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'vinai/bertweet-base',\n",
       "  'downloads': 188493,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dmis-lab/biobert-v1.1': {'modelId': 'dmis-lab/biobert-v1.1',\n",
       "  'sha': '551ca18efd7f052c8dfa0b01c94c2a8e68bc5488',\n",
       "  'lastModified': '2021-05-19T16:03:17.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'dmis-lab',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'dmis-lab/biobert-v1.1',\n",
       "  'downloads': 184048,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pyannote/segmentation': {'modelId': 'pyannote/segmentation',\n",
       "  'sha': '56d5828a2dc341a294417c01e7dcdd0da4cf461a',\n",
       "  'lastModified': '2022-03-23T09:25:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'dataset:ami',\n",
       "   'dataset:dihard',\n",
       "   'dataset:voxconverse',\n",
       "   'arxiv:2104.04045',\n",
       "   'pyannote-audio',\n",
       "   'pyannote',\n",
       "   'pyannote-audio-model',\n",
       "   'audio',\n",
       "   'voice',\n",
       "   'speech',\n",
       "   'speaker',\n",
       "   'speaker-segmentation',\n",
       "   'voice-activity-detection',\n",
       "   'overlapped-speech-detection',\n",
       "   'resegmentation',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'voice-activity-detection',\n",
       "  'private': False,\n",
       "  'author': 'pyannote',\n",
       "  'config': None,\n",
       "  'id': 'pyannote/segmentation',\n",
       "  'downloads': 183970,\n",
       "  'library_name': 'pyannote-audio',\n",
       "  'likes': 20,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['pyannote',\n",
       "    'pyannote-audio',\n",
       "    'pyannote-audio-model',\n",
       "    'audio',\n",
       "    'voice',\n",
       "    'speech',\n",
       "    'speaker',\n",
       "    'speaker-segmentation',\n",
       "    'voice-activity-detection',\n",
       "    'overlapped-speech-detection',\n",
       "    'resegmentation'],\n",
       "   'datasets': ['ami', 'dihard', 'voxconverse'],\n",
       "   'license': 'mit',\n",
       "   'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'cross-encoder/ms-marco-MiniLM-L-6-v2': {'modelId': 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
       "  'sha': 'b2cfda50a1a9fc7919e7444afbb52610d268af92',\n",
       "  'lastModified': '2021-08-05T08:39:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
       "  'downloads': 183868,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'echarlaix/bert-base-uncased-sst2-acc91.1-d37-hybrid': {'modelId': 'echarlaix/bert-base-uncased-sst2-acc91.1-d37-hybrid',\n",
       "  'sha': 'f9c8e9f03396500a107dbe97024d7efa23f57e69',\n",
       "  'lastModified': '2022-07-04T09:04:59.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:sst2',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'echarlaix',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'echarlaix/bert-base-uncased-sst2-acc91.1-d37-hybrid',\n",
       "  'downloads': 182079,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['text-classification'],\n",
       "   'datasets': ['sst2'],\n",
       "   'metrics': ['accuracy']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bhadresh-savani/bert-base-go-emotion': {'modelId': 'bhadresh-savani/bert-base-go-emotion',\n",
       "  'sha': '6ecebb2840243665ab089020504c52e086862848',\n",
       "  'lastModified': '2021-11-29T10:43:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'en',\n",
       "   'dataset:go_emotions',\n",
       "   'transformers',\n",
       "   'text-classification',\n",
       "   'go-emotion',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'bhadresh-savani',\n",
       "  'config': {'architectures': ['DistilBertForMultilabelSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'bhadresh-savani/bert-base-go-emotion',\n",
       "  'downloads': 181018,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'thumbnail': 'https://avatars3.githubusercontent.com/u/32437151?s=460&u=4ec59abc8d21d5feea3dab323d23a5860e6996a4&v=4',\n",
       "   'tags': ['text-classification', 'go-emotion', 'pytorch'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['go_emotions'],\n",
       "   'metrics': ['Accuracy']},\n",
       "  'transformersInfo': {'auto_model': 'DistilBertForMultilabelSequenceClassification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'j-hartmann/emotion-english-distilroberta-base': {'modelId': 'j-hartmann/emotion-english-distilroberta-base',\n",
       "  'sha': 'd23807173703d44b48d60ca252664f60d0d46563',\n",
       "  'lastModified': '2022-06-09T12:43:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'distilroberta',\n",
       "   'sentiment',\n",
       "   'emotion',\n",
       "   'twitter',\n",
       "   'reddit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'j-hartmann',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'j-hartmann/emotion-english-distilroberta-base',\n",
       "  'downloads': 179816,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': \"Oh wow. I didn't know that.\"},\n",
       "   {'text': 'This movie always makes me cry..'},\n",
       "   {'text': 'Oh Happy Day'}],\n",
       "  'likes': 29,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['distilroberta', 'sentiment', 'emotion', 'twitter', 'reddit'],\n",
       "   'widget': [{'text': \"Oh wow. I didn't know that.\"},\n",
       "    {'text': 'This movie always makes me cry..'},\n",
       "    {'text': 'Oh Happy Day'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'prithivida/parrot_paraphraser_on_T5': {'modelId': 'prithivida/parrot_paraphraser_on_T5',\n",
       "  'sha': '9f32aa1e456e8e8a90d97e8673365f3090fa49fa',\n",
       "  'lastModified': '2021-05-18T07:53:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'prithivida',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'prithivida/parrot_paraphraser_on_T5',\n",
       "  'downloads': 175057,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 18,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'monologg/bert-base-cased-goemotions-original': {'modelId': 'monologg/bert-base-cased-goemotions-original',\n",
       "  'sha': '13c44c849132f82bb61188d909a574badffb27a3',\n",
       "  'lastModified': '2021-05-19T23:48:33.000Z',\n",
       "  'tags': ['pytorch', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'monologg',\n",
       "  'config': {'architectures': ['BertForMultiLabelClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'monologg/bert-base-cased-goemotions-original',\n",
       "  'downloads': 173202,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'BertForMultiLabelClassification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/bigbird-roberta-base': {'modelId': 'google/bigbird-roberta-base',\n",
       "  'sha': '5a145f7852cba9bd431386a58137bf8a29903b90',\n",
       "  'lastModified': '2021-06-02T14:30:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'big_bird',\n",
       "   'pretraining',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:cc_news',\n",
       "   'arxiv:2007.14062',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['BigBirdForPreTraining'],\n",
       "   'model_type': 'big_bird'},\n",
       "  'id': 'google/bigbird-roberta-base',\n",
       "  'downloads': 172226,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 14,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia', 'cc_news']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-large-uncased-whole-word-masking': {'modelId': 'bert-large-uncased-whole-word-masking',\n",
       "  'sha': '90d3333009848fae4860a5338419d17f70be940c',\n",
       "  'lastModified': '2021-05-18T16:37:36.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-large-uncased-whole-word-masking',\n",
       "  'downloads': 172022,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 't5-large': {'modelId': 't5-large',\n",
       "  'sha': '0c716e16629cc307ee64f3870869c1227c24fef1',\n",
       "  'lastModified': '2022-03-18T17:24:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'fr',\n",
       "   'ro',\n",
       "   'de',\n",
       "   'dataset:c4',\n",
       "   'arxiv:1805.12471',\n",
       "   'arxiv:1708.00055',\n",
       "   'arxiv:1704.05426',\n",
       "   'arxiv:1606.05250',\n",
       "   'arxiv:1808.09121',\n",
       "   'arxiv:1810.12885',\n",
       "   'arxiv:1905.10044',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['T5WithLMHeadModel'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 't5-large',\n",
       "  'downloads': 171229,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 15,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'fr', 'ro', 'de'],\n",
       "   'datasets': ['c4'],\n",
       "   'tags': ['summarization', 'translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelWithLMHead',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'neuralmind/bert-base-portuguese-cased': {'modelId': 'neuralmind/bert-base-portuguese-cased',\n",
       "  'sha': '94d69c95f98f7d5b2a8700c420230ae10def0baa',\n",
       "  'lastModified': '2022-06-14T14:37:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'pt',\n",
       "   'dataset:brWaC',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'neuralmind',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'neuralmind/bert-base-portuguese-cased',\n",
       "  'downloads': 167156,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 28,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'pt',\n",
       "   'license': 'mit',\n",
       "   'tags': ['bert', 'pytorch'],\n",
       "   'datasets': ['brWaC']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allegro/herbert-base-cased': {'modelId': 'allegro/herbert-base-cased',\n",
       "  'sha': '50e33e0567be0c0b313832314c586e3df0dc2297',\n",
       "  'lastModified': '2022-06-09T11:36:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'pl',\n",
       "   'transformers',\n",
       "   'herbert',\n",
       "   'license:cc-by-4.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'allegro',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'allegro/herbert-base-cased',\n",
       "  'downloads': 164340,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'pl', 'tags': ['herbert'], 'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-large-cased-finetuned-conll03-english': {'modelId': 'dbmdz/bert-large-cased-finetuned-conll03-english',\n",
       "  'sha': 'f2482bf01f5da0f0eb8e183ffd8cc3885aa90b14',\n",
       "  'lastModified': '2021-05-19T15:17:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-large-cased-finetuned-conll03-english',\n",
       "  'downloads': 159877,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flair/pos-english-fast': {'modelId': 'flair/pos-english-fast',\n",
       "  'sha': '78bf413a631e2de4cb977e1f2794295d981e4c13',\n",
       "  'lastModified': '2021-03-02T22:19:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'dataset:ontonotes',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/pos-english-fast',\n",
       "  'downloads': 157175,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'I love Berlin.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': 'en',\n",
       "   'datasets': ['ontonotes'],\n",
       "   'widget': [{'text': 'I love Berlin.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'finiteautomata/bertweet-base-sentiment-analysis': {'modelId': 'finiteautomata/bertweet-base-sentiment-analysis',\n",
       "  'sha': 'cf6b0f60e84096e077c171fe3176093674370291',\n",
       "  'lastModified': '2022-06-23T13:01:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'arxiv:2106.09462',\n",
       "   'transformers',\n",
       "   'sentiment-analysis'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'finiteautomata',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'finiteautomata/bertweet-base-sentiment-analysis',\n",
       "  'downloads': 153824,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 17,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'], 'tags': ['sentiment-analysis']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'EleutherAI/gpt-neo-125M': {'modelId': 'EleutherAI/gpt-neo-125M',\n",
       "  'sha': 'b559e35121e91087f94903c07213d208d2412f68',\n",
       "  'lastModified': '2021-12-31T13:46:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'gpt_neo',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'dataset:The Pile',\n",
       "   'transformers',\n",
       "   'text generation',\n",
       "   'causal-lm',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'EleutherAI',\n",
       "  'config': {'architectures': ['GPTNeoForCausalLM'], 'model_type': 'gpt_neo'},\n",
       "  'id': 'EleutherAI/gpt-neo-125M',\n",
       "  'downloads': 149642,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 26,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['text generation', 'pytorch', 'causal-lm'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['The Pile']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ramsrigouthamg/t5-large-paraphraser-diverse-high-quality': {'modelId': 'ramsrigouthamg/t5-large-paraphraser-diverse-high-quality',\n",
       "  'sha': '443d721ecccee1cb38cce6f50cfd6c15e44e6ea0',\n",
       "  'lastModified': '2021-09-21T05:21:49.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'ramsrigouthamg',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'ramsrigouthamg/t5-large-paraphraser-diverse-high-quality',\n",
       "  'downloads': 148487,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'oliverguhr/fullstop-punctuation-multilang-large': {'modelId': 'oliverguhr/fullstop-punctuation-multilang-large',\n",
       "  'sha': '4740a83c496dc2416c0cf8ae3c6572dfb6851228',\n",
       "  'lastModified': '2022-06-09T11:51:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'token-classification',\n",
       "   'en',\n",
       "   'de',\n",
       "   'fr',\n",
       "   'it',\n",
       "   'dataset:wmt/europarl',\n",
       "   'transformers',\n",
       "   'punctuation prediction',\n",
       "   'punctuation',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'oliverguhr',\n",
       "  'config': {'architectures': ['XLMRobertaForTokenClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'oliverguhr/fullstop-punctuation-multilang-large',\n",
       "  'downloads': 138173,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Ho sentito che ti sei laureata il che mi fa molto piacere',\n",
       "    'example_title': 'Italian'},\n",
       "   {'text': 'Tous les matins vers quatre heures mon père ouvrait la porte de ma chambre',\n",
       "    'example_title': 'French'},\n",
       "   {'text': 'Ist das eine Frage Frau Müller', 'example_title': 'German'},\n",
       "   {'text': 'Yet she blushed as if with guilt when Cynthia reading her thoughts said to her one day Molly youre very glad to get rid of us are not you',\n",
       "    'example_title': 'English'}],\n",
       "  'likes': 28,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'de', 'fr', 'it'],\n",
       "   'tags': ['punctuation prediction', 'punctuation'],\n",
       "   'datasets': 'wmt/europarl',\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': 'Ho sentito che ti sei laureata il che mi fa molto piacere',\n",
       "     'example_title': 'Italian'},\n",
       "    {'text': 'Tous les matins vers quatre heures mon père ouvrait la porte de ma chambre',\n",
       "     'example_title': 'French'},\n",
       "    {'text': 'Ist das eine Frage Frau Müller', 'example_title': 'German'},\n",
       "    {'text': 'Yet she blushed as if with guilt when Cynthia reading her thoughts said to her one day Molly youre very glad to get rid of us are not you',\n",
       "     'example_title': 'English'}],\n",
       "   'metrics': ['f1']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'huggingface/CodeBERTa-small-v1': {'modelId': 'huggingface/CodeBERTa-small-v1',\n",
       "  'sha': 'e93b5898cff07f03f1c1c09cde284d1b85962363',\n",
       "  'lastModified': '2022-06-27T15:48:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'code',\n",
       "   'dataset:code_search_net',\n",
       "   'arxiv:1909.09436',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'huggingface',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'huggingface/CodeBERTa-small-v1',\n",
       "  'downloads': 137341,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'code',\n",
       "   'thumbnail': 'https://cdn-media.huggingface.co/CodeBERTa/CodeBERTa.png',\n",
       "   'datasets': ['code_search_net']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wav2vec2-large-960h': {'modelId': 'facebook/wav2vec2-large-960h',\n",
       "  'sha': 'bdeaacdf88f7a155f50a2704bc967aa81fbbb2ab',\n",
       "  'lastModified': '2022-04-05T16:40:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'en',\n",
       "   'dataset:librispeech_asr',\n",
       "   'arxiv:2006.11477',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'facebook/wav2vec2-large-960h',\n",
       "  'downloads': 136715,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['librispeech_asr'],\n",
       "   'tags': ['speech'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'uer/albert-base-chinese-cluecorpussmall': {'modelId': 'uer/albert-base-chinese-cluecorpussmall',\n",
       "  'sha': 'f26cf2e20801f5f9b4974eda54f4155159de7104',\n",
       "  'lastModified': '2022-02-20T08:46:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'Chinese',\n",
       "   'dataset:CLUECorpusSmall',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'uer',\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'uer/albert-base-chinese-cluecorpussmall',\n",
       "  'downloads': 134960,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '中国的首都是[MASK]京'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'Chinese',\n",
       "   'datasets': 'CLUECorpusSmall',\n",
       "   'widget': [{'text': '中国的首都是[MASK]京'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cmarkea/distilcamembert-base-sentiment': {'modelId': 'cmarkea/distilcamembert-base-sentiment',\n",
       "  'sha': 'b7804e295dc3cf2aa8ce8cff83f22e0bdd249558',\n",
       "  'lastModified': '2022-05-24T15:56:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'camembert',\n",
       "   'text-classification',\n",
       "   'fr',\n",
       "   'dataset:amazon_reviews_multi',\n",
       "   'dataset:allocine',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cmarkea',\n",
       "  'config': {'architectures': ['CamembertForSequenceClassification'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'cmarkea/distilcamembert-base-sentiment',\n",
       "  'downloads': 134584,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': \"Je pensais lire un livre nul, mais finalement je l'ai trouvé super !\"},\n",
       "   {'text': \"Cette banque est très bien, mais elle n'offre pas les services de paiements sans contact.\"},\n",
       "   {'text': 'Cette banque est très bien et elle offre en plus les services de paiements sans contact.'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['amazon_reviews_multi', 'allocine'],\n",
       "   'widget': [{'text': \"Je pensais lire un livre nul, mais finalement je l'ai trouvé super !\"},\n",
       "    {'text': \"Cette banque est très bien, mais elle n'offre pas les services de paiements sans contact.\"},\n",
       "    {'text': 'Cette banque est très bien et elle offre en plus les services de paiements sans contact.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'tuner007/pegasus_paraphrase': {'modelId': 'tuner007/pegasus_paraphrase',\n",
       "  'sha': '0159e2949ca73657a2f1329898f51b7bb53b9ab2',\n",
       "  'lastModified': '2021-03-22T21:11:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'pegasus',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'paraphrasing',\n",
       "   'seq2seq',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'tuner007',\n",
       "  'config': {'architectures': ['PegasusForConditionalGeneration'],\n",
       "   'model_type': 'pegasus'},\n",
       "  'id': 'tuner007/pegasus_paraphrase',\n",
       "  'downloads': 133980,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 50,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['pegasus', 'paraphrasing', 'seq2seq']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/allenai-specter': {'modelId': 'sentence-transformers/allenai-specter',\n",
       "  'sha': '29f9f45ff2a85fe9dfe8ce2cef3d8ec4e65c5f37',\n",
       "  'lastModified': '2022-06-15T21:31:20.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/allenai-specter',\n",
       "  'downloads': 133673,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/mbart-large-50-one-to-many-mmt': {'modelId': 'facebook/mbart-large-50-one-to-many-mmt',\n",
       "  'sha': '3cc64aaf129efb58cdc6345618b39ce776d888b4',\n",
       "  'lastModified': '2022-05-26T22:28:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'mbart',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'ar',\n",
       "   'cs',\n",
       "   'de',\n",
       "   'en',\n",
       "   'es',\n",
       "   'et',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'gu',\n",
       "   'hi',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'kk',\n",
       "   'ko',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'si',\n",
       "   'tr',\n",
       "   'vi',\n",
       "   'zh',\n",
       "   'af',\n",
       "   'az',\n",
       "   'bn',\n",
       "   'fa',\n",
       "   'he',\n",
       "   'hr',\n",
       "   'id',\n",
       "   'ka',\n",
       "   'km',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'th',\n",
       "   'tl',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'xh',\n",
       "   'gl',\n",
       "   'sl',\n",
       "   'arxiv:2008.00401',\n",
       "   'transformers',\n",
       "   'mbart-50',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['MBartForConditionalGeneration'],\n",
       "   'model_type': 'mbart'},\n",
       "  'id': 'facebook/mbart-large-50-one-to-many-mmt',\n",
       "  'downloads': 132999,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'ar',\n",
       "    'cs',\n",
       "    'de',\n",
       "    'en',\n",
       "    'es',\n",
       "    'et',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'gu',\n",
       "    'hi',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'kk',\n",
       "    'ko',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'si',\n",
       "    'tr',\n",
       "    'vi',\n",
       "    'zh',\n",
       "    'af',\n",
       "    'az',\n",
       "    'bn',\n",
       "    'fa',\n",
       "    'he',\n",
       "    'hr',\n",
       "    'id',\n",
       "    'ka',\n",
       "    'km',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'th',\n",
       "    'tl',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'xh',\n",
       "    'gl',\n",
       "    'sl'],\n",
       "   'tags': ['mbart-50']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/ms-marco-MiniLM-L-2-v2': {'modelId': 'cross-encoder/ms-marco-MiniLM-L-2-v2',\n",
       "  'sha': 'f4db9595e5310ba9e0cfbf391154583933b533eb',\n",
       "  'lastModified': '2021-08-05T08:39:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'cross-encoder/ms-marco-MiniLM-L-2-v2',\n",
       "  'downloads': 131564,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/hubert-large-ls960-ft': {'modelId': 'facebook/hubert-large-ls960-ft',\n",
       "  'sha': 'ece5fabbf034c1073acae96d5401b25be96709d8',\n",
       "  'lastModified': '2022-05-24T10:43:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'hubert',\n",
       "   'automatic-speech-recognition',\n",
       "   'en',\n",
       "   'dataset:libri-light',\n",
       "   'dataset:librispeech_asr',\n",
       "   'arxiv:2106.07447',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'audio',\n",
       "   'hf-asr-leaderboard',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['HubertForCTC'], 'model_type': 'hubert'},\n",
       "  'id': 'facebook/hubert-large-ls960-ft',\n",
       "  'downloads': 131404,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 16,\n",
       "  'model-index': [{'name': 'hubert-large-ls960-ft',\n",
       "    'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'LibriSpeech (clean)',\n",
       "       'type': 'librispeech_asr',\n",
       "       'config': 'clean',\n",
       "       'split': 'test',\n",
       "       'args': {'language': 'en'}},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 1.9}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['libri-light', 'librispeech_asr'],\n",
       "   'tags': ['speech',\n",
       "    'audio',\n",
       "    'automatic-speech-recognition',\n",
       "    'hf-asr-leaderboard'],\n",
       "   'license': 'apache-2.0',\n",
       "   'model-index': [{'name': 'hubert-large-ls960-ft',\n",
       "     'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'LibriSpeech (clean)',\n",
       "        'type': 'librispeech_asr',\n",
       "        'config': 'clean',\n",
       "        'split': 'test',\n",
       "        'args': {'language': 'en'}},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 1.9}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/electra-base-discriminator': {'modelId': 'google/electra-base-discriminator',\n",
       "  'sha': '1b48ef100dac4676d84125a8a7b7ab7c51e00386',\n",
       "  'lastModified': '2021-04-30T07:33:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'electra',\n",
       "   'pretraining',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['ElectraForPreTraining'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'google/electra-base-discriminator',\n",
       "  'downloads': 130555,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cl-tohoku/bert-base-japanese': {'modelId': 'cl-tohoku/bert-base-japanese',\n",
       "  'sha': '5dc6dbba88a42d21da3b71025c109c42462307f2',\n",
       "  'lastModified': '2021-09-23T13:45:36.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ja',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'cl-tohoku',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'cl-tohoku/bert-base-japanese',\n",
       "  'downloads': 130402,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '東北大学で[MASK]の研究をしています。'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'datasets': ['wikipedia'],\n",
       "   'widget': [{'text': '東北大学で[MASK]の研究をしています。'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flexudy/t5-base-multi-sentence-doctor': {'modelId': 'flexudy/t5-base-multi-sentence-doctor',\n",
       "  'sha': '85ef24d555e2e6cabd5ce8264e9ce1627c406bad',\n",
       "  'lastModified': '2020-12-11T23:33:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'flexudy',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'flexudy/t5-base-multi-sentence-doctor',\n",
       "  'downloads': 129272,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ramsrigouthamg/t5_sentence_paraphraser': {'modelId': 'ramsrigouthamg/t5_sentence_paraphraser',\n",
       "  'sha': '6887902ca669ce785cb8a01b3425e843011bc110',\n",
       "  'lastModified': '2021-06-23T13:47:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'ramsrigouthamg',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'ramsrigouthamg/t5_sentence_paraphraser',\n",
       "  'downloads': 127843,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'castorini/afriberta_large': {'modelId': 'castorini/afriberta_large',\n",
       "  'sha': 'e74edb9488208f8a2aeb69be4c16d179ab385564',\n",
       "  'lastModified': '2022-06-10T12:05:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'om',\n",
       "   'am',\n",
       "   'rw',\n",
       "   'rn',\n",
       "   'ha',\n",
       "   'ig',\n",
       "   'pcm',\n",
       "   'so',\n",
       "   'sw',\n",
       "   'ti',\n",
       "   'yo',\n",
       "   'multilingual',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'castorini',\n",
       "  'config': {'architectures': ['XLMRobertaForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'castorini/afriberta_large',\n",
       "  'downloads': 125304,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['om',\n",
       "    'am',\n",
       "    'rw',\n",
       "    'rn',\n",
       "    'ha',\n",
       "    'ig',\n",
       "    'pcm',\n",
       "    'so',\n",
       "    'sw',\n",
       "    'ti',\n",
       "    'yo',\n",
       "    'multilingual']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mrm8488/t5-base-finetuned-summarize-news': {'modelId': 'mrm8488/t5-base-finetuned-summarize-news',\n",
       "  'sha': 'ada499546852c489d6327cae23439ec309f6869f',\n",
       "  'lastModified': '2022-01-18T15:07:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'news',\n",
       "   'summary',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'mrm8488/t5-base-finetuned-summarize-news',\n",
       "  'downloads': 125079,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'tags': ['news', 'summary']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/all-distilroberta-v1': {'modelId': 'sentence-transformers/all-distilroberta-v1',\n",
       "  'sha': '073209d43b6df0d157f306d61bd9af92939be0c5',\n",
       "  'lastModified': '2022-06-21T14:56:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'arxiv:1904.06472',\n",
       "   'arxiv:2102.07033',\n",
       "   'arxiv:2104.08727',\n",
       "   'arxiv:1704.05179',\n",
       "   'arxiv:1810.09305',\n",
       "   'sentence-transformers',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/all-distilroberta-v1',\n",
       "  'downloads': 124608,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allenai/longformer-base-4096': {'modelId': 'allenai/longformer-base-4096',\n",
       "  'sha': 'e351d9d5da3eed48886f39eed7b64014debe4925',\n",
       "  'lastModified': '2021-03-10T02:30:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rust',\n",
       "   'longformer',\n",
       "   'arxiv:2004.05150',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'model_type': 'longformer'},\n",
       "  'id': 'allenai/longformer-base-4096',\n",
       "  'downloads': 124267,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 30,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'tals/albert-xlarge-vitaminc-mnli': {'modelId': 'tals/albert-xlarge-vitaminc-mnli',\n",
       "  'sha': '4c79eb5353f6104eb148d9221560c913f45677c7',\n",
       "  'lastModified': '2022-06-24T01:33:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'albert',\n",
       "   'text-classification',\n",
       "   'python',\n",
       "   'dataset:fever',\n",
       "   'dataset:glue',\n",
       "   'dataset:multi_nli',\n",
       "   'dataset:tals/vitaminc',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'tals',\n",
       "  'config': {'architectures': ['AlbertForSequenceClassification'],\n",
       "   'model_type': 'albert'},\n",
       "  'id': 'tals/albert-xlarge-vitaminc-mnli',\n",
       "  'downloads': 123954,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'python',\n",
       "   'datasets': ['fever', 'glue', 'multi_nli', 'tals/vitaminc']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'valhalla/distilbart-mnli-12-1': {'modelId': 'valhalla/distilbart-mnli-12-1',\n",
       "  'sha': '506336d4214470e3b3b36021358daae28e25ceac',\n",
       "  'lastModified': '2021-06-14T10:27:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bart',\n",
       "   'text-classification',\n",
       "   'dataset:mnli',\n",
       "   'transformers',\n",
       "   'distilbart',\n",
       "   'distilbart-mnli',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'valhalla',\n",
       "  'config': {'architectures': ['BartForSequenceClassification'],\n",
       "   'model_type': 'bart'},\n",
       "  'id': 'valhalla/distilbart-mnli-12-1',\n",
       "  'downloads': 123703,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system’s largest world. Konstantin Batygin did not set out to solve one of the solar system’s most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system’s missing “Planet Nine,” spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn’t rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: “Oh! This is how Europa formed.” Europa is one of Jupiter’s four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the Côte d’Azur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system’s formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['mnli'],\n",
       "   'tags': ['distilbart', 'distilbart-mnli'],\n",
       "   'pipeline_tag': 'zero-shot-classification'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'SEBIS/code_trans_t5_small_program_synthese_transfer_learning_finetune': {'modelId': 'SEBIS/code_trans_t5_small_program_synthese_transfer_learning_finetune',\n",
       "  'sha': 'cf3d414acf70f8f8e68108a2efde164b129e6bfa',\n",
       "  'lastModified': '2022-06-27T20:56:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'feature-extraction',\n",
       "   'arxiv:2104.02443',\n",
       "   'arxiv:1910.09700',\n",
       "   'arxiv:2105.09680',\n",
       "   'transformers',\n",
       "   'summarization'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'SEBIS',\n",
       "  'config': {'architectures': ['T5Model'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'max_length': 512,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'program synthesis: '}}},\n",
       "  'id': 'SEBIS/code_trans_t5_small_program_synthese_transfer_learning_finetune',\n",
       "  'downloads': 120430,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'you are given an array of numbers a and a number b , compute the difference of elements in a and b'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['summarization'],\n",
       "   'widget': [{'text': 'you are given an array of numbers a and a number b , compute the difference of elements in a and b'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/dpr-reader-single-nq-base': {'modelId': 'facebook/dpr-reader-single-nq-base',\n",
       "  'sha': '5114ec3299284784702848f7d4e598c59df1a35c',\n",
       "  'lastModified': '2020-11-25T16:59:53.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'dpr', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['DPRReader'], 'model_type': 'dpr'},\n",
       "  'id': 'facebook/dpr-reader-single-nq-base',\n",
       "  'downloads': 120154,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'DPRReader',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2': {'modelId': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
       "  'sha': 'ef15aed8b328d308d7237b9bf15269f2cd19e268',\n",
       "  'lastModified': '2022-06-15T19:38:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['XLMRobertaModel'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
       "  'downloads': 119720,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 15,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mrm8488/t5-base-finetuned-span-sentiment-extraction': {'modelId': 'mrm8488/t5-base-finetuned-span-sentiment-extraction',\n",
       "  'sha': '04a3fe1f7373c1f33b82e9fb06d2b2635e0fc5a0',\n",
       "  'lastModified': '2021-08-23T21:29:49.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'sentiment',\n",
       "   'extracion',\n",
       "   'passage',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'mrm8488/t5-base-finetuned-span-sentiment-extraction',\n",
       "  'downloads': 119013,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'question: positive context: On the monday, so i wont be able to be with you! i love you'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['sentiment', 'extracion', 'passage'],\n",
       "   'widget': [{'text': 'question: positive context: On the monday, so i wont be able to be with you! i love you'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'prithivida/grammar_error_correcter_v1': {'modelId': 'prithivida/grammar_error_correcter_v1',\n",
       "  'sha': '28f92ee33c2512814c22268b056a725364dae143',\n",
       "  'lastModified': '2021-07-04T10:44:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'prithivida',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'prithivida/grammar_error_correcter_v1',\n",
       "  'downloads': 118737,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 14,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-de-en': {'modelId': 'Helsinki-NLP/opus-mt-de-en',\n",
       "  'sha': '6137149949ac01d19d8eeef6e35d32221dabc8e4',\n",
       "  'lastModified': '2021-09-09T21:30:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'de',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-de-en',\n",
       "  'downloads': 116939,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'EleutherAI/gpt-neo-1.3B': {'modelId': 'EleutherAI/gpt-neo-1.3B',\n",
       "  'sha': '797174552ae47f449ab70b684cabcb6603e5e85e',\n",
       "  'lastModified': '2021-12-31T13:48:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'gpt_neo',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'dataset:the_pile',\n",
       "   'transformers',\n",
       "   'text generation',\n",
       "   'causal-lm',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'EleutherAI',\n",
       "  'config': {'architectures': ['GPTNeoForCausalLM'],\n",
       "   'model_type': 'gpt_neo',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50,\n",
       "     'temperature': 0.9}}},\n",
       "  'id': 'EleutherAI/gpt-neo-1.3B',\n",
       "  'downloads': 115918,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 39,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['text generation', 'pytorch', 'causal-lm'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['the_pile']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'digitalepidemiologylab/covid-twitter-bert': {'modelId': 'digitalepidemiologylab/covid-twitter-bert',\n",
       "  'sha': '945b4ea68241df3ccb8554cd1927ba81d2c9ecaa',\n",
       "  'lastModified': '2021-05-19T15:52:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'Twitter',\n",
       "   'COVID-19',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'digitalepidemiologylab',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'digitalepidemiologylab/covid-twitter-bert',\n",
       "  'downloads': 115096,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://raw.githubusercontent.com/digitalepidemiologylab/covid-twitter-bert/master/images/COVID-Twitter-BERT_small.png',\n",
       "   'tags': ['Twitter', 'COVID-19'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'deepset/gbert-base': {'modelId': 'deepset/gbert-base',\n",
       "  'sha': '4a45e506eccc3405ed2e2a0502995d3f7e483509',\n",
       "  'lastModified': '2022-02-17T14:05:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'fill-mask',\n",
       "   'de',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:OPUS',\n",
       "   'dataset:OpenLegalData',\n",
       "   'arxiv:2010.10906',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['BertForMaskedLM']},\n",
       "  'id': 'deepset/gbert-base',\n",
       "  'downloads': 114931,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['wikipedia', 'OPUS', 'OpenLegalData']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask'}},\n",
       " 'sentence-transformers/distilbert-multilingual-nli-stsb-quora-ranking': {'modelId': 'sentence-transformers/distilbert-multilingual-nli-stsb-quora-ranking',\n",
       "  'sha': '8ae74eb0fbe7c8d82bb3d1a91fca56f352074e7f',\n",
       "  'lastModified': '2022-06-15T19:34:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/distilbert-multilingual-nli-stsb-quora-ranking',\n",
       "  'downloads': 112322,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-base-turkish-cased': {'modelId': 'dbmdz/bert-base-turkish-cased',\n",
       "  'sha': 'bd38a3ecfe5d183400a573b1903e940d8d34902b',\n",
       "  'lastModified': '2021-05-19T15:14:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'tr',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-base-turkish-cased',\n",
       "  'downloads': 111865,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'tr', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/paraphrase-distilroberta-base-v2': {'modelId': 'sentence-transformers/paraphrase-distilroberta-base-v2',\n",
       "  'sha': 'd9461390caf1e64923d00bc55fa02d3c1ed2b9e5',\n",
       "  'lastModified': '2022-06-15T19:42:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/paraphrase-distilroberta-base-v2',\n",
       "  'downloads': 111064,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/detr-resnet-50': {'modelId': 'facebook/detr-resnet-50',\n",
       "  'sha': '272941311143979e4ade5424ede52fb5e84c9969',\n",
       "  'lastModified': '2022-06-27T08:29:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'detr',\n",
       "   'object-detection',\n",
       "   'dataset:coco',\n",
       "   'arxiv:2005.12872',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'object-detection',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['DetrForObjectDetection'],\n",
       "   'model_type': 'detr'},\n",
       "  'id': 'facebook/detr-resnet-50',\n",
       "  'downloads': 105601,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg',\n",
       "    'example_title': 'Savanna'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg',\n",
       "    'example_title': 'Football Match'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg',\n",
       "    'example_title': 'Airport'}],\n",
       "  'likes': 42,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['object-detection', 'vision'],\n",
       "   'datasets': ['coco'],\n",
       "   'widget': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg',\n",
       "     'example_title': 'Savanna'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg',\n",
       "     'example_title': 'Football Match'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg',\n",
       "     'example_title': 'Airport'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForObjectDetection',\n",
       "   'pipeline_tag': 'object-detection',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'Helsinki-NLP/opus-mt-es-en': {'modelId': 'Helsinki-NLP/opus-mt-es-en',\n",
       "  'sha': '7709af724cf305012a250cbd13cf3bfdbd2b66b0',\n",
       "  'lastModified': '2021-01-18T08:23:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'es',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-es-en',\n",
       "  'downloads': 104492,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Me llamo Wolfgang y vivo en Berlin'},\n",
       "   {'text': 'Los ingredientes de una tortilla de patatas son: huevos, patatas y cebolla'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['es', 'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/roberta-base-squad2-covid': {'modelId': 'deepset/roberta-base-squad2-covid',\n",
       "  'sha': 'b3506f363ab164823a64b5372d5cc98f36504cd6',\n",
       "  'lastModified': '2021-10-21T12:19:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['RobertaForQuestionAnswering'],\n",
       "   'model_type': 'roberta',\n",
       "   'task_specific_params': None},\n",
       "  'id': 'deepset/roberta-base-squad2-covid',\n",
       "  'downloads': 104396,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'aubmindlab/bert-base-arabertv2': {'modelId': 'aubmindlab/bert-base-arabertv2',\n",
       "  'sha': '599b85458968e0cbad56126802f8328e649b3bec',\n",
       "  'lastModified': '2022-04-06T15:22:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ar',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:OSIAN',\n",
       "   'dataset:1.5B Arabic Corpus',\n",
       "   'dataset:OSCAR Arabic Unshuffled',\n",
       "   'arxiv:2003.00104',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'aubmindlab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'aubmindlab/bert-base-arabertv2',\n",
       "  'downloads': 104252,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': ' عاصم +ة لبنان هي [MASK] .'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ar',\n",
       "   'datasets': ['wikipedia',\n",
       "    'OSIAN',\n",
       "    '1.5B Arabic Corpus',\n",
       "    'OSCAR Arabic Unshuffled'],\n",
       "   'widget': [{'text': ' عاصم +ة لبنان هي [MASK] .'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-base-italian-xxl-cased': {'modelId': 'dbmdz/bert-base-italian-xxl-cased',\n",
       "  'sha': 'e25680c78556c0d9002dba60d712e1df3095240e',\n",
       "  'lastModified': '2021-05-19T15:01:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'it',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-base-italian-xxl-cased',\n",
       "  'downloads': 104236,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': \"Roma è la [MASK] d'Italia.\"},\n",
       "   {'text': 'Lo scopo della vita è [MASK].'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'it', 'license': 'mit', 'datasets': ['wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wav2vec2-base': {'modelId': 'facebook/wav2vec2-base',\n",
       "  'sha': '0b5b8e868dd84f03fd87d01f9c4ff0f080fecfe8',\n",
       "  'lastModified': '2021-12-28T12:44:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wav2vec2',\n",
       "   'pretraining',\n",
       "   'en',\n",
       "   'dataset:librispeech_asr',\n",
       "   'arxiv:2006.11477',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Wav2Vec2ForPreTraining'],\n",
       "   'model_type': 'wav2vec2'},\n",
       "  'id': 'facebook/wav2vec2-base',\n",
       "  'downloads': 104121,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 14,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['librispeech_asr'],\n",
       "   'tags': ['speech'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'bert-base-german-dbmdz-uncased': {'modelId': 'bert-base-german-dbmdz-uncased',\n",
       "  'sha': '8f95e94bad647a9267df8620d729248ede4f86f4',\n",
       "  'lastModified': '2021-05-18T16:16:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-base-german-dbmdz-uncased',\n",
       "  'downloads': 102436,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cointegrated/LaBSE-en-ru': {'modelId': 'cointegrated/LaBSE-en-ru',\n",
       "  'sha': '9e6d1e5fa79584f5346a5262b69bb6dc64b46f98',\n",
       "  'lastModified': '2022-06-23T12:43:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'pretraining',\n",
       "   'ru',\n",
       "   'en',\n",
       "   'arxiv:2007.01852',\n",
       "   'transformers',\n",
       "   'feature-extraction',\n",
       "   'embeddings',\n",
       "   'sentence-similarity'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'cointegrated',\n",
       "  'config': {'architectures': ['BertForPreTraining'], 'model_type': 'bert'},\n",
       "  'id': 'cointegrated/LaBSE-en-ru',\n",
       "  'downloads': 99433,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru', 'en'],\n",
       "   'tags': ['feature-extraction', 'embeddings', 'sentence-similarity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/mobilebert-uncased': {'modelId': 'google/mobilebert-uncased',\n",
       "  'sha': '1f90a6c24c7879273a291d34a849033eba2dbc0f',\n",
       "  'lastModified': '2021-04-19T13:32:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rust',\n",
       "   'mobilebert',\n",
       "   'pretraining',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['MobileBertForPreTraining'],\n",
       "   'model_type': 'mobilebert'},\n",
       "  'id': 'google/mobilebert-uncased',\n",
       "  'downloads': 97856,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'aubmindlab/bert-base-arabert': {'modelId': 'aubmindlab/bert-base-arabert',\n",
       "  'sha': '4b7ceb4967371d5e0b559b275e006f54d671c48e',\n",
       "  'lastModified': '2021-05-19T11:49:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ar',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:OSIAN',\n",
       "   'dataset:1.5B Arabic Corpus',\n",
       "   'arxiv:2003.00104',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'aubmindlab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'aubmindlab/bert-base-arabert',\n",
       "  'downloads': 96387,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': ' عاصم +ة لبنان هي [MASK] .'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ar',\n",
       "   'datasets': ['wikipedia', 'OSIAN', '1.5B Arabic Corpus'],\n",
       "   'widget': [{'text': ' عاصم +ة لبنان هي [MASK] .'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-ru-en': {'modelId': 'Helsinki-NLP/opus-mt-ru-en',\n",
       "  'sha': '1f663e796e1a2187db2f04065d143dda4c634a44',\n",
       "  'lastModified': '2021-09-10T14:02:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'ru',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-ru-en',\n",
       "  'downloads': 95567,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Меня зовут Вольфганг и я живу в Берлине'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'speechbrain/spkrec-ecapa-voxceleb': {'modelId': 'speechbrain/spkrec-ecapa-voxceleb',\n",
       "  'sha': '5c0be3875fda05e81f3c004ed8c7c06be308de1e',\n",
       "  'lastModified': '2022-06-26T23:15:06.000Z',\n",
       "  'tags': ['en',\n",
       "   'dataset:voxceleb',\n",
       "   'arxiv:2106.04624',\n",
       "   'speechbrain',\n",
       "   'embeddings',\n",
       "   'Speaker',\n",
       "   'Verification',\n",
       "   'Identification',\n",
       "   'pytorch',\n",
       "   'ECAPA',\n",
       "   'TDNN',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'speechbrain',\n",
       "  'config': {'speechbrain': {'interface': 'SpeakerRecognition'}},\n",
       "  'id': 'speechbrain/spkrec-ecapa-voxceleb',\n",
       "  'downloads': 95347,\n",
       "  'library_name': 'speechbrain',\n",
       "  'widgetData': [{'example_title': 'VoxCeleb Speaker id10003',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/VoxCeleb1_00003.wav'},\n",
       "   {'example_title': 'VoxCeleb Speaker id10004',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/VoxCeleb_00004.wav'}],\n",
       "  'likes': 17,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': None,\n",
       "   'tags': ['speechbrain',\n",
       "    'embeddings',\n",
       "    'Speaker',\n",
       "    'Verification',\n",
       "    'Identification',\n",
       "    'pytorch',\n",
       "    'ECAPA',\n",
       "    'TDNN'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['voxceleb'],\n",
       "   'metrics': ['EER'],\n",
       "   'widget': [{'example_title': 'VoxCeleb Speaker id10003',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/VoxCeleb1_00003.wav'},\n",
       "    {'example_title': 'VoxCeleb Speaker id10004',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/VoxCeleb_00004.wav'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'dbmdz/distilbert-base-turkish-cased': {'modelId': 'dbmdz/distilbert-base-turkish-cased',\n",
       "  'sha': '2fffa20b389e66113ec7182349efdec00fce1ff5',\n",
       "  'lastModified': '2021-01-24T01:01:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'tr',\n",
       "   'arxiv:1910.01108',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'model_type': 'distilbert'},\n",
       "  'id': 'dbmdz/distilbert-base-turkish-cased',\n",
       "  'downloads': 95188,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'tr', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'EleutherAI/gpt-neo-2.7B': {'modelId': 'EleutherAI/gpt-neo-2.7B',\n",
       "  'sha': '51568a6e0ae813a3f2a9da558ab7beac5e3acc24',\n",
       "  'lastModified': '2021-12-31T13:46:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'gpt_neo',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'dataset:The Pile',\n",
       "   'transformers',\n",
       "   'text generation',\n",
       "   'causal-lm',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'EleutherAI',\n",
       "  'config': {'architectures': ['GPTNeoForCausalLM'],\n",
       "   'model_type': 'gpt_neo',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50,\n",
       "     'temperature': 0.9}}},\n",
       "  'id': 'EleutherAI/gpt-neo-2.7B',\n",
       "  'downloads': 95126,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 85,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['text generation', 'pytorch', 'causal-lm'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['The Pile']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-base-german-cased': {'modelId': 'bert-base-german-cased',\n",
       "  'sha': '702774c02b32a4f360d5fea60ab034d64bf0141c',\n",
       "  'lastModified': '2021-05-18T16:14:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-base-german-cased',\n",
       "  'downloads': 93964,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de',\n",
       "   'license': 'mit',\n",
       "   'thumbnail': 'https://static.tildacdn.com/tild6438-3730-4164-b266-613634323466/german_bert.png',\n",
       "   'tags': ['exbert']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/layoutxlm-base': {'modelId': 'microsoft/layoutxlm-base',\n",
       "  'sha': 'b95ef788341ccd507115d74e10c4bb7137559f19',\n",
       "  'lastModified': '2022-06-15T14:51:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'layoutlmv2',\n",
       "   'arxiv:2104.08836',\n",
       "   'transformers',\n",
       "   'license:cc-by-nc-sa-4.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'layoutlmv2'},\n",
       "  'id': 'microsoft/layoutxlm-base',\n",
       "  'downloads': 93961,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 19,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'cc-by-nc-sa-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'bigscience/T0_3B': {'modelId': 'bigscience/T0_3B',\n",
       "  'sha': '8794c7177e3a67b8a0ec739d94eecfa6a591c974',\n",
       "  'lastModified': '2022-06-21T01:31:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:bigscience/P3',\n",
       "   'arxiv:2110.08207',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'bigscience',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'bigscience/T0_3B',\n",
       "  'downloads': 93943,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': \"A is the son's of B's uncle. What is the family relationship between A and B?\"},\n",
       "   {'text': 'Reorder the words in this sentence: justin and name bieber years is my am I 27 old.'},\n",
       "   {'text': 'Task: copy but say the opposite.\\n PSG won its match against Barca.'},\n",
       "   {'text': 'Is this review positive or negative? Review: Best cast iron skillet you will every buy.',\n",
       "    'example_title': 'Sentiment analysis'},\n",
       "   {'text': 'Question A: How is air traffic controlled? \\nQuestion B: How do you become an air traffic controller?\\nPick one: these questions are duplicates or not duplicates.'},\n",
       "   {'text': \"Barack Obama nominated Hilary Clinton as his secretary of state on Monday. He chose her because she had foreign affairs experience as a former First Lady. \\nIn the previous sentence, decide who 'her' is referring to.\",\n",
       "    'example_title': 'Coreference resolution'},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\\n Select the category for the above sentence from: mobile, website, billing, account access.'},\n",
       "   {'text': 'Sentence 1: Gyorgy Heizler, head of the local disaster unit, said the coach was carrying 38 passengers.\\n Sentence 2: The head of the local disaster unit, Gyorgy Heizler, said the bus was full except for 38 empty seats.\\n\\n Do sentences 1 and 2 have the same meaning?',\n",
       "    'example_title': 'Paraphrase identification'},\n",
       "   {'text': \"Here's the beginning of an article, choose a tag that best describes the topic of the article: business, cinema, politics, health, travel, sports.\\n\\n The best and worst fo 007 as 'No time to die' marks Daniel Craig's exit.\\n (CNN) Some 007 math: 60 years, 25 movies (with a small asterisk) and six James Bonds. For a Cold War creation, Ian Fleming's suave spy has certainly gotten around, but despite different guises in the tuxedo and occasional scuba gear, when it comes to Bond ratings, there really shouldn't be much argument about who wore it best.\"},\n",
       "   {'text': \"Max: Know any good websites to buy clothes from?\\n Payton: Sure :) LINK 1, LINK 2, LINK 3\\n Max: That's a lot of them!\\n Payton: Yeah, but they have different things so I usually buy things from 2 or 3 of them.\\n Max: I'll check them out. Thanks.\\n\\n Who or what are Payton and Max referring to when they say 'them'?\"},\n",
       "   {'text': \"Is the word 'table' used in the same meaning in the two following sentences?\\n\\n Sentence A: you can leave the books on the table over there.\\n Sentence B: the tables in this book are very hard to read.\"},\n",
       "   {'text': 'On a shelf, there are five books: a gray book, a red book, a purple book, a blue book, and a black book.\\n The red book is to the right of the gray book. The black book is to the left of the blue book. The blue book is to the left of the gray book. The purple book is the second from the right.\\n\\n Which book is the leftmost book?',\n",
       "    'example_title': 'Logic puzzles'},\n",
       "   {'text': \"The two men running to become New York City's next mayor will face off in their first debate Wednesday night.\\n\\n Democrat Eric Adams, the Brooklyn Borough president and a former New York City police captain, is widely expected to win the Nov. 2 election against Republican Curtis Sliwa, the founder of the 1970s-era Guardian Angels anti-crime patril.\\n\\n Who are the men running for mayor?\",\n",
       "    'example_title': 'Reading comprehension'},\n",
       "   {'text': \"The word 'binne' means any animal that is furry and has four legs, and the word 'bam' means a simple sort of dwelling.\\n\\n Which of the following best characterizes binne bams?\\n - Sentence 1: Binne bams are for pets.\\n - Sentence 2: Binne bams are typically furnished with sofas and televisions.\\n - Sentence 3: Binne bams are luxurious apartments.\\n - Sentence 4: Binne bams are places where people live.\"}],\n",
       "  'likes': 40,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['bigscience/P3'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'widget': [{'text': \"A is the son's of B's uncle. What is the family relationship between A and B?\"},\n",
       "    {'text': 'Reorder the words in this sentence: justin and name bieber years is my am I 27 old.'},\n",
       "    {'text': 'Task: copy but say the opposite.\\n PSG won its match against Barca.'},\n",
       "    {'text': 'Is this review positive or negative? Review: Best cast iron skillet you will every buy.',\n",
       "     'example_title': 'Sentiment analysis'},\n",
       "    {'text': 'Question A: How is air traffic controlled? \\nQuestion B: How do you become an air traffic controller?\\nPick one: these questions are duplicates or not duplicates.'},\n",
       "    {'text': \"Barack Obama nominated Hilary Clinton as his secretary of state on Monday. He chose her because she had foreign affairs experience as a former First Lady. \\nIn the previous sentence, decide who 'her' is referring to.\",\n",
       "     'example_title': 'Coreference resolution'},\n",
       "    {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\\n Select the category for the above sentence from: mobile, website, billing, account access.'},\n",
       "    {'text': 'Sentence 1: Gyorgy Heizler, head of the local disaster unit, said the coach was carrying 38 passengers.\\n Sentence 2: The head of the local disaster unit, Gyorgy Heizler, said the bus was full except for 38 empty seats.\\n\\n Do sentences 1 and 2 have the same meaning?',\n",
       "     'example_title': 'Paraphrase identification'},\n",
       "    {'text': \"Here's the beginning of an article, choose a tag that best describes the topic of the article: business, cinema, politics, health, travel, sports.\\n\\n The best and worst fo 007 as 'No time to die' marks Daniel Craig's exit.\\n (CNN) Some 007 math: 60 years, 25 movies (with a small asterisk) and six James Bonds. For a Cold War creation, Ian Fleming's suave spy has certainly gotten around, but despite different guises in the tuxedo and occasional scuba gear, when it comes to Bond ratings, there really shouldn't be much argument about who wore it best.\"},\n",
       "    {'text': \"Max: Know any good websites to buy clothes from?\\n Payton: Sure :) LINK 1, LINK 2, LINK 3\\n Max: That's a lot of them!\\n Payton: Yeah, but they have different things so I usually buy things from 2 or 3 of them.\\n Max: I'll check them out. Thanks.\\n\\n Who or what are Payton and Max referring to when they say 'them'?\"},\n",
       "    {'text': \"Is the word 'table' used in the same meaning in the two following sentences?\\n\\n Sentence A: you can leave the books on the table over there.\\n Sentence B: the tables in this book are very hard to read.\"},\n",
       "    {'text': 'On a shelf, there are five books: a gray book, a red book, a purple book, a blue book, and a black book.\\n The red book is to the right of the gray book. The black book is to the left of the blue book. The blue book is to the left of the gray book. The purple book is the second from the right.\\n\\n Which book is the leftmost book?',\n",
       "     'example_title': 'Logic puzzles'},\n",
       "    {'text': \"The two men running to become New York City's next mayor will face off in their first debate Wednesday night.\\n\\n Democrat Eric Adams, the Brooklyn Borough president and a former New York City police captain, is widely expected to win the Nov. 2 election against Republican Curtis Sliwa, the founder of the 1970s-era Guardian Angels anti-crime patril.\\n\\n Who are the men running for mayor?\",\n",
       "     'example_title': 'Reading comprehension'},\n",
       "    {'text': \"The word 'binne' means any animal that is furry and has four legs, and the word 'bam' means a simple sort of dwelling.\\n\\n Which of the following best characterizes binne bams?\\n - Sentence 1: Binne bams are for pets.\\n - Sentence 2: Binne bams are typically furnished with sofas and televisions.\\n - Sentence 3: Binne bams are luxurious apartments.\\n - Sentence 4: Binne bams are places where people live.\"}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'squeezebert/squeezebert-uncased': {'modelId': 'squeezebert/squeezebert-uncased',\n",
       "  'sha': '7978b0c163f11850ec35d5cd541828159313ac41',\n",
       "  'lastModified': '2020-12-11T22:02:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'squeezebert',\n",
       "   'arxiv:2006.11316',\n",
       "   'arxiv:1904.00962',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'squeezebert',\n",
       "  'config': {'model_type': 'squeezebert'},\n",
       "  'id': 'squeezebert/squeezebert-uncased',\n",
       "  'downloads': 93796,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 0,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'google/mt5-base': {'modelId': 'google/mt5-base',\n",
       "  'sha': 'd86816880b5acc27e697e52bc237e816dc828b17',\n",
       "  'lastModified': '2022-05-27T15:05:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'mt5',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'az',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'ca',\n",
       "   'ceb',\n",
       "   'co',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'eo',\n",
       "   'es',\n",
       "   'et',\n",
       "   'eu',\n",
       "   'fa',\n",
       "   'fi',\n",
       "   'fil',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'haw',\n",
       "   'hi',\n",
       "   'hmn',\n",
       "   'ht',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'ig',\n",
       "   'is',\n",
       "   'it',\n",
       "   'iw',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'ku',\n",
       "   'ky',\n",
       "   'la',\n",
       "   'lb',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mi',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'mt',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'ny',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sd',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'sm',\n",
       "   'sn',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'st',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'tg',\n",
       "   'th',\n",
       "   'tr',\n",
       "   'uk',\n",
       "   'und',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'xh',\n",
       "   'yi',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'zu',\n",
       "   'dataset:mc4',\n",
       "   'arxiv:2010.11934',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['MT5ForConditionalGeneration'],\n",
       "   'model_type': 'mt5'},\n",
       "  'id': 'google/mt5-base',\n",
       "  'downloads': 93164,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 27,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'az',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'ca',\n",
       "    'ceb',\n",
       "    'co',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'eo',\n",
       "    'es',\n",
       "    'et',\n",
       "    'eu',\n",
       "    'fa',\n",
       "    'fi',\n",
       "    'fil',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'haw',\n",
       "    'hi',\n",
       "    'hmn',\n",
       "    'ht',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'ig',\n",
       "    'is',\n",
       "    'it',\n",
       "    'iw',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'ku',\n",
       "    'ky',\n",
       "    'la',\n",
       "    'lb',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mi',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'mt',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'ny',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sd',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'sm',\n",
       "    'sn',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'st',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'tg',\n",
       "    'th',\n",
       "    'tr',\n",
       "    'uk',\n",
       "    'und',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'xh',\n",
       "    'yi',\n",
       "    'yo',\n",
       "    'zh',\n",
       "    'zu'],\n",
       "   'datasets': ['mc4'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'BaptisteDoyen/camembert-base-xnli': {'modelId': 'BaptisteDoyen/camembert-base-xnli',\n",
       "  'sha': '791c5260a7c5984c7d96e622b45ca4c3ee6ea7d8',\n",
       "  'lastModified': '2022-06-29T09:30:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'camembert',\n",
       "   'text-classification',\n",
       "   'fr',\n",
       "   'dataset:xnli',\n",
       "   'transformers',\n",
       "   'zero-shot-classification',\n",
       "   'xnli',\n",
       "   'nli',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'BaptisteDoyen',\n",
       "  'config': {'architectures': ['CamembertForSequenceClassification'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'BaptisteDoyen/camembert-base-xnli',\n",
       "  'downloads': 92269,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['fr'],\n",
       "   'thumbnail': None,\n",
       "   'tags': ['zero-shot-classification', 'xnli', 'nli', 'fr'],\n",
       "   'license': 'mit',\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'datasets': ['xnli'],\n",
       "   'metrics': ['accuracy']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cmarkea/distilcamembert-base': {'modelId': 'cmarkea/distilcamembert-base',\n",
       "  'sha': 'bf14fbad88b19c837997f26dd1684bf98404f96b',\n",
       "  'lastModified': '2022-05-24T15:57:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'camembert',\n",
       "   'fill-mask',\n",
       "   'fr',\n",
       "   'dataset:oscar',\n",
       "   'arxiv:1910.01108',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'cmarkea',\n",
       "  'config': {'architectures': ['CamembertForMaskedLM'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'cmarkea/distilcamembert-base',\n",
       "  'downloads': 91806,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': \"J'aime lire les <mask> de SF.\"}],\n",
       "  'likes': 17,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['oscar'],\n",
       "   'widget': [{'text': \"J'aime lire les <mask> de SF.\"}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bvanaken/clinical-assertion-negation-bert': {'modelId': 'bvanaken/clinical-assertion-negation-bert',\n",
       "  'sha': 'f381df19e34e690108f3b8e3e8433f7c9d2e2f9d',\n",
       "  'lastModified': '2022-06-01T12:28:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'medical',\n",
       "   'clinical',\n",
       "   'assertion',\n",
       "   'negation'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'bvanaken',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'bvanaken/clinical-assertion-negation-bert',\n",
       "  'downloads': 91226,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Patient denies [entity] SOB [entity].'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['bert',\n",
       "    'medical',\n",
       "    'clinical',\n",
       "    'assertion',\n",
       "    'negation',\n",
       "    'text-classification'],\n",
       "   'widget': [{'text': 'Patient denies [entity] SOB [entity].'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cambridgeltl/BioRedditBERT-uncased': {'modelId': 'cambridgeltl/BioRedditBERT-uncased',\n",
       "  'sha': '53c71817b807682020273a0fa13aca033dfca292',\n",
       "  'lastModified': '2021-05-19T13:43:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'arxiv:2010.03295',\n",
       "   'transformers',\n",
       "   'BioNLP',\n",
       "   'social_media'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'cambridgeltl',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'cambridgeltl/BioRedditBERT-uncased',\n",
       "  'downloads': 91060,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'], 'tags': ['BioNLP', 'social_media']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/stsb-roberta-large': {'modelId': 'cross-encoder/stsb-roberta-large',\n",
       "  'sha': '9e35bf01ec28b309411c8903d0d4165567303eb4',\n",
       "  'lastModified': '2021-08-05T08:42:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cross-encoder/stsb-roberta-large',\n",
       "  'downloads': 90047,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'monologg/koelectra-small-v3-discriminator': {'modelId': 'monologg/koelectra-small-v3-discriminator',\n",
       "  'sha': '7488f8db0f208beff4a1f3f9bb3ed04650a89ed7',\n",
       "  'lastModified': '2020-12-26T16:24:33.000Z',\n",
       "  'tags': ['pytorch', 'electra', 'pretraining', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'monologg',\n",
       "  'config': {'architectures': ['ElectraForPreTraining'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'monologg/koelectra-small-v3-discriminator',\n",
       "  'downloads': 87921,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/mpnet-base': {'modelId': 'microsoft/mpnet-base',\n",
       "  'sha': '5b7474c98ab5f1801502f9d2348485acf4cbbe71',\n",
       "  'lastModified': '2020-12-03T15:59:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'mpnet',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['MPNetForMaskedLM'], 'model_type': 'mpnet'},\n",
       "  'id': 'microsoft/mpnet-base',\n",
       "  'downloads': 87630,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pyannote/speaker-diarization': {'modelId': 'pyannote/speaker-diarization',\n",
       "  'sha': '3602c22f60dd2d07ef42263fa798c08b4dacedd0',\n",
       "  'lastModified': '2022-03-23T09:22:56.000Z',\n",
       "  'tags': ['dataset:ami',\n",
       "   'dataset:dihard',\n",
       "   'dataset:voxconverse',\n",
       "   'dataset:voxceleb',\n",
       "   'arxiv:2012.01477',\n",
       "   'pyannote-audio',\n",
       "   'pyannote',\n",
       "   'pyannote-audio-pipeline',\n",
       "   'audio',\n",
       "   'voice',\n",
       "   'speech',\n",
       "   'speaker',\n",
       "   'speaker-diarization',\n",
       "   'speaker-change-detection',\n",
       "   'voice-activity-detection',\n",
       "   'overlapped-speech-detection',\n",
       "   'automatic-speech-recognition',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'pyannote',\n",
       "  'config': None,\n",
       "  'id': 'pyannote/speaker-diarization',\n",
       "  'downloads': 87600,\n",
       "  'library_name': 'pyannote-audio',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['pyannote',\n",
       "    'pyannote-audio',\n",
       "    'pyannote-audio-pipeline',\n",
       "    'audio',\n",
       "    'voice',\n",
       "    'speech',\n",
       "    'speaker',\n",
       "    'speaker-diarization',\n",
       "    'speaker-change-detection',\n",
       "    'voice-activity-detection',\n",
       "    'overlapped-speech-detection',\n",
       "    'automatic-speech-recognition'],\n",
       "   'datasets': ['ami', 'dihard', 'voxconverse', 'voxceleb'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'valhalla/distilbart-mnli-12-3': {'modelId': 'valhalla/distilbart-mnli-12-3',\n",
       "  'sha': 'ef9a58ce6a9cd44cd0d4c2f7db1cd67f81019a8b',\n",
       "  'lastModified': '2021-06-14T10:29:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bart',\n",
       "   'text-classification',\n",
       "   'dataset:mnli',\n",
       "   'transformers',\n",
       "   'distilbart',\n",
       "   'distilbart-mnli',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'valhalla',\n",
       "  'config': {'architectures': ['BartForSequenceClassification'],\n",
       "   'model_type': 'bart'},\n",
       "  'id': 'valhalla/distilbart-mnli-12-3',\n",
       "  'downloads': 84725,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system’s largest world. Konstantin Batygin did not set out to solve one of the solar system’s most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system’s missing “Planet Nine,” spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn’t rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: “Oh! This is how Europa formed.” Europa is one of Jupiter’s four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the Côte d’Azur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system’s formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['mnli'],\n",
       "   'tags': ['distilbart', 'distilbart-mnli'],\n",
       "   'pipeline_tag': 'zero-shot-classification'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cl-tohoku/bert-base-japanese-v2': {'modelId': 'cl-tohoku/bert-base-japanese-v2',\n",
       "  'sha': 'e4211d7c20b078ac29b022be35ae4b63f3fe1679',\n",
       "  'lastModified': '2021-09-23T13:45:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ja',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'cl-tohoku',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'cl-tohoku/bert-base-japanese-v2',\n",
       "  'downloads': 84442,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '東北大学で[MASK]の研究をしています。'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'datasets': ['wikipedia'],\n",
       "   'widget': [{'text': '東北大学で[MASK]の研究をしています。'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/mbart-large-50-many-to-many-mmt': {'modelId': 'facebook/mbart-large-50-many-to-many-mmt',\n",
       "  'sha': '0ece2bb75a89350002537169ecadeb2b3d043b6b',\n",
       "  'lastModified': '2022-05-26T22:28:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'mbart',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'ar',\n",
       "   'cs',\n",
       "   'de',\n",
       "   'en',\n",
       "   'es',\n",
       "   'et',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'gu',\n",
       "   'hi',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'kk',\n",
       "   'ko',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'si',\n",
       "   'tr',\n",
       "   'vi',\n",
       "   'zh',\n",
       "   'af',\n",
       "   'az',\n",
       "   'bn',\n",
       "   'fa',\n",
       "   'he',\n",
       "   'hr',\n",
       "   'id',\n",
       "   'ka',\n",
       "   'km',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'th',\n",
       "   'tl',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'xh',\n",
       "   'gl',\n",
       "   'sl',\n",
       "   'arxiv:2008.00401',\n",
       "   'transformers',\n",
       "   'mbart-50',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['MBartForConditionalGeneration'],\n",
       "   'model_type': 'mbart'},\n",
       "  'id': 'facebook/mbart-large-50-many-to-many-mmt',\n",
       "  'downloads': 84171,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'ar',\n",
       "    'cs',\n",
       "    'de',\n",
       "    'en',\n",
       "    'es',\n",
       "    'et',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'gu',\n",
       "    'hi',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'kk',\n",
       "    'ko',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'si',\n",
       "    'tr',\n",
       "    'vi',\n",
       "    'zh',\n",
       "    'af',\n",
       "    'az',\n",
       "    'bn',\n",
       "    'fa',\n",
       "    'he',\n",
       "    'hr',\n",
       "    'id',\n",
       "    'ka',\n",
       "    'km',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'th',\n",
       "    'tl',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'xh',\n",
       "    'gl',\n",
       "    'sl'],\n",
       "   'tags': ['mbart-50']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/mbart-large-en-ro': {'modelId': 'facebook/mbart-large-en-ro',\n",
       "  'sha': '2534e987b9ed03c416bbbaefa1a39e3441439bdd',\n",
       "  'lastModified': '2021-03-10T03:46:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'mbart',\n",
       "   'en',\n",
       "   'ro',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'model_type': 'mbart'},\n",
       "  'id': 'facebook/mbart-large-en-ro',\n",
       "  'downloads': 83723,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'],\n",
       "   'language': ['en', 'ro'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'openai-gpt': {'modelId': 'openai-gpt',\n",
       "  'sha': 'f5d43767fdae6d1add916f893596656254202d13',\n",
       "  'lastModified': '2020-12-09T18:29:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rust',\n",
       "   'openai-gpt',\n",
       "   'text-generation',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['OpenAIGPTLMHeadModel'],\n",
       "   'model_type': 'openai-gpt',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'openai-gpt',\n",
       "  'downloads': 83002,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'valhalla/t5-small-qa-qg-hl': {'modelId': 'valhalla/t5-small-qa-qg-hl',\n",
       "  'sha': 'a9d81e686f2169360fd59d8329235d3c4ba74f4f',\n",
       "  'lastModified': '2021-06-23T14:42:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'dataset:squad',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'question-generation',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'valhalla',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'translation_en_to_fr': {'early_stopping': True,\n",
       "     'length_penalty': 1,\n",
       "     'max_length': 32,\n",
       "     'num_beams': 4,\n",
       "     'prefix': ''}}},\n",
       "  'id': 'valhalla/t5-small-qa-qg-hl',\n",
       "  'downloads': 82671,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'generate question: <hl> 42 <hl> is the answer to life, the universe and everything. </s>'},\n",
       "   {'text': 'question: What is 42 context: 42 is the answer to life, the universe and everything. </s>'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['squad'],\n",
       "   'tags': ['question-generation'],\n",
       "   'widget': [{'text': 'generate question: <hl> 42 <hl> is the answer to life, the universe and everything. </s>'},\n",
       "    {'text': 'question: What is 42 context: 42 is the answer to life, the universe and everything. </s>'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/multi-qa-MiniLM-L6-cos-v1': {'modelId': 'sentence-transformers/multi-qa-MiniLM-L6-cos-v1',\n",
       "  'sha': '491739ea56794e0ffa273a3fdf7737d26db1e969',\n",
       "  'lastModified': '2022-06-21T14:48:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/multi-qa-MiniLM-L6-cos-v1',\n",
       "  'downloads': 82546,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 23,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'philschmid/bart-large-cnn-samsum': {'modelId': 'philschmid/bart-large-cnn-samsum',\n",
       "  'sha': '78e20b3792d507739ebb9e5a417bcc87606d3293',\n",
       "  'lastModified': '2022-07-04T13:10:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:samsum',\n",
       "   'transformers',\n",
       "   'sagemaker',\n",
       "   'summarization',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'philschmid',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4}}},\n",
       "  'id': 'philschmid/bart-large-cnn-samsum',\n",
       "  'downloads': 82164,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Jeff: Can I train a 🤗 Transformers model on Amazon SageMaker? \\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\nJeff: ok.\\nJeff: and how can I get started? \\nJeff: where can I find documentation? \\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face\\n'}],\n",
       "  'likes': 15,\n",
       "  'model-index': [{'name': 'bart-large-cnn-samsum',\n",
       "    'results': [{'task': {'type': 'summarization', 'name': 'Summarization'},\n",
       "      'dataset': {'name': 'SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization',\n",
       "       'type': 'samsum'},\n",
       "      'metrics': [{'name': 'Validation ROGUE-1',\n",
       "        'type': 'rogue-1',\n",
       "        'value': 42.621},\n",
       "       {'name': 'Validation ROGUE-2', 'type': 'rogue-2', 'value': 21.9825},\n",
       "       {'name': 'Validation ROGUE-L', 'type': 'rogue-l', 'value': 33.034},\n",
       "       {'name': 'Test ROGUE-1', 'type': 'rogue-1', 'value': 41.3174},\n",
       "       {'name': 'Test ROGUE-2', 'type': 'rogue-2', 'value': 20.8716},\n",
       "       {'name': 'Test ROGUE-L', 'type': 'rogue-l', 'value': 32.1337}]},\n",
       "     {'task': {'type': 'summarization', 'name': 'Summarization'},\n",
       "      'dataset': {'name': 'samsum',\n",
       "       'type': 'samsum',\n",
       "       'config': 'samsum',\n",
       "       'split': 'test'},\n",
       "      'metrics': [{'name': 'ROUGE-1',\n",
       "        'type': 'rouge',\n",
       "        'value': 41.3282,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-2',\n",
       "        'type': 'rouge',\n",
       "        'value': 20.8755,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-L',\n",
       "        'type': 'rouge',\n",
       "        'value': 32.1353,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-LSUM',\n",
       "        'type': 'rouge',\n",
       "        'value': 38.401,\n",
       "        'verified': True},\n",
       "       {'name': 'loss',\n",
       "        'type': 'loss',\n",
       "        'value': 1.4297215938568115,\n",
       "        'verified': True},\n",
       "       {'name': 'gen_len',\n",
       "        'type': 'gen_len',\n",
       "        'value': 60.0757,\n",
       "        'verified': True}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['sagemaker', 'bart', 'summarization'],\n",
       "   'datasets': ['samsum'],\n",
       "   'widget': [{'text': 'Jeff: Can I train a 🤗 Transformers model on Amazon SageMaker? \\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\nJeff: ok.\\nJeff: and how can I get started? \\nJeff: where can I find documentation? \\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face\\n'}],\n",
       "   'model-index': [{'name': 'bart-large-cnn-samsum',\n",
       "     'results': [{'task': {'type': 'summarization', 'name': 'Summarization'},\n",
       "       'dataset': {'name': 'SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization',\n",
       "        'type': 'samsum'},\n",
       "       'metrics': [{'name': 'Validation ROGUE-1',\n",
       "         'type': 'rogue-1',\n",
       "         'value': 42.621},\n",
       "        {'name': 'Validation ROGUE-2', 'type': 'rogue-2', 'value': 21.9825},\n",
       "        {'name': 'Validation ROGUE-L', 'type': 'rogue-l', 'value': 33.034},\n",
       "        {'name': 'Test ROGUE-1', 'type': 'rogue-1', 'value': 41.3174},\n",
       "        {'name': 'Test ROGUE-2', 'type': 'rogue-2', 'value': 20.8716},\n",
       "        {'name': 'Test ROGUE-L', 'type': 'rogue-l', 'value': 32.1337}]},\n",
       "      {'task': {'type': 'summarization', 'name': 'Summarization'},\n",
       "       'dataset': {'name': 'samsum',\n",
       "        'type': 'samsum',\n",
       "        'config': 'samsum',\n",
       "        'split': 'test'},\n",
       "       'metrics': [{'name': 'ROUGE-1',\n",
       "         'type': 'rouge',\n",
       "         'value': 41.3282,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-2',\n",
       "         'type': 'rouge',\n",
       "         'value': 20.8755,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-L',\n",
       "         'type': 'rouge',\n",
       "         'value': 32.1353,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-LSUM',\n",
       "         'type': 'rouge',\n",
       "         'value': 38.401,\n",
       "         'verified': True},\n",
       "        {'name': 'loss',\n",
       "         'type': 'loss',\n",
       "         'value': 1.4297215938568115,\n",
       "         'verified': True},\n",
       "        {'name': 'gen_len',\n",
       "         'type': 'gen_len',\n",
       "         'value': 60.0757,\n",
       "         'verified': True}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'DeepPavlov/rubert-base-cased-sentence': {'modelId': 'DeepPavlov/rubert-base-cased-sentence',\n",
       "  'sha': '78b5122d6365337dd4114281b0d08cd1edbb3bc8',\n",
       "  'lastModified': '2021-05-18T18:18:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'ru',\n",
       "   'arxiv:1508.05326',\n",
       "   'arxiv:1809.05053',\n",
       "   'arxiv:1908.10084',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'DeepPavlov',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'DeepPavlov/rubert-base-cased-sentence',\n",
       "  'downloads': 81886,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'huggingtweets/pabloiglesias': {'modelId': 'huggingtweets/pabloiglesias',\n",
       "  'sha': '3d30ed2ab81feb7ce8eac1df3d7fa03db8c13e4e',\n",
       "  'lastModified': '2021-05-22T17:52:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'huggingtweets'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'huggingtweets',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 160,\n",
       "     'min_length': 10,\n",
       "     'prefix': '<|endoftext|>',\n",
       "     'temperature': 1,\n",
       "     'top_p': 0.95}}},\n",
       "  'id': 'huggingtweets/pabloiglesias',\n",
       "  'downloads': 80764,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My dream is'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://www.huggingtweets.com/pabloiglesias/1621002350351/predictions.png',\n",
       "   'tags': ['huggingtweets'],\n",
       "   'widget': [{'text': 'My dream is'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'smanjil/German-MedBERT': {'modelId': 'smanjil/German-MedBERT',\n",
       "  'sha': 'b4e8a3e260ca938390616816402ab23d98775b07',\n",
       "  'lastModified': '2022-06-13T16:52:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'German',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'smanjil',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'smanjil/German-MedBERT',\n",
       "  'downloads': 80576,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de', 'tags': ['exbert', 'German']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/xglm-4.5B': {'modelId': 'facebook/xglm-4.5B',\n",
       "  'sha': '19523cf39b8f6f61232e9aa4191fa9473b398bff',\n",
       "  'lastModified': '2022-02-15T01:32:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xglm',\n",
       "   'text-generation',\n",
       "   'arxiv:2112.10668',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['XGLMForCausalLM'], 'model_type': 'xglm'},\n",
       "  'id': 'facebook/xglm-4.5B',\n",
       "  'downloads': 80188,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'mit',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png',\n",
       "   'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'anferico/bert-for-patents': {'modelId': 'anferico/bert-for-patents',\n",
       "  'sha': 'd1a25632e9c586399068a2f139d5664306b32ad8',\n",
       "  'lastModified': '2022-06-23T19:22:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'masked-lm',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'anferico',\n",
       "  'config': {'architectures': ['BertForMaskedLM']},\n",
       "  'id': 'anferico/bert-for-patents',\n",
       "  'downloads': 79700,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'The present [MASK] provides a torque sensor that is small and highly rigid and for which high production efficiency is possible.'},\n",
       "   {'text': 'The present invention relates to [MASK] accessories and pertains particularly to a brake light unit for bicycles.'},\n",
       "   {'text': 'The present invention discloses a space-bound-free [MASK] and its coordinate determining circuit for determining a coordinate of a stylus pen.'},\n",
       "   {'text': 'The illuminated [MASK] includes a substantially translucent canopy supported by a plurality of ribs pivotally swingable towards and away from a shaft.'}],\n",
       "  'likes': 23,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['masked-lm', 'pytorch'],\n",
       "   'pipeline-tag': 'fill-mask',\n",
       "   'mask-token': '[MASK]',\n",
       "   'widget': [{'text': 'The present [MASK] provides a torque sensor that is small and highly rigid and for which high production efficiency is possible.'},\n",
       "    {'text': 'The present invention relates to [MASK] accessories and pertains particularly to a brake light unit for bicycles.'},\n",
       "    {'text': 'The present invention discloses a space-bound-free [MASK] and its coordinate determining circuit for determining a coordinate of a stylus pen.'},\n",
       "    {'text': 'The illuminated [MASK] includes a substantially translucent canopy supported by a plurality of ribs pivotally swingable towards and away from a shaft.'}],\n",
       "   'license': 'apache-2.0',\n",
       "   'metrics': ['perplexity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask'}},\n",
       " 'patrickvonplaten/t5-tiny-random': {'modelId': 'patrickvonplaten/t5-tiny-random',\n",
       "  'sha': 'a3735d6adf1f23b7c32e6622fd6da7bc46d7f123',\n",
       "  'lastModified': '2021-11-03T17:13:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'patrickvonplaten',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'patrickvonplaten/t5-tiny-random',\n",
       "  'downloads': 78167,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'funnel-transformer/small': {'modelId': 'funnel-transformer/small',\n",
       "  'sha': 'ff0f4c11e46720ca10aa2dd668c2c58fe00ad214',\n",
       "  'lastModified': '2020-12-11T21:40:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'funnel',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:gigaword',\n",
       "   'arxiv:2006.03236',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'funnel-transformer',\n",
       "  'config': {'architectures': ['FunnelModel'], 'model_type': 'funnel'},\n",
       "  'id': 'funnel-transformer/small',\n",
       "  'downloads': 77622,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia', 'gigaword']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/distiluse-base-multilingual-cased': {'modelId': 'sentence-transformers/distiluse-base-multilingual-cased',\n",
       "  'sha': '47709c6fb2c53d30c871b05b8fb2693a5428d96a',\n",
       "  'lastModified': '2022-06-21T14:55:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rust',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'multilingual',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/distiluse-base-multilingual-cased',\n",
       "  'downloads': 77232,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'language': 'multilingual',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/vit-base-patch16-224-in21k': {'modelId': 'google/vit-base-patch16-224-in21k',\n",
       "  'sha': '1ba429d32753f33a0660b80ac6f43a3c80c18938',\n",
       "  'lastModified': '2022-01-12T08:03:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'vit',\n",
       "   'feature-extraction',\n",
       "   'dataset:imagenet-21k',\n",
       "   'arxiv:2010.11929',\n",
       "   'arxiv:2006.03677',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['ViTModel'], 'model_type': 'vit'},\n",
       "  'id': 'google/vit-base-patch16-224-in21k',\n",
       "  'downloads': 76868,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['vision'],\n",
       "   'datasets': ['imagenet-21k'],\n",
       "   'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'google/vit-base-patch16-224': {'modelId': 'google/vit-base-patch16-224',\n",
       "  'sha': '5dca96d358b3fcb9d53b3d3881eb1ae20b6752d1',\n",
       "  'lastModified': '2022-06-23T07:42:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'vit',\n",
       "   'image-classification',\n",
       "   'dataset:imagenet-1k',\n",
       "   'dataset:imagenet-21k',\n",
       "   'arxiv:2010.11929',\n",
       "   'arxiv:2006.03677',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-classification',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['ViTForImageClassification'],\n",
       "   'model_type': 'vit'},\n",
       "  'id': 'google/vit-base-patch16-224',\n",
       "  'downloads': 76229,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg',\n",
       "    'example_title': 'Tiger'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg',\n",
       "    'example_title': 'Teapot'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg',\n",
       "    'example_title': 'Palace'}],\n",
       "  'likes': 46,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['vision', 'image-classification'],\n",
       "   'datasets': ['imagenet-1k', 'imagenet-21k'],\n",
       "   'widget': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg',\n",
       "     'example_title': 'Tiger'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg',\n",
       "     'example_title': 'Teapot'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg',\n",
       "     'example_title': 'Palace'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageClassification',\n",
       "   'pipeline_tag': 'image-classification',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'deepset/bert-base-cased-squad2': {'modelId': 'deepset/bert-base-cased-squad2',\n",
       "  'sha': '66162c7d1eb6b238ca147485bf392ca907576b57',\n",
       "  'lastModified': '2021-05-19T15:24:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'deepset/bert-base-cased-squad2',\n",
       "  'downloads': 75516,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-et-en': {'modelId': 'Helsinki-NLP/opus-mt-et-en',\n",
       "  'sha': '3649f4761e2a1b25f2c19e3f1732c6ba9ef61519',\n",
       "  'lastModified': '2021-09-09T21:46:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'et',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-et-en',\n",
       "  'downloads': 75388,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/roberta-large-squad2': {'modelId': 'deepset/roberta-large-squad2',\n",
       "  'sha': '9de49fc56513eb53d822f3344ce2e898cfeca170',\n",
       "  'lastModified': '2021-05-20T16:09:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['RobertaForQuestionAnswering'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'deepset/roberta-large-squad2',\n",
       "  'downloads': 75045,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/distilbert-base-nli-stsb-mean-tokens': {'modelId': 'sentence-transformers/distilbert-base-nli-stsb-mean-tokens',\n",
       "  'sha': '888433dbfd0f07dc22d9e038d9acb20e5ca7e0d5',\n",
       "  'lastModified': '2022-06-15T20:07:20.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/distilbert-base-nli-stsb-mean-tokens',\n",
       "  'downloads': 74899,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/LaBSE': {'modelId': 'sentence-transformers/LaBSE',\n",
       "  'sha': '931b5f9a111859fa72549cd1a7cb32168ebbe010',\n",
       "  'lastModified': '2022-06-15T19:56:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/LaBSE',\n",
       "  'downloads': 74743,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 21,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'etalab-ia/camembert-base-squadFR-fquad-piaf': {'modelId': 'etalab-ia/camembert-base-squadFR-fquad-piaf',\n",
       "  'sha': '63296563d30b341d2cbb3feae651a3545dc1c74d',\n",
       "  'lastModified': '2022-07-04T08:16:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'camembert',\n",
       "   'question-answering',\n",
       "   'fr',\n",
       "   'dataset:piaf',\n",
       "   'dataset:FQuAD',\n",
       "   'dataset:SQuAD-FR',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'etalab-ia',\n",
       "  'config': {'architectures': ['RobertaForQuestionAnswering'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'etalab-ia/camembert-base-squadFR-fquad-piaf',\n",
       "  'downloads': 74622,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': \"Comment s'appelle le portail open data du gouvernement ?\",\n",
       "    'context': \"Etalab est une administration publique française qui fait notamment office de Chief Data Officer de l'État et coordonne la conception et la mise en œuvre de sa stratégie dans le domaine de la donnée (ouverture et partage des données publiques ou open data, exploitation des données et intelligence artificielle...). Ainsi, Etalab développe et maintient le portail des données ouvertes du gouvernement français data.gouv.fr. Etalab promeut également une plus grande ouverture l'administration sur la société (gouvernement ouvert) : transparence de l'action publique, innovation ouverte, participation citoyenne... elle promeut l’innovation, l’expérimentation, les méthodes de travail ouvertes, agiles et itératives, ainsi que les synergies avec la société civile pour décloisonner l’administration et favoriser l’adoption des meilleures pratiques professionnelles dans le domaine du numérique. À ce titre elle étudie notamment l’opportunité de recourir à des technologies en voie de maturation issues du monde de la recherche. Cette entité chargée de l'innovation au sein de l'administration doit contribuer à l'amélioration du service public grâce au numérique. Elle est rattachée à la Direction interministérielle du numérique, dont les missions et l’organisation ont été fixées par le décret du 30 octobre 2019.\\u2009 Dirigé par Laure Lucchesi depuis 2016, elle rassemble une équipe pluridisciplinaire d'une trentaine de personnes.\"}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr',\n",
       "   'datasets': ['piaf', 'FQuAD', 'SQuAD-FR'],\n",
       "   'widget': [{'text': \"Comment s'appelle le portail open data du gouvernement ?\",\n",
       "     'context': \"Etalab est une administration publique française qui fait notamment office de Chief Data Officer de l'État et coordonne la conception et la mise en œuvre de sa stratégie dans le domaine de la donnée (ouverture et partage des données publiques ou open data, exploitation des données et intelligence artificielle...). Ainsi, Etalab développe et maintient le portail des données ouvertes du gouvernement français data.gouv.fr. Etalab promeut également une plus grande ouverture l'administration sur la société (gouvernement ouvert) : transparence de l'action publique, innovation ouverte, participation citoyenne... elle promeut l’innovation, l’expérimentation, les méthodes de travail ouvertes, agiles et itératives, ainsi que les synergies avec la société civile pour décloisonner l’administration et favoriser l’adoption des meilleures pratiques professionnelles dans le domaine du numérique. À ce titre elle étudie notamment l’opportunité de recourir à des technologies en voie de maturation issues du monde de la recherche. Cette entité chargée de l'innovation au sein de l'administration doit contribuer à l'amélioration du service public grâce au numérique. Elle est rattachée à la Direction interministérielle du numérique, dont les missions et l’organisation ont été fixées par le décret du 30 octobre 2019.\\u2009 Dirigé par Laure Lucchesi depuis 2016, elle rassemble une équipe pluridisciplinaire d'une trentaine de personnes.\"}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/xlm-roberta-large-squad2': {'modelId': 'deepset/xlm-roberta-large-squad2',\n",
       "  'sha': '8f1f44ba7cbfe49d8452579c90d0fd0b0ac496da',\n",
       "  'lastModified': '2021-10-21T12:19:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'question-answering',\n",
       "   'multilingual',\n",
       "   'dataset:squad_v2',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['XLMRobertaForQuestionAnswering'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'deepset/xlm-roberta-large-squad2',\n",
       "  'downloads': 74066,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 17,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'tags': ['question-answering'],\n",
       "   'datasets': ['squad_v2'],\n",
       "   'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nyust-eb210/braslab-bert-drcd-384': {'modelId': 'nyust-eb210/braslab-bert-drcd-384',\n",
       "  'sha': 'abb9294fe9c0605d2f498a3228bfc6a30e8d2fbb',\n",
       "  'lastModified': '2021-05-31T14:47:20.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'zh-tw',\n",
       "   'dataset:DRCD',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'nyust-eb210',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'nyust-eb210/braslab-bert-drcd-384',\n",
       "  'downloads': 73536,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'zh-tw',\n",
       "   'datasets': 'DRCD',\n",
       "   'tasks': 'Question Answering'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepparag/Aeona': {'modelId': 'deepparag/Aeona',\n",
       "  'sha': '05c943eff79282ab56c3996e53ef7ccb3089f81f',\n",
       "  'lastModified': '2022-06-30T05:09:59.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'deepparag',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'do_sample': True,\n",
       "     'max_length': 1000,\n",
       "     'no_repeat_ngram_size': 4,\n",
       "     'temperature': 0.85,\n",
       "     'top_k': 100,\n",
       "     'top_p': 0.7}}},\n",
       "  'id': 'deepparag/Aeona',\n",
       "  'downloads': 73475,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://images-ext-2.discordapp.net/external/Wvtx1L98EbA7DR2lpZPbDxDuO4qmKt03nZygATZtXgk/%3Fsize%3D4096/https/cdn.discordapp.com/avatars/931226824753700934/338a9e413bbceaeb9095a29e97d4fac0.png',\n",
       "   'tags': ['conversational'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/ms-marco-TinyBERT-L-2-v2': {'modelId': 'cross-encoder/ms-marco-TinyBERT-L-2-v2',\n",
       "  'sha': 'e9ea2688951463fc2791a2ea2ddfce6762900675',\n",
       "  'lastModified': '2021-08-05T08:39:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'cross-encoder/ms-marco-TinyBERT-L-2-v2',\n",
       "  'downloads': 73316,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pysentimiento/robertuito-sentiment-analysis': {'modelId': 'pysentimiento/robertuito-sentiment-analysis',\n",
       "  'sha': 'e3be95c8efad7f480ce8aab2221188ecb78e40f3',\n",
       "  'lastModified': '2022-06-23T13:01:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'es',\n",
       "   'arxiv:2106.09462',\n",
       "   'arxiv:2111.09453',\n",
       "   'transformers',\n",
       "   'twitter',\n",
       "   'sentiment-analysis'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'pysentimiento',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'pysentimiento/robertuito-sentiment-analysis',\n",
       "  'downloads': 73003,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Te quiero. Te amo.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['es'], 'tags': ['twitter', 'sentiment-analysis']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/electra-large-discriminator': {'modelId': 'google/electra-large-discriminator',\n",
       "  'sha': '96c9a247e8ef7e818408efedfbd5fd2a26aa13ae',\n",
       "  'lastModified': '2021-04-30T07:38:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'electra',\n",
       "   'pretraining',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['ElectraForPreTraining'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'google/electra-large-discriminator',\n",
       "  'downloads': 72327,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'felflare/bert-restore-punctuation': {'modelId': 'felflare/bert-restore-punctuation',\n",
       "  'sha': '954108a105ef1f89f08b71c25d6e33bb89cde724',\n",
       "  'lastModified': '2021-05-24T03:04:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'en',\n",
       "   'dataset:yelp_polarity',\n",
       "   'transformers',\n",
       "   'punctuation',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'felflare',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'felflare/bert-restore-punctuation',\n",
       "  'downloads': 71213,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 21,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['punctuation'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['yelp_polarity'],\n",
       "   'metrics': ['f1']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'yjernite/retribert-base-uncased': {'modelId': 'yjernite/retribert-base-uncased',\n",
       "  'sha': 'aeab2b097862fa41e084db47e0e02229649bbe53',\n",
       "  'lastModified': '2021-03-10T02:54:37.000Z',\n",
       "  'tags': ['pytorch', 'retribert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'yjernite',\n",
       "  'config': {'architectures': ['RetriBertModel'], 'model_type': 'retribert'},\n",
       "  'id': 'yjernite/retribert-base-uncased',\n",
       "  'downloads': 70663,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/DialoGPT-medium': {'modelId': 'microsoft/DialoGPT-medium',\n",
       "  'sha': '8bada3b953e25ec171dea4e28c52f1e8b546d707',\n",
       "  'lastModified': '2021-05-23T09:11:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'arxiv:1911.00536',\n",
       "   'transformers',\n",
       "   'conversational',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'microsoft/DialoGPT-medium',\n",
       "  'downloads': 70452,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 24,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/dialogpt.png',\n",
       "   'tags': ['conversational'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext': {'modelId': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',\n",
       "  'sha': 'eaa409b6b7c9380a5f2ba7a59aa97712ff30f386',\n",
       "  'lastModified': '2021-09-22T20:09:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'arxiv:2007.15779',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',\n",
       "  'downloads': 70287,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '[MASK] is a tumor suppressor gene.'}],\n",
       "  'likes': 32,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': '[MASK] is a tumor suppressor gene.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/reformer-crime-and-punishment': {'modelId': 'google/reformer-crime-and-punishment',\n",
       "  'sha': '0e6c3decb8211d49bf881013425dc8b0448b3f5a',\n",
       "  'lastModified': '2021-02-01T17:53:38.000Z',\n",
       "  'tags': ['pytorch', 'rust', 'reformer', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['ReformerModelWithLMHead'],\n",
       "   'model_type': 'reformer',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 100}}},\n",
       "  'id': 'google/reformer-crime-and-punishment',\n",
       "  'downloads': 69746,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'albert-xxlarge-v2': {'modelId': 'albert-xxlarge-v2',\n",
       "  'sha': 'aaec31cf649a4d91a96b11f83eb5b2985eaf8ee5',\n",
       "  'lastModified': '2021-01-13T15:33:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1909.11942',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'albert-xxlarge-v2',\n",
       "  'downloads': 69726,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['exbert'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'DeepPavlov/rubert-base-cased': {'modelId': 'DeepPavlov/rubert-base-cased',\n",
       "  'sha': '4036cab694767a299f2b9e6492909664d9414229',\n",
       "  'lastModified': '2021-11-23T08:03:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'ru',\n",
       "   'arxiv:1905.07213',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'DeepPavlov',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'DeepPavlov/rubert-base-cased',\n",
       "  'downloads': 69071,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cardiffnlp/twitter-roberta-base-emotion': {'modelId': 'cardiffnlp/twitter-roberta-base-emotion',\n",
       "  'sha': 'dff452c4f42c15a25bd51aff1f1ca5d15ec08c23',\n",
       "  'lastModified': '2022-03-23T14:34:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'arxiv:2010.12421',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cardiffnlp',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cardiffnlp/twitter-roberta-base-emotion',\n",
       "  'downloads': 66608,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/distilbert-base-nli-mean-tokens': {'modelId': 'sentence-transformers/distilbert-base-nli-mean-tokens',\n",
       "  'sha': '683b927b0b0f77e70b9a7d15f7f7601a515925a9',\n",
       "  'lastModified': '2022-06-15T19:35:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/distilbert-base-nli-mean-tokens',\n",
       "  'downloads': 65922,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'feature-extraction',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-ROMANCE-en': {'modelId': 'Helsinki-NLP/opus-mt-ROMANCE-en',\n",
       "  'sha': 'dd27a5df7623594b19ab50244084e2beddc2181c',\n",
       "  'lastModified': '2021-09-09T21:25:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'roa',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-ROMANCE-en',\n",
       "  'downloads': 65625,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 't5-3b': {'modelId': 't5-3b',\n",
       "  'sha': '89a53f1bc99ac6386b6757c149b150f5819a29f2',\n",
       "  'lastModified': '2022-03-18T17:24:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'fr',\n",
       "   'ro',\n",
       "   'de',\n",
       "   'dataset:c4',\n",
       "   'arxiv:1805.12471',\n",
       "   'arxiv:1708.00055',\n",
       "   'arxiv:1704.05426',\n",
       "   'arxiv:1606.05250',\n",
       "   'arxiv:1808.09121',\n",
       "   'arxiv:1810.12885',\n",
       "   'arxiv:1905.10044',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['T5WithLMHeadModel'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 't5-3b',\n",
       "  'downloads': 65493,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'fr', 'ro', 'de'],\n",
       "   'datasets': ['c4'],\n",
       "   'tags': ['summarization', 'translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelWithLMHead',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Narsil/deberta-large-mnli-zero-cls': {'modelId': 'Narsil/deberta-large-mnli-zero-cls',\n",
       "  'sha': '47eecd0a22df5e7d6ad4d9ff6fa4b6f322db5700',\n",
       "  'lastModified': '2021-08-23T13:27:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'deberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'deberta-v1',\n",
       "   'deberta-mnli',\n",
       "   'license:mit',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'Narsil',\n",
       "  'config': {'architectures': ['DebertaForSequenceClassification'],\n",
       "   'model_type': 'deberta'},\n",
       "  'id': 'Narsil/deberta-large-mnli-zero-cls',\n",
       "  'downloads': 64833,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system’s largest world. Konstantin Batygin did not set out to solve one of the solar system’s most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system’s missing “Planet Nine,” spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn’t rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: “Oh! This is how Europa formed.” Europa is one of Jupiter’s four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the Côte d’Azur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system’s formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['deberta-v1', 'deberta-mnli'],\n",
       "   'tasks': 'mnli',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit',\n",
       "   'pipeline_tag': 'zero-shot-classification'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/swin-tiny-patch4-window7-224': {'modelId': 'microsoft/swin-tiny-patch4-window7-224',\n",
       "  'sha': '83d40fb5b9320b349382208d9e7fe998484e99df',\n",
       "  'lastModified': '2022-05-16T18:24:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'swin',\n",
       "   'image-classification',\n",
       "   'dataset:imagenet-1k',\n",
       "   'arxiv:2103.14030',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['SwinForImageClassification'],\n",
       "   'model_type': 'swin'},\n",
       "  'id': 'microsoft/swin-tiny-patch4-window7-224',\n",
       "  'downloads': 63588,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg',\n",
       "    'example_title': 'Tiger'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg',\n",
       "    'example_title': 'Teapot'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg',\n",
       "    'example_title': 'Palace'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['vision', 'image-classification'],\n",
       "   'datasets': ['imagenet-1k'],\n",
       "   'widget': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg',\n",
       "     'example_title': 'Tiger'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg',\n",
       "     'example_title': 'Teapot'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg',\n",
       "     'example_title': 'Palace'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageClassification',\n",
       "   'pipeline_tag': 'image-classification',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'google/mt5-small': {'modelId': 'google/mt5-small',\n",
       "  'sha': 'f03a52d3eaa650878b6f52e443bc4d5b385e786e',\n",
       "  'lastModified': '2022-05-27T15:06:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'mt5',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'az',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'ca',\n",
       "   'ceb',\n",
       "   'co',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'eo',\n",
       "   'es',\n",
       "   'et',\n",
       "   'eu',\n",
       "   'fa',\n",
       "   'fi',\n",
       "   'fil',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'haw',\n",
       "   'hi',\n",
       "   'hmn',\n",
       "   'ht',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'ig',\n",
       "   'is',\n",
       "   'it',\n",
       "   'iw',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'ku',\n",
       "   'ky',\n",
       "   'la',\n",
       "   'lb',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mi',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'mt',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'ny',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sd',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'sm',\n",
       "   'sn',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'st',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'tg',\n",
       "   'th',\n",
       "   'tr',\n",
       "   'uk',\n",
       "   'und',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'xh',\n",
       "   'yi',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'zu',\n",
       "   'dataset:mc4',\n",
       "   'arxiv:2010.11934',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['MT5ForConditionalGeneration'],\n",
       "   'model_type': 'mt5'},\n",
       "  'id': 'google/mt5-small',\n",
       "  'downloads': 63334,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 17,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'az',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'ca',\n",
       "    'ceb',\n",
       "    'co',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'eo',\n",
       "    'es',\n",
       "    'et',\n",
       "    'eu',\n",
       "    'fa',\n",
       "    'fi',\n",
       "    'fil',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'haw',\n",
       "    'hi',\n",
       "    'hmn',\n",
       "    'ht',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'ig',\n",
       "    'is',\n",
       "    'it',\n",
       "    'iw',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'ku',\n",
       "    'ky',\n",
       "    'la',\n",
       "    'lb',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mi',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'mt',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'ny',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sd',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'sm',\n",
       "    'sn',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'st',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'tg',\n",
       "    'th',\n",
       "    'tr',\n",
       "    'uk',\n",
       "    'und',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'xh',\n",
       "    'yi',\n",
       "    'yo',\n",
       "    'zh',\n",
       "    'zu'],\n",
       "   'datasets': ['mc4'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/multi-qa-mpnet-base-dot-v1': {'modelId': 'sentence-transformers/multi-qa-mpnet-base-dot-v1',\n",
       "  'sha': '0675a4fa3b0e6e4a7900924c121277f73adcdb2b',\n",
       "  'lastModified': '2021-08-23T18:28:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mpnet',\n",
       "   'fill-mask',\n",
       "   'sentence-transformers',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['MPNetForMaskedLM'], 'model_type': 'mpnet'},\n",
       "  'id': 'sentence-transformers/multi-qa-mpnet-base-dot-v1',\n",
       "  'downloads': 62898,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12': {'modelId': 'bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12',\n",
       "  'sha': 'c656dbe1fc4d6c7771d93bcaff21b2e7984f64c8',\n",
       "  'lastModified': '2021-09-24T07:46:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'en',\n",
       "   'dataset:PubMed',\n",
       "   'dataset:MIMIC-III',\n",
       "   'transformers',\n",
       "   'bert',\n",
       "   'bluebert',\n",
       "   'license:cc0-1.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'bionlp',\n",
       "  'config': {},\n",
       "  'id': 'bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12',\n",
       "  'downloads': 61300,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['bert', 'bluebert'],\n",
       "   'license': 'cc0-1.0',\n",
       "   'datasets': ['PubMed', 'MIMIC-III']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'arpanghoshal/EmoRoBERTa': {'modelId': 'arpanghoshal/EmoRoBERTa',\n",
       "  'sha': 'ed312eedd348a666a9b151d968a0fb98d73394d0',\n",
       "  'lastModified': '2022-06-20T22:42:02.000Z',\n",
       "  'tags': ['tf',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:go_emotions',\n",
       "   'transformers',\n",
       "   'tensorflow',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'arpanghoshal',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'arpanghoshal/EmoRoBERTa',\n",
       "  'downloads': 60661,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 21,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['text-classification', 'tensorflow', 'roberta'],\n",
       "   'datasets': ['go_emotions'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dangvantuan/sentence-camembert-large': {'modelId': 'dangvantuan/sentence-camembert-large',\n",
       "  'sha': '046a7d414b0a573af813c0e4d3ff2d0c7ca5c4b3',\n",
       "  'lastModified': '2022-03-11T17:01:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'camembert',\n",
       "   'feature-extraction',\n",
       "   'fr',\n",
       "   'dataset:stsb_multi_mt',\n",
       "   'arxiv:1908.10084',\n",
       "   'transformers',\n",
       "   'Text',\n",
       "   'Sentence Similarity',\n",
       "   'Sentence-Embedding',\n",
       "   'camembert-large',\n",
       "   'license:apache-2.0',\n",
       "   'sentence-similarity',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'dangvantuan',\n",
       "  'config': {'architectures': ['CamembertModel'], 'model_type': 'camembert'},\n",
       "  'id': 'dangvantuan/sentence-camembert-large',\n",
       "  'downloads': 60568,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': \"C'est une personne heureuse\",\n",
       "    'sentences': [\"C'est un chien heureux\",\n",
       "     \"C'est une personne très heureuse\",\n",
       "     \"Aujourd'hui est une journée ensoleillée\"]}],\n",
       "  'likes': 6,\n",
       "  'model-index': [{'name': 'sentence-camembert-large by Van Tuan DANG',\n",
       "    'results': [{'task': {'name': 'Sentence-Embedding',\n",
       "       'type': 'Text Similarity'},\n",
       "      'dataset': {'name': 'Text Similarity fr',\n",
       "       'type': 'stsb_multi_mt',\n",
       "       'args': 'fr'},\n",
       "      'metrics': [{'name': 'Test Pearson correlation coefficient',\n",
       "        'type': 'Pearson_correlation_coefficient',\n",
       "        'value': 'xx.xx'}]}]}],\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'language': 'fr',\n",
       "   'datasets': ['stsb_multi_mt'],\n",
       "   'tags': ['Text',\n",
       "    'Sentence Similarity',\n",
       "    'Sentence-Embedding',\n",
       "    'camembert-large'],\n",
       "   'license': 'apache-2.0',\n",
       "   'model-index': [{'name': 'sentence-camembert-large by Van Tuan DANG',\n",
       "     'results': [{'task': {'name': 'Sentence-Embedding',\n",
       "        'type': 'Text Similarity'},\n",
       "       'dataset': {'name': 'Text Similarity fr',\n",
       "        'type': 'stsb_multi_mt',\n",
       "        'args': 'fr'},\n",
       "       'metrics': [{'name': 'Test Pearson correlation coefficient',\n",
       "         'type': 'Pearson_correlation_coefficient',\n",
       "         'value': 'xx.xx'}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Davlan/bert-base-multilingual-cased-ner-hrl': {'modelId': 'Davlan/bert-base-multilingual-cased-ner-hrl',\n",
       "  'sha': '6f69c39cadcdba0ab1401fb1f164964e7557e471',\n",
       "  'lastModified': '2022-06-25T17:01:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'ar',\n",
       "   'de',\n",
       "   'en',\n",
       "   'es',\n",
       "   'fr',\n",
       "   'it',\n",
       "   'lv',\n",
       "   'nl',\n",
       "   'pt',\n",
       "   'zh',\n",
       "   'multilingual',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'Davlan',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Davlan/bert-base-multilingual-cased-ner-hrl',\n",
       "  'downloads': 60486,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'إسمي محمد وأسكن في برلين'},\n",
       "   {'text': 'إسمي ساره وأسكن في لندن'},\n",
       "   {'text': 'إسمي سامي وأسكن في القدس في فلسطين.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ar',\n",
       "    'de',\n",
       "    'en',\n",
       "    'es',\n",
       "    'fr',\n",
       "    'it',\n",
       "    'lv',\n",
       "    'nl',\n",
       "    'pt',\n",
       "    'zh',\n",
       "    'multilingual']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-nl-en': {'modelId': 'Helsinki-NLP/opus-mt-nl-en',\n",
       "  'sha': '642ab6dc2d08ca9b5706ff44191dc443eae738e1',\n",
       "  'lastModified': '2021-09-10T13:59:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'nl',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-nl-en',\n",
       "  'downloads': 60478,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-mul-en': {'modelId': 'Helsinki-NLP/opus-mt-mul-en',\n",
       "  'sha': 'bc0ba94fb12f8b8cf88bd8a925b15ccd5fb94340',\n",
       "  'lastModified': '2020-08-21T14:42:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'ca',\n",
       "   'es',\n",
       "   'os',\n",
       "   'eo',\n",
       "   'ro',\n",
       "   'fy',\n",
       "   'cy',\n",
       "   'is',\n",
       "   'lb',\n",
       "   'su',\n",
       "   'an',\n",
       "   'sq',\n",
       "   'fr',\n",
       "   'ht',\n",
       "   'rm',\n",
       "   'cv',\n",
       "   'ig',\n",
       "   'am',\n",
       "   'eu',\n",
       "   'tr',\n",
       "   'ps',\n",
       "   'af',\n",
       "   'ny',\n",
       "   'ch',\n",
       "   'uk',\n",
       "   'sl',\n",
       "   'lt',\n",
       "   'tk',\n",
       "   'sg',\n",
       "   'ar',\n",
       "   'lg',\n",
       "   'bg',\n",
       "   'be',\n",
       "   'ka',\n",
       "   'gd',\n",
       "   'ja',\n",
       "   'si',\n",
       "   'br',\n",
       "   'mh',\n",
       "   'km',\n",
       "   'th',\n",
       "   'ty',\n",
       "   'rw',\n",
       "   'te',\n",
       "   'mk',\n",
       "   'or',\n",
       "   'wo',\n",
       "   'kl',\n",
       "   'mr',\n",
       "   'ru',\n",
       "   'yo',\n",
       "   'hu',\n",
       "   'fo',\n",
       "   'zh',\n",
       "   'ti',\n",
       "   'co',\n",
       "   'ee',\n",
       "   'oc',\n",
       "   'sn',\n",
       "   'mt',\n",
       "   'ts',\n",
       "   'pl',\n",
       "   'gl',\n",
       "   'nb',\n",
       "   'bn',\n",
       "   'tt',\n",
       "   'bo',\n",
       "   'lo',\n",
       "   'id',\n",
       "   'gn',\n",
       "   'nv',\n",
       "   'hy',\n",
       "   'kn',\n",
       "   'to',\n",
       "   'io',\n",
       "   'so',\n",
       "   'vi',\n",
       "   'da',\n",
       "   'fj',\n",
       "   'gv',\n",
       "   'sm',\n",
       "   'nl',\n",
       "   'mi',\n",
       "   'pt',\n",
       "   'hi',\n",
       "   'se',\n",
       "   'as',\n",
       "   'ta',\n",
       "   'et',\n",
       "   'kw',\n",
       "   'ga',\n",
       "   'sv',\n",
       "   'ln',\n",
       "   'na',\n",
       "   'mn',\n",
       "   'gu',\n",
       "   'wa',\n",
       "   'lv',\n",
       "   'jv',\n",
       "   'el',\n",
       "   'my',\n",
       "   'ba',\n",
       "   'it',\n",
       "   'hr',\n",
       "   'ur',\n",
       "   'ce',\n",
       "   'nn',\n",
       "   'fi',\n",
       "   'mg',\n",
       "   'rn',\n",
       "   'xh',\n",
       "   'ab',\n",
       "   'de',\n",
       "   'cs',\n",
       "   'he',\n",
       "   'zu',\n",
       "   'yi',\n",
       "   'ml',\n",
       "   'mul',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-mul-en',\n",
       "  'downloads': 60017,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ca',\n",
       "    'es',\n",
       "    'os',\n",
       "    'eo',\n",
       "    'ro',\n",
       "    'fy',\n",
       "    'cy',\n",
       "    'is',\n",
       "    'lb',\n",
       "    'su',\n",
       "    'an',\n",
       "    'sq',\n",
       "    'fr',\n",
       "    'ht',\n",
       "    'rm',\n",
       "    'cv',\n",
       "    'ig',\n",
       "    'am',\n",
       "    'eu',\n",
       "    'tr',\n",
       "    'ps',\n",
       "    'af',\n",
       "    'ny',\n",
       "    'ch',\n",
       "    'uk',\n",
       "    'sl',\n",
       "    'lt',\n",
       "    'tk',\n",
       "    'sg',\n",
       "    'ar',\n",
       "    'lg',\n",
       "    'bg',\n",
       "    'be',\n",
       "    'ka',\n",
       "    'gd',\n",
       "    'ja',\n",
       "    'si',\n",
       "    'br',\n",
       "    'mh',\n",
       "    'km',\n",
       "    'th',\n",
       "    'ty',\n",
       "    'rw',\n",
       "    'te',\n",
       "    'mk',\n",
       "    'or',\n",
       "    'wo',\n",
       "    'kl',\n",
       "    'mr',\n",
       "    'ru',\n",
       "    'yo',\n",
       "    'hu',\n",
       "    'fo',\n",
       "    'zh',\n",
       "    'ti',\n",
       "    'co',\n",
       "    'ee',\n",
       "    'oc',\n",
       "    'sn',\n",
       "    'mt',\n",
       "    'ts',\n",
       "    'pl',\n",
       "    'gl',\n",
       "    'nb',\n",
       "    'bn',\n",
       "    'tt',\n",
       "    'bo',\n",
       "    'lo',\n",
       "    'id',\n",
       "    'gn',\n",
       "    'nv',\n",
       "    'hy',\n",
       "    'kn',\n",
       "    'to',\n",
       "    'io',\n",
       "    'so',\n",
       "    'vi',\n",
       "    'da',\n",
       "    'fj',\n",
       "    'gv',\n",
       "    'sm',\n",
       "    'nl',\n",
       "    'mi',\n",
       "    'pt',\n",
       "    'hi',\n",
       "    'se',\n",
       "    'as',\n",
       "    'ta',\n",
       "    'et',\n",
       "    'kw',\n",
       "    'ga',\n",
       "    'sv',\n",
       "    'ln',\n",
       "    'na',\n",
       "    'mn',\n",
       "    'gu',\n",
       "    'wa',\n",
       "    'lv',\n",
       "    'jv',\n",
       "    'el',\n",
       "    'my',\n",
       "    'ba',\n",
       "    'it',\n",
       "    'hr',\n",
       "    'ur',\n",
       "    'ce',\n",
       "    'nn',\n",
       "    'fi',\n",
       "    'mg',\n",
       "    'rn',\n",
       "    'xh',\n",
       "    'ab',\n",
       "    'de',\n",
       "    'cs',\n",
       "    'he',\n",
       "    'zu',\n",
       "    'yi',\n",
       "    'ml',\n",
       "    'mul',\n",
       "    'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-distilbert': {'modelId': 'hf-internal-testing/tiny-random-distilbert',\n",
       "  'sha': '2ef615d573271690c9822df720b8024148d6715a',\n",
       "  'lastModified': '2021-11-26T16:32:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'architectures': ['DistilBertForSequenceClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'hf-internal-testing/tiny-random-distilbert',\n",
       "  'downloads': 59962,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'text-classification'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/pegasus-large': {'modelId': 'google/pegasus-large',\n",
       "  'sha': '51b039cd8c644561432f7bfbe75e65f720b38f66',\n",
       "  'lastModified': '2021-09-14T07:50:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'pegasus',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:1912.08777',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['PegasusForConditionalGeneration'],\n",
       "   'model_type': 'pegasus',\n",
       "   'task_specific_params': {'summarization_aeslc': {'length_penalty': 0.6,\n",
       "     'max_length': 32,\n",
       "     'max_position_embeddings': 512},\n",
       "    'summarization_arxiv': {'length_penalty': 0.8,\n",
       "     'max_length': 256,\n",
       "     'max_position_embeddings': 1024},\n",
       "    'summarization_big_patent': {'length_penalty': 0.7,\n",
       "     'max_length': 256,\n",
       "     'max_position_embeddings': 1024},\n",
       "    'summarization_billsum': {'length_penalty': 0.6,\n",
       "     'max_length': 256,\n",
       "     'max_position_embeddings': 1024},\n",
       "    'summarization_cnn_dailymail': {'length_penalty': 0.8,\n",
       "     'max_length': 128,\n",
       "     'max_position_embeddings': 1024},\n",
       "    'summarization_gigaword': {'length_penalty': 0.6,\n",
       "     'max_length': 32,\n",
       "     'max_position_embeddings': 128},\n",
       "    'summarization_large': {'length_penalty': 0.8,\n",
       "     'max_length': 256,\n",
       "     'max_position_embeddings': 1024},\n",
       "    'summarization_multi_news': {'length_penalty': 0.8,\n",
       "     'max_length': 256,\n",
       "     'max_position_embeddings': 1024},\n",
       "    'summarization_newsroom': {'length_penalty': 0.8,\n",
       "     'max_length': 128,\n",
       "     'max_position_embeddings': 512},\n",
       "    'summarization_pubmed': {'length_penalty': 0.8,\n",
       "     'max_length': 256,\n",
       "     'max_position_embeddings': 1024},\n",
       "    'summarization_reddit_tifu': {'length_penalty': 0.6,\n",
       "     'max_length': 128,\n",
       "     'max_position_embeddings': 512},\n",
       "    'summarization_wikihow': {'length_penalty': 0.6,\n",
       "     'max_length': 256,\n",
       "     'max_position_embeddings': 512},\n",
       "    'summarization_xsum': {'length_penalty': 0.8,\n",
       "     'max_length': 64,\n",
       "     'max_position_embeddings': 512}}},\n",
       "  'id': 'google/pegasus-large',\n",
       "  'downloads': 59449,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 20,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'tags': ['summarization']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/DialoGPT-small': {'modelId': 'microsoft/DialoGPT-small',\n",
       "  'sha': 'f9c829d0285e7addb0667aeb6e33956916ec6cd0',\n",
       "  'lastModified': '2021-05-23T09:14:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'arxiv:1911.00536',\n",
       "   'transformers',\n",
       "   'conversational',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'microsoft/DialoGPT-small',\n",
       "  'downloads': 59436,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/dialogpt.png',\n",
       "   'tags': ['conversational'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'oliverguhr/german-sentiment-bert': {'modelId': 'oliverguhr/german-sentiment-bert',\n",
       "  'sha': 'c5c8dd0c5b966460dce1b7c5851bd90af1d2c6b6',\n",
       "  'lastModified': '2022-07-04T08:59:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'sentiment',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'oliverguhr',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'oliverguhr/german-sentiment-bert',\n",
       "  'downloads': 59391,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Das ist gar nicht mal so schlecht'}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['de'],\n",
       "   'tags': ['sentiment', 'bert'],\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': 'Das ist gar nicht mal so schlecht'}],\n",
       "   'metrics': ['f1']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-base-german-uncased': {'modelId': 'dbmdz/bert-base-german-uncased',\n",
       "  'sha': '26dd62c10449c89b8029c4440855983dfc5a5e83',\n",
       "  'lastModified': '2021-05-19T14:57:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-base-german-uncased',\n",
       "  'downloads': 59116,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ntu-spml/distilhubert': {'modelId': 'ntu-spml/distilhubert',\n",
       "  'sha': '9c4eece5b1dd98770108a416c101096fb04813de',\n",
       "  'lastModified': '2021-11-05T12:43:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'hubert',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'dataset:librispeech_asr',\n",
       "   'arxiv:2110.01900',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'ntu-spml',\n",
       "  'config': {'architectures': ['HubertModel'], 'model_type': 'hubert'},\n",
       "  'id': 'ntu-spml/distilhubert',\n",
       "  'downloads': 58815,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['librispeech_asr'],\n",
       "   'tags': ['speech'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/bert-base-nli-stsb-mean-tokens': {'modelId': 'sentence-transformers/bert-base-nli-stsb-mean-tokens',\n",
       "  'sha': '1bd90bb33d5c6601f5fbd26d91e955a65059ee55',\n",
       "  'lastModified': '2022-06-15T20:01:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/bert-base-nli-stsb-mean-tokens',\n",
       "  'downloads': 58115,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flair/ner-english-fast': {'modelId': 'flair/ner-english-fast',\n",
       "  'sha': '3d3d35790f78a00ef319939b9004209d1d05f788',\n",
       "  'lastModified': '2021-02-26T15:39:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'dataset:conll2003',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/ner-english-fast',\n",
       "  'downloads': 57903,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'George Washington went to Washington'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': 'en',\n",
       "   'datasets': ['conll2003'],\n",
       "   'widget': [{'text': 'George Washington went to Washington'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'typeform/distilbert-base-uncased-mnli': {'modelId': 'typeform/distilbert-base-uncased-mnli',\n",
       "  'sha': '996dacf8ea284d96ea21f88a345fd7d597de1f1f',\n",
       "  'lastModified': '2022-06-24T15:43:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:multi_nli',\n",
       "   'arxiv:1910.09700',\n",
       "   'arxiv:2105.09680',\n",
       "   'transformers',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'typeform',\n",
       "  'config': {'architectures': ['DistilBertForSequenceClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'typeform/distilbert-base-uncased-mnli',\n",
       "  'downloads': 57675,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system’s largest world. Konstantin Batygin did not set out to solve one of the solar system’s most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system’s missing “Planet Nine,” spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn’t rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: “Oh! This is how Europa formed.” Europa is one of Jupiter’s four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the Côte d’Azur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system’s formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'tags': ['distilbert'],\n",
       "   'datasets': ['multi_nli'],\n",
       "   'metrics': ['accuracy']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-it-en': {'modelId': 'Helsinki-NLP/opus-mt-it-en',\n",
       "  'sha': '23f2c7f29233a3e0accc900625d65ddf6a49b93e',\n",
       "  'lastModified': '2021-09-10T13:52:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'it',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-it-en',\n",
       "  'downloads': 57087,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Mi chiamo Wolfgang e vivo a Berlino'},\n",
       "   {'text': 'Mi chiamo Sarah e vivo a Londra'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dmis-lab/biobert-base-cased-v1.1': {'modelId': 'dmis-lab/biobert-base-cased-v1.1',\n",
       "  'sha': '924f12e0c3db7f156a765ad53fb6b11e7afedbc8',\n",
       "  'lastModified': '2020-10-14T07:02:59.000Z',\n",
       "  'tags': ['pytorch', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dmis-lab',\n",
       "  'config': {},\n",
       "  'id': 'dmis-lab/biobert-base-cased-v1.1',\n",
       "  'downloads': 57060,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'ynie/albert-xxlarge-v2-snli_mnli_fever_anli_R1_R2_R3-nli': {'modelId': 'ynie/albert-xxlarge-v2-snli_mnli_fever_anli_R1_R2_R3-nli',\n",
       "  'sha': 'ff7d96201d917e7fcc8b5b95f2631602a0777428',\n",
       "  'lastModified': '2020-10-17T02:05:17.000Z',\n",
       "  'tags': ['pytorch', 'albert', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'ynie',\n",
       "  'config': {'architectures': ['AlbertForSequenceClassification'],\n",
       "   'model_type': 'albert'},\n",
       "  'id': 'ynie/albert-xxlarge-v2-snli_mnli_fever_anli_R1_R2_R3-nli',\n",
       "  'downloads': 56543,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/msmarco-distilbert-dot-v5': {'modelId': 'sentence-transformers/msmarco-distilbert-dot-v5',\n",
       "  'sha': '52b2679c1e6789ee4b2d3b81a27a4590a1bc5348',\n",
       "  'lastModified': '2022-06-15T20:15:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/msmarco-distilbert-dot-v5',\n",
       "  'downloads': 56396,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/dpr-question_encoder-multiset-base': {'modelId': 'facebook/dpr-question_encoder-multiset-base',\n",
       "  'sha': '1b547dba8676a9b96d143a6fffabe21b50553928',\n",
       "  'lastModified': '2020-11-25T16:59:33.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'dpr', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['DPRQuestionEncoder'], 'model_type': 'dpr'},\n",
       "  'id': 'facebook/dpr-question_encoder-multiset-base',\n",
       "  'downloads': 54639,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sshleifer/tiny-gpt2': {'modelId': 'sshleifer/tiny-gpt2',\n",
       "  'sha': '5f91d94bd9cd7190a9f3216ff93cd1dd95f2c7be',\n",
       "  'lastModified': '2021-05-23T12:55:11.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'jax', 'gpt2', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'sshleifer/tiny-gpt2',\n",
       "  'downloads': 54282,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dccuchile/bert-base-spanish-wwm-cased': {'modelId': 'dccuchile/bert-base-spanish-wwm-cased',\n",
       "  'sha': '56a7647b957a4230fc3f80dafbe80f2ba9b0de73',\n",
       "  'lastModified': '2022-05-31T15:01:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'es',\n",
       "   'arxiv:1904.09077',\n",
       "   'arxiv:1906.01502',\n",
       "   'arxiv:1812.10464',\n",
       "   'arxiv:1901.07291',\n",
       "   'arxiv:1904.02099',\n",
       "   'arxiv:1906.01569',\n",
       "   'arxiv:1908.11828',\n",
       "   'transformers',\n",
       "   'masked-lm',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'dccuchile',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'dccuchile/bert-base-spanish-wwm-cased',\n",
       "  'downloads': 53907,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Mi nombre es [MASK] y vivo en Nueva York.'},\n",
       "   {'text': 'El español es un idioma muy [MASK] en el mundo.'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['es'], 'tags': ['masked-lm']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'vinai/phobert-base': {'modelId': 'vinai/phobert-base',\n",
       "  'sha': '667b55927a1571811539f27c0f374429a1c75759',\n",
       "  'lastModified': '2022-06-08T04:44:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'arxiv:2003.00744',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'vinai',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'vinai/phobert-base',\n",
       "  'downloads': 53792,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/all-mpnet-base-v1': {'modelId': 'sentence-transformers/all-mpnet-base-v1',\n",
       "  'sha': '5db7848555eaffcf26ac367f5dc9e0711acb2106',\n",
       "  'lastModified': '2021-08-31T07:35:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mpnet',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'arxiv:1904.06472',\n",
       "   'arxiv:2102.07033',\n",
       "   'arxiv:2104.08727',\n",
       "   'arxiv:1704.05179',\n",
       "   'arxiv:1810.09305',\n",
       "   'sentence-transformers',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['MPNetForMaskedLM'], 'model_type': 'mpnet'},\n",
       "  'id': 'sentence-transformers/all-mpnet-base-v1',\n",
       "  'downloads': 53617,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'valhalla/t5-base-e2e-qg': {'modelId': 'valhalla/t5-base-e2e-qg',\n",
       "  'sha': 'c652651334cd5516f2bd0f0fb5303a01a678024e',\n",
       "  'lastModified': '2021-06-23T14:40:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'dataset:squad',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'question-generation',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'valhalla',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 1.5,\n",
       "     'max_length': 256,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'generate questions: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'length_penalty': 1.5,\n",
       "     'max_length': 256,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'generate questions: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'length_penalty': 1.5,\n",
       "     'max_length': 256,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'generate questions: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'length_penalty': 1.5,\n",
       "     'max_length': 256,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'generate questions: '}}},\n",
       "  'id': 'valhalla/t5-base-e2e-qg',\n",
       "  'downloads': 52862,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Python is a programming language. It is developed by Guido Van Rossum and released in 1991. </s>'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['squad'],\n",
       "   'tags': ['question-generation'],\n",
       "   'widget': [{'text': 'Python is a programming language. It is developed by Guido Van Rossum and released in 1991. </s>'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'csebuetnlp/mT5_multilingual_XLSum': {'modelId': 'csebuetnlp/mT5_multilingual_XLSum',\n",
       "  'sha': '361416d0a10fe5df7e139081f3b5476fd39c860f',\n",
       "  'lastModified': '2021-10-03T13:14:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mt5',\n",
       "   'text2text-generation',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'az',\n",
       "   'bn',\n",
       "   'my',\n",
       "   'zh',\n",
       "   'en',\n",
       "   'fr',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'hi',\n",
       "   'ig',\n",
       "   'id',\n",
       "   'ja',\n",
       "   'rn',\n",
       "   'ko',\n",
       "   'ky',\n",
       "   'mr',\n",
       "   'ne',\n",
       "   'om',\n",
       "   'ps',\n",
       "   'fa',\n",
       "   'pcm',\n",
       "   'pt',\n",
       "   'pa',\n",
       "   'ru',\n",
       "   'gd',\n",
       "   'sr',\n",
       "   'si',\n",
       "   'so',\n",
       "   'es',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'th',\n",
       "   'ti',\n",
       "   'tr',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'cy',\n",
       "   'yo',\n",
       "   'dataset:csebuetnlp/xlsum',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'mT5',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'csebuetnlp',\n",
       "  'config': {'architectures': ['MT5ForConditionalGeneration'],\n",
       "   'model_type': 'mt5'},\n",
       "  'id': 'csebuetnlp/mT5_multilingual_XLSum',\n",
       "  'downloads': 52364,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said.  The policy includes the termination of accounts of anti-vaccine influencers.  Tech giants have been criticised for not doing more to counter false health information on their sites.  In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue.  YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines.  In a blog post, the company said it had seen false claims about Covid jabs \"spill over into misinformation about vaccines in general\". The new policy covers long-approved vaccines, such as those against measles or hepatitis B.  \"We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO,\" the post said, referring to the World Health Organization.'}],\n",
       "  'likes': 44,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['summarization', 'mT5'],\n",
       "   'datasets': ['csebuetnlp/xlsum'],\n",
       "   'language': ['am',\n",
       "    'ar',\n",
       "    'az',\n",
       "    'bn',\n",
       "    'my',\n",
       "    'zh',\n",
       "    'en',\n",
       "    'fr',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'hi',\n",
       "    'ig',\n",
       "    'id',\n",
       "    'ja',\n",
       "    'rn',\n",
       "    'ko',\n",
       "    'ky',\n",
       "    'mr',\n",
       "    'ne',\n",
       "    'om',\n",
       "    'ps',\n",
       "    'fa',\n",
       "    'pcm',\n",
       "    'pt',\n",
       "    'pa',\n",
       "    'ru',\n",
       "    'gd',\n",
       "    'sr',\n",
       "    'si',\n",
       "    'so',\n",
       "    'es',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'th',\n",
       "    'ti',\n",
       "    'tr',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'cy',\n",
       "    'yo'],\n",
       "   'licenses': ['cc-by-nc-sa-4.0'],\n",
       "   'widget': [{'text': 'Videos that say approved vaccines are dangerous and cause autism, cancer or infertility are among those that will be taken down, the company said.  The policy includes the termination of accounts of anti-vaccine influencers.  Tech giants have been criticised for not doing more to counter false health information on their sites.  In July, US President Joe Biden said social media platforms were largely responsible for people\\'s scepticism in getting vaccinated by spreading misinformation, and appealed for them to address the issue.  YouTube, which is owned by Google, said 130,000 videos were removed from its platform since last year, when it implemented a ban on content spreading misinformation about Covid vaccines.  In a blog post, the company said it had seen false claims about Covid jabs \"spill over into misinformation about vaccines in general\". The new policy covers long-approved vaccines, such as those against measles or hepatitis B.  \"We\\'re expanding our medical misinformation policies on YouTube with new guidelines on currently administered vaccines that are approved and confirmed to be safe and effective by local health authorities and the WHO,\" the post said, referring to the World Health Organization.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-bert': {'modelId': 'hf-internal-testing/tiny-random-bert',\n",
       "  'sha': '9b8c223d42b2188cb49d29af482996f9d0f3e5a6',\n",
       "  'lastModified': '2021-09-17T19:21:17.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'hf-internal-testing/tiny-random-bert',\n",
       "  'downloads': 52119,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'deepset/xlm-roberta-base-squad2': {'modelId': 'deepset/xlm-roberta-base-squad2',\n",
       "  'sha': '9ecd20ab82354b46f26dbd6fab132951c34e3435',\n",
       "  'lastModified': '2021-10-21T12:16:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'question-answering',\n",
       "   'dataset:squad_v2',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['XLMRobertaForQuestionAnswering'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'deepset/xlm-roberta-base-squad2',\n",
       "  'downloads': 51977,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['squad_v2'], 'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'prajjwal1/bert-small': {'modelId': 'prajjwal1/bert-small',\n",
       "  'sha': '0ec5f86f27c1a77d704439db5e01c307ea11b9d4',\n",
       "  'lastModified': '2021-10-27T18:31:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'arxiv:1908.08962',\n",
       "   'arxiv:2110.01518',\n",
       "   'transformers',\n",
       "   'BERT',\n",
       "   'MNLI',\n",
       "   'NLI',\n",
       "   'transformer',\n",
       "   'pre-training',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'prajjwal1',\n",
       "  'config': {},\n",
       "  'id': 'prajjwal1/bert-small',\n",
       "  'downloads': 51434,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'license': ['mit'],\n",
       "   'tags': ['BERT', 'MNLI', 'NLI', 'transformer', 'pre-training']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'rsvp-ai/bertserini-bert-base-squad': {'modelId': 'rsvp-ai/bertserini-bert-base-squad',\n",
       "  'sha': '1c93f9f29544f8ce8d6ee99133f91e5bd4dfed36',\n",
       "  'lastModified': '2022-06-23T14:13:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'rsvp-ai',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'rsvp-ai/bertserini-bert-base-squad',\n",
       "  'downloads': 51080,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nghuyong/ernie-2.0-en': {'modelId': 'nghuyong/ernie-2.0-en',\n",
       "  'sha': 'c18a9f28b99a65011e3a6c61e2109f03833a447b',\n",
       "  'lastModified': '2021-05-20T01:42:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'en',\n",
       "   'arxiv:1907.12412',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'nghuyong',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'nghuyong/ernie-2.0-en',\n",
       "  'downloads': 50501,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'facebook/opt-125m': {'modelId': 'facebook/opt-125m',\n",
       "  'sha': '934b6a077313f3ee660a918a95313f5d0b136c5a',\n",
       "  'lastModified': '2022-06-22T09:52:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'opt',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'arxiv:2205.01068',\n",
       "   'arxiv:2005.14165',\n",
       "   'transformers',\n",
       "   'license:other'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['OPTForCausalLM'], 'model_type': 'opt'},\n",
       "  'id': 'facebook/opt-125m',\n",
       "  'downloads': 50319,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'inference': False,\n",
       "   'tags': ['text-generation', 'opt'],\n",
       "   'license': 'other',\n",
       "   'commercial': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flair/pos-english': {'modelId': 'flair/pos-english',\n",
       "  'sha': '22061e2903f36383754ba0101bc988c432aa4e06',\n",
       "  'lastModified': '2021-03-02T22:20:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'dataset:ontonotes',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/pos-english',\n",
       "  'downloads': 50238,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'I love Berlin.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': 'en',\n",
       "   'datasets': ['ontonotes'],\n",
       "   'widget': [{'text': 'I love Berlin.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'classla/bcms-bertic-ner': {'modelId': 'classla/bcms-bertic-ner',\n",
       "  'sha': '4bd46a99b73827a3f6a095ceafa08b6933986dc0',\n",
       "  'lastModified': '2022-02-04T14:26:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'electra',\n",
       "   'token-classification',\n",
       "   'hr',\n",
       "   'bs',\n",
       "   'sr',\n",
       "   'cnr',\n",
       "   'hbs',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'classla',\n",
       "  'config': {'architectures': ['ElectraForTokenClassification'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'classla/bcms-bertic-ner',\n",
       "  'downloads': 50224,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Zovem se Marko i živim u Zagrebu. Studirao sam u Beogradu na Filozofskom fakultetu. Obožavam album Moanin.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['hr', 'bs', 'sr', 'cnr', 'hbs'],\n",
       "   'widget': [{'text': 'Zovem se Marko i živim u Zagrebu. Studirao sam u Beogradu na Filozofskom fakultetu. Obožavam album Moanin.'}],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allenai/led-base-16384': {'modelId': 'allenai/led-base-16384',\n",
       "  'sha': '25756ed025a94fdf2bc4987af86a58fd999047ec',\n",
       "  'lastModified': '2021-01-11T14:51:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'led',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:2004.05150',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'architectures': ['LEDForConditionalGeneration'],\n",
       "   'model_type': 'led'},\n",
       "  'id': 'allenai/led-base-16384',\n",
       "  'downloads': 50216,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/chinese-roberta-wwm-ext-large': {'modelId': 'hfl/chinese-roberta-wwm-ext-large',\n",
       "  'sha': 'a25cc9e05974bd9687e528edd516f2cfdb3f5db9',\n",
       "  'lastModified': '2022-03-01T09:15:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'arxiv:1906.08101',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'hfl/chinese-roberta-wwm-ext-large',\n",
       "  'downloads': 50028,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '巴黎是[MASK]国的首都。'}, {'text': '生活的真谛是[MASK]。'}],\n",
       "  'likes': 16,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'tags': ['bert'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flair/ner-english': {'modelId': 'flair/ner-english',\n",
       "  'sha': '627fd305bf597ea90fa54a50228ccfd4b412caf5',\n",
       "  'lastModified': '2021-03-02T22:11:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'dataset:conll2003',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/ner-english',\n",
       "  'downloads': 49427,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'George Washington went to Washington'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': 'en',\n",
       "   'datasets': ['conll2003'],\n",
       "   'widget': [{'text': 'George Washington went to Washington'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'cmarkea/distilcamembert-base-ner': {'modelId': 'cmarkea/distilcamembert-base-ner',\n",
       "  'sha': '21167ca4a0fd71a615e579dc4898c4079e86b014',\n",
       "  'lastModified': '2022-05-24T15:55:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'camembert',\n",
       "   'token-classification',\n",
       "   'fr',\n",
       "   'dataset:Jean-Baptiste/wikiner_fr',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'cmarkea',\n",
       "  'config': {'architectures': ['CamembertForTokenClassification'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'cmarkea/distilcamembert-base-ner',\n",
       "  'downloads': 48855,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': \"Boulanger, habitant à Boulanger et travaillant dans le magasin Boulanger situé dans la ville de Boulanger. Boulanger a écrit le livre éponyme Boulanger édité par la maison d'édition Boulanger.\"},\n",
       "   {'text': \"Quentin Jerome Tarantino naît le 27 mars 1963 à Knoxville, dans le Tennessee. Il est le fils de Connie McHugh, une infirmière, née le 3 septembre 1946, et de Tony Tarantino, acteur et musicien amateur né à New York. Ce dernier est d'origine italienne par son père ; sa mère a des ascendances irlandaises et cherokees. Il est prénommé d'après Quint Asper, le personnage joué par Burt Reynolds dans la série Gunsmoke et Quentin Compson, personnage du roman Le Bruit et la Fureur. Son père quitte le domicile familial avant même sa naissance. En 1965, sa mère déménage à Torrance, dans la banlieue sud de Los Angeles, et se remarie avec Curtis Zastoupil, un pianiste de bar, qui lui fait découvrir le cinéma. Le couple divorce alors que le jeune Quentin a une dizaine d'années.\"}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['Jean-Baptiste/wikiner_fr'],\n",
       "   'widget': [{'text': \"Boulanger, habitant à Boulanger et travaillant dans le magasin Boulanger situé dans la ville de Boulanger. Boulanger a écrit le livre éponyme Boulanger édité par la maison d'édition Boulanger.\"},\n",
       "    {'text': \"Quentin Jerome Tarantino naît le 27 mars 1963 à Knoxville, dans le Tennessee. Il est le fils de Connie McHugh, une infirmière, née le 3 septembre 1946, et de Tony Tarantino, acteur et musicien amateur né à New York. Ce dernier est d'origine italienne par son père ; sa mère a des ascendances irlandaises et cherokees. Il est prénommé d'après Quint Asper, le personnage joué par Burt Reynolds dans la série Gunsmoke et Quentin Compson, personnage du roman Le Bruit et la Fureur. Son père quitte le domicile familial avant même sa naissance. En 1965, sa mère déménage à Torrance, dans la banlieue sud de Los Angeles, et se remarie avec Curtis Zastoupil, un pianiste de bar, qui lui fait découvrir le cinéma. Le couple divorce alors que le jeune Quentin a une dizaine d'années.\"}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nlpaueb/bert-base-uncased-contracts': {'modelId': 'nlpaueb/bert-base-uncased-contracts',\n",
       "  'sha': 'f918d2e0cf491ba2c5fcf9f82d5e9603b8c5f3ea',\n",
       "  'lastModified': '2022-04-28T14:43:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'legal',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'fill-mask'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'nlpaueb',\n",
       "  'config': {'architectures': None, 'model_type': 'bert'},\n",
       "  'id': 'nlpaueb/bert-base-uncased-contracts',\n",
       "  'downloads': 48782,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'This [MASK] Agreement is between General Motors and John Murray.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'thumbnail': 'https://i.ibb.co/p3kQ7Rw/Screenshot-2020-10-06-at-12-16-36-PM.png',\n",
       "   'tags': ['legal'],\n",
       "   'widget': [{'text': 'This [MASK] Agreement is between General Motors and John Murray.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'microsoft/deberta-v3-small': {'modelId': 'microsoft/deberta-v3-small',\n",
       "  'sha': '23bfba973812a80178eb6c2c600e85cc461ffc2c',\n",
       "  'lastModified': '2022-01-13T17:59:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'deberta-v2',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'arxiv:2111.09543',\n",
       "   'transformers',\n",
       "   'deberta',\n",
       "   'deberta-v3',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'deberta-v2'},\n",
       "  'id': 'microsoft/deberta-v3-small',\n",
       "  'downloads': 48513,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['deberta', 'deberta-v3'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sagorsarker/bangla-bert-base': {'modelId': 'sagorsarker/bangla-bert-base',\n",
       "  'sha': '315fa6f024884c29b34a3909a016decc2b068222',\n",
       "  'lastModified': '2021-09-22T09:37:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'bn',\n",
       "   'dataset:common_crawl',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:oscar',\n",
       "   'arxiv:1810.04805',\n",
       "   'arxiv:2012.14353',\n",
       "   'arxiv:2104.08613',\n",
       "   'arxiv:2107.03844',\n",
       "   'arxiv:2101.00204',\n",
       "   'transformers',\n",
       "   'bengali',\n",
       "   'bengali-lm',\n",
       "   'bangla',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'sagorsarker',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'sagorsarker/bangla-bert-base',\n",
       "  'downloads': 47979,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'আমি বাংলায় [MASK] গাই।'},\n",
       "   {'text': 'আমি [MASK] খুব ভালোবাসি। '}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'bn',\n",
       "   'tags': ['bert', 'bengali', 'bengali-lm', 'bangla'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['common_crawl', 'wikipedia', 'oscar']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'IlyaGusev/mbart_ru_sum_gazeta': {'modelId': 'IlyaGusev/mbart_ru_sum_gazeta',\n",
       "  'sha': '56aba7873fb373f9324f3e4c3880eccbf20c316d',\n",
       "  'lastModified': '2021-12-15T09:58:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mbart',\n",
       "   'text2text-generation',\n",
       "   'ru',\n",
       "   'ru-RU',\n",
       "   'dataset:IlyaGusev/gazeta',\n",
       "   'arxiv:2006.11063',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'IlyaGusev',\n",
       "  'config': {'architectures': ['MBartForConditionalGeneration'],\n",
       "   'model_type': 'mbart'},\n",
       "  'id': 'IlyaGusev/mbart_ru_sum_gazeta',\n",
       "  'downloads': 47915,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Высота башни составляет 324 метра (1063 фута), примерно такая же высота, как у 81-этажного здания, и самое высокое сооружение в Париже. Его основание квадратно, размером 125 метров (410 футов) с любой стороны. Во время строительства Эйфелева башня превзошла монумент Вашингтона, став самым высоким искусственным сооружением в мире, и этот титул она удерживала в течение 41 года до завершения строительство здания Крайслер в Нью-Йорке в 1930 году. Это первое сооружение которое достигло высоты 300 метров. Из-за добавления вещательной антенны на вершине башни в 1957 году она сейчас выше здания Крайслер на 5,2 метра (17 футов). За исключением передатчиков, Эйфелева башня является второй самой высокой отдельно стоящей структурой во Франции после виадука Мийо.',\n",
       "    'example_title': 'Википедия'},\n",
       "   {'text': 'С 1 сентября в России вступают в силу поправки в закон «О банкротстве» — теперь должники смогут освобождаться от непосильных обязательств во внесудебном порядке, если сумма задолженности составляет не менее 50 тыс. рублей и не превышает 500 тыс. рублей без учета штрафов, пени, процентов за просрочку платежа и прочих имущественных или финансовых санкций. У физлиц и индивидуальных предпринимателей появилась возможность пройти процедуру банкротства без участия суда и финансового управляющего — достаточно подать соответствующее заявление через МФЦ. Сумму задолженности и список всех известных заявителю кредиторов нужно предоставить самостоятельно. Если все условия соблюдены, сведения внесут в Единый федеральный реестр в течение трех рабочих дней. При этом на момент подачи заявления в отношении заявителя должно быть окончено исполнительное производство с возвращением исполнительного документа взыскателю. Это значит, что у потенциального банкрота не должно быть имущества, которое можно взыскать. Кроме того, в отношении гражданина не должно быть возбуждено другое исполнительное производство. В период всей процедуры заявитель не сможет брать займы, кредиты, выдавать поручительства, совершать иные обеспечительные сделки. Внесудебное банкротство будет длиться шесть месяцев, в течение которых также будет действовать мораторий на удовлетворение требований кредиторов, отмеченных в заявлении должника, и мораторий об уплате обязательных платежей. Кроме того, прекращается начисление неустоек и иных финансовых санкций; имущественные взыскания (кроме алиментов) также будут приостановлены. По завершению процедуры заявителя освободят от дальнейшего выполнения требований кредиторов, указанных в заявлении о признании его банкротом, а эта задолженность признается безнадежной. В прошлом месяце стало известно, что за первое полугодие 2020 года российские суды признали банкротами 42,7 тыс. граждан (в том числе индивидуальных предпринимателей) — по данным единого реестра «Федресурс», это на 47,2% больше показателя аналогичного периода 2019 года. Рост числа обанкротившихся граждан во втором квартале по сравнению с первым замедлился — такая динамика обусловлена тем, что в период ограничений с 19 марта по 11 мая суды редко рассматривали банкротные дела компаний и меньше, чем обычно, в отношении граждан, объяснял руководитель проекта «Федресурс» Алексей Юхнин. Он прогнозирует, что во втором полугодии мы увидим рост показателя, когда суды рассмотрят все дела, что не смогли ранее в режиме ограничений. По его данным, уже в июне число личных банкротств выросло до 11,5 тыс., что в два раза превышает показатель аналогичного периода 2019 года.',\n",
       "    'example_title': 'Новости'},\n",
       "   {'text': 'Актуальность проблемы. Электронная информация играет все большую  роль во всех сферах жизни современного общества. В последние годы объем научно-технической текстовой информации в электронном виде возрос настолько, что возникает угроза обесценивания этой информации в связи с трудностями поиска необходимых сведений среди множества доступных текстов. Развитие информационных ресурсов Интернет многократно усугубило проблему информационной перегрузки. В этой ситуации особенно актуальными становятся методы автоматизации реферирования текстовой информации, то есть методы получения сжатого представления текстовых документов–рефератов (аннотаций). Постановка  проблемы  автоматического реферирования текста и соответственно попытки ее решения с использованием различных подходов предпринимались многими исследователями. История применения вычислительной техники для реферирования  насчитывает уже более 50 лет и связана с именами таких исследователей, как Г.П. Лун, В.Е. Берзон, И.П. Cевбо, Э.Ф. Скороходько, Д.Г. Лахути, Р.Г. Пиотровский и др. За эти годы  выработаны  многочисленные подходы к решению данной проблемы, которые достаточно четко подразделяются на два направления: автоматическое реферирование, основанное на экстрагировании из первичных документов с помощью определенных формальных признаков «наиболее информативных» фраз (фрагментов), совокупность которых образует некоторый экстракт; автоматическое реферирование, основанное на выделении из текстов с помощью специальных информационных языков наиболее существенной информации и порождении новых текстов (рефератов), содержательно обобщающих первичные  документы.',\n",
       "    'example_title': 'Научная статья'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru', 'ru-RU'],\n",
       "   'tags': ['summarization', 'mbart'],\n",
       "   'datasets': ['IlyaGusev/gazeta'],\n",
       "   'license': 'apache-2.0',\n",
       "   'inference': {'parameters': {'no_repeat_ngram_size': 4}},\n",
       "   'widget': [{'text': 'Высота башни составляет 324 метра (1063 фута), примерно такая же высота, как у 81-этажного здания, и самое высокое сооружение в Париже. Его основание квадратно, размером 125 метров (410 футов) с любой стороны. Во время строительства Эйфелева башня превзошла монумент Вашингтона, став самым высоким искусственным сооружением в мире, и этот титул она удерживала в течение 41 года до завершения строительство здания Крайслер в Нью-Йорке в 1930 году. Это первое сооружение которое достигло высоты 300 метров. Из-за добавления вещательной антенны на вершине башни в 1957 году она сейчас выше здания Крайслер на 5,2 метра (17 футов). За исключением передатчиков, Эйфелева башня является второй самой высокой отдельно стоящей структурой во Франции после виадука Мийо.',\n",
       "     'example_title': 'Википедия'},\n",
       "    {'text': 'С 1 сентября в России вступают в силу поправки в закон «О банкротстве» — теперь должники смогут освобождаться от непосильных обязательств во внесудебном порядке, если сумма задолженности составляет не менее 50 тыс. рублей и не превышает 500 тыс. рублей без учета штрафов, пени, процентов за просрочку платежа и прочих имущественных или финансовых санкций. У физлиц и индивидуальных предпринимателей появилась возможность пройти процедуру банкротства без участия суда и финансового управляющего — достаточно подать соответствующее заявление через МФЦ. Сумму задолженности и список всех известных заявителю кредиторов нужно предоставить самостоятельно. Если все условия соблюдены, сведения внесут в Единый федеральный реестр в течение трех рабочих дней. При этом на момент подачи заявления в отношении заявителя должно быть окончено исполнительное производство с возвращением исполнительного документа взыскателю. Это значит, что у потенциального банкрота не должно быть имущества, которое можно взыскать. Кроме того, в отношении гражданина не должно быть возбуждено другое исполнительное производство. В период всей процедуры заявитель не сможет брать займы, кредиты, выдавать поручительства, совершать иные обеспечительные сделки. Внесудебное банкротство будет длиться шесть месяцев, в течение которых также будет действовать мораторий на удовлетворение требований кредиторов, отмеченных в заявлении должника, и мораторий об уплате обязательных платежей. Кроме того, прекращается начисление неустоек и иных финансовых санкций; имущественные взыскания (кроме алиментов) также будут приостановлены. По завершению процедуры заявителя освободят от дальнейшего выполнения требований кредиторов, указанных в заявлении о признании его банкротом, а эта задолженность признается безнадежной. В прошлом месяце стало известно, что за первое полугодие 2020 года российские суды признали банкротами 42,7 тыс. граждан (в том числе индивидуальных предпринимателей) — по данным единого реестра «Федресурс», это на 47,2% больше показателя аналогичного периода 2019 года. Рост числа обанкротившихся граждан во втором квартале по сравнению с первым замедлился — такая динамика обусловлена тем, что в период ограничений с 19 марта по 11 мая суды редко рассматривали банкротные дела компаний и меньше, чем обычно, в отношении граждан, объяснял руководитель проекта «Федресурс» Алексей Юхнин. Он прогнозирует, что во втором полугодии мы увидим рост показателя, когда суды рассмотрят все дела, что не смогли ранее в режиме ограничений. По его данным, уже в июне число личных банкротств выросло до 11,5 тыс., что в два раза превышает показатель аналогичного периода 2019 года.',\n",
       "     'example_title': 'Новости'},\n",
       "    {'text': 'Актуальность проблемы. Электронная информация играет все большую  роль во всех сферах жизни современного общества. В последние годы объем научно-технической текстовой информации в электронном виде возрос настолько, что возникает угроза обесценивания этой информации в связи с трудностями поиска необходимых сведений среди множества доступных текстов. Развитие информационных ресурсов Интернет многократно усугубило проблему информационной перегрузки. В этой ситуации особенно актуальными становятся методы автоматизации реферирования текстовой информации, то есть методы получения сжатого представления текстовых документов–рефератов (аннотаций). Постановка  проблемы  автоматического реферирования текста и соответственно попытки ее решения с использованием различных подходов предпринимались многими исследователями. История применения вычислительной техники для реферирования  насчитывает уже более 50 лет и связана с именами таких исследователей, как Г.П. Лун, В.Е. Берзон, И.П. Cевбо, Э.Ф. Скороходько, Д.Г. Лахути, Р.Г. Пиотровский и др. За эти годы  выработаны  многочисленные подходы к решению данной проблемы, которые достаточно четко подразделяются на два направления: автоматическое реферирование, основанное на экстрагировании из первичных документов с помощью определенных формальных признаков «наиболее информативных» фраз (фрагментов), совокупность которых образует некоторый экстракт; автоматическое реферирование, основанное на выделении из текстов с помощью специальных информационных языков наиболее существенной информации и порождении новых текстов (рефератов), содержательно обобщающих первичные  документы.',\n",
       "     'example_title': 'Научная статья'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/stsb-roberta-base-v2': {'modelId': 'sentence-transformers/stsb-roberta-base-v2',\n",
       "  'sha': 'b5e9e8dbc4a7d931c766a9113d1a04963a480c06',\n",
       "  'lastModified': '2022-06-15T20:05:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/stsb-roberta-base-v2',\n",
       "  'downloads': 47592,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/opt-350m': {'modelId': 'facebook/opt-350m',\n",
       "  'sha': '10517ad5b51c8c6e02db7824d8494721d4874488',\n",
       "  'lastModified': '2022-06-16T15:25:49.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'opt',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'arxiv:2205.01068',\n",
       "   'arxiv:2005.14165',\n",
       "   'transformers',\n",
       "   'license:other'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['OPTForCausalLM'], 'model_type': 'opt'},\n",
       "  'id': 'facebook/opt-350m',\n",
       "  'downloads': 47482,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'inference': False,\n",
       "   'tags': ['text-generation'],\n",
       "   'license': 'other',\n",
       "   'commercial': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Anjoe/gbert-large': {'modelId': 'Anjoe/gbert-large',\n",
       "  'sha': '0d03ce7b19d93c48c43bf2cd9640af58bcfaa536',\n",
       "  'lastModified': '2022-06-25T17:08:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'generated_from_trainer',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Anjoe',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'Anjoe/gbert-large',\n",
       "  'downloads': 47378,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'model-index': [{'name': 'gbert-large', 'results': []}],\n",
       "  'cardData': {'tags': ['generated_from_trainer'],\n",
       "   'model-index': [{'name': 'gbert-large', 'results': []}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/sentence-t5-large': {'modelId': 'sentence-transformers/sentence-t5-large',\n",
       "  'sha': '266640df151776ad39f66a2595b81c97ae678195',\n",
       "  'lastModified': '2022-02-09T14:01:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'en',\n",
       "   'arxiv:2108.08877',\n",
       "   'sentence-transformers',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['T5EncoderModel'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'sentence-transformers/sentence-t5-large',\n",
       "  'downloads': 47117,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Tatyana/rubert-base-cased-sentiment-new': {'modelId': 'Tatyana/rubert-base-cased-sentiment-new',\n",
       "  'sha': 'a1ff066aeb2b26b5f1b8d793862e51d77a1090d3',\n",
       "  'lastModified': '2021-05-30T23:12:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'ru',\n",
       "   'dataset:Tatyana/ru_sentiment_dataset',\n",
       "   'transformers',\n",
       "   'sentiment'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Tatyana',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Tatyana/rubert-base-cased-sentiment-new',\n",
       "  'downloads': 46869,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Ты мне нравишься. Я тебя люблю'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru'],\n",
       "   'tags': ['sentiment', 'text-classification'],\n",
       "   'datasets': ['Tatyana/ru_sentiment_dataset']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/paraphrase-MiniLM-L3-v2': {'modelId': 'sentence-transformers/paraphrase-MiniLM-L3-v2',\n",
       "  'sha': '055d33c3b40d293cd55714774c68fcc2de81d463',\n",
       "  'lastModified': '2022-06-15T20:08:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/paraphrase-MiniLM-L3-v2',\n",
       "  'downloads': 45848,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-fi-en': {'modelId': 'Helsinki-NLP/opus-mt-fi-en',\n",
       "  'sha': '7fb1e75696c8b8930df5afae6bb5d22ffca4ed30',\n",
       "  'lastModified': '2021-01-18T08:32:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'fi',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-fi-en',\n",
       "  'downloads': 45724,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['fi', 'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-roberta': {'modelId': 'hf-internal-testing/tiny-random-roberta',\n",
       "  'sha': '73def02fc9f13169a1ce21ad4602aae38d7cbd5a',\n",
       "  'lastModified': '2021-09-17T19:22:24.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'roberta', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'model_type': 'roberta'},\n",
       "  'id': 'hf-internal-testing/tiny-random-roberta',\n",
       "  'downloads': 45646,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'TurkuNLP/bert-base-finnish-cased-v1': {'modelId': 'TurkuNLP/bert-base-finnish-cased-v1',\n",
       "  'sha': '9800b205abb21a898401af85073e2849699f999b',\n",
       "  'lastModified': '2022-06-10T08:43:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'fi',\n",
       "   'arxiv:1912.07076',\n",
       "   'arxiv:1908.04212',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'TurkuNLP',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'TurkuNLP/bert-base-finnish-cased-v1',\n",
       "  'downloads': 45286,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fi'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-ar-en': {'modelId': 'Helsinki-NLP/opus-mt-ar-en',\n",
       "  'sha': '9c5efffe6f69dcb65d7156f40dfa27b54be34258',\n",
       "  'lastModified': '2021-09-09T21:26:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'ar',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-ar-en',\n",
       "  'downloads': 45269,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'إسمي محمد وأسكن في برلين'},\n",
       "   {'text': 'إسمي ساره وأسكن في لندن'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/mbart-large-cc25': {'modelId': 'facebook/mbart-large-cc25',\n",
       "  'sha': '2df0e6dd8a0e7f6df056fe4d0d95941a04b64e4f',\n",
       "  'lastModified': '2021-03-10T03:48:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mbart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'ar',\n",
       "   'cs',\n",
       "   'de',\n",
       "   'et',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'gu',\n",
       "   'hi',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'kk',\n",
       "   'ko',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'si',\n",
       "   'tr',\n",
       "   'vi',\n",
       "   'zh',\n",
       "   'multilingual',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['MBartForConditionalGeneration'],\n",
       "   'model_type': 'mbart',\n",
       "   'task_specific_params': {'translation_en_to_ro': {'decoder_start_token_id': 250020}}},\n",
       "  'id': 'facebook/mbart-large-cc25',\n",
       "  'downloads': 45090,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 15,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'],\n",
       "   'language': ['en',\n",
       "    'ar',\n",
       "    'cs',\n",
       "    'de',\n",
       "    'et',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'gu',\n",
       "    'hi',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'kk',\n",
       "    'ko',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'si',\n",
       "    'tr',\n",
       "    'vi',\n",
       "    'zh',\n",
       "    'multilingual']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'HooshvareLab/bert-base-parsbert-uncased': {'modelId': 'HooshvareLab/bert-base-parsbert-uncased',\n",
       "  'sha': 'd73a0e2c7492c33bd5819bcdb23eba207404dd19',\n",
       "  'lastModified': '2021-05-18T20:47:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'arxiv:2005.12515',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'HooshvareLab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'HooshvareLab/bert-base-parsbert-uncased',\n",
       "  'downloads': 45084,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'DeepChem/ChemBERTa-10M-MTR': {'modelId': 'DeepChem/ChemBERTa-10M-MTR',\n",
       "  'sha': '53d08e205841e15b77483b42aac0c55bdb1d1822',\n",
       "  'lastModified': '2022-01-20T17:51:35.000Z',\n",
       "  'tags': ['pytorch', 'roberta', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'DeepChem',\n",
       "  'config': {'architectures': ['RobertaForRegression'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'DeepChem/ChemBERTa-10M-MTR',\n",
       "  'downloads': 44792,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'RobertaForRegression',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/contriever': {'modelId': 'facebook/contriever',\n",
       "  'sha': '2bd46a25019aeea091fd42d1f0fd4801675cf699',\n",
       "  'lastModified': '2022-01-19T17:23:28.000Z',\n",
       "  'tags': ['pytorch', 'bert', 'arxiv:2112.09118', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Contriever'], 'model_type': 'bert'},\n",
       "  'id': 'facebook/contriever',\n",
       "  'downloads': 44408,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'Contriever',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-sv-en': {'modelId': 'Helsinki-NLP/opus-mt-sv-en',\n",
       "  'sha': 'e4f28e50a1614873bbe8d16d3c48b52e37778ead',\n",
       "  'lastModified': '2021-09-10T14:06:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'sv',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-sv-en',\n",
       "  'downloads': 44343,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/t5-v1_1-xxl': {'modelId': 'google/t5-v1_1-xxl',\n",
       "  'sha': '6e80045f023a868fdb58e0b697d1ace5fe4880be',\n",
       "  'lastModified': '2020-11-19T19:55:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:c4',\n",
       "   'arxiv:2002.05202',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/t5-v1_1-xxl',\n",
       "  'downloads': 44096,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'datasets': ['c4'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'vicgalle/xlm-roberta-large-xnli-anli': {'modelId': 'vicgalle/xlm-roberta-large-xnli-anli',\n",
       "  'sha': '81de27372786f6a034c81c4bf1c53ebe9afa10d7',\n",
       "  'lastModified': '2021-03-04T17:05:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'text-classification',\n",
       "   'multilingual',\n",
       "   'dataset:mnli',\n",
       "   'dataset:xnli',\n",
       "   'dataset:anli',\n",
       "   'transformers',\n",
       "   'zero-shot-classification',\n",
       "   'nli',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'vicgalle',\n",
       "  'config': {'architectures': ['XLMRobertaForSequenceClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'vicgalle/xlm-roberta-large-xnli-anli',\n",
       "  'downloads': 44078,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'De pugna erat fantastic. Nam Crixo decem quam dilexit et praeciderunt caput aemulus.',\n",
       "    'candidate_labels': 'violent, peaceful'},\n",
       "   {'text': 'La película empezaba bien pero terminó siendo un desastre.',\n",
       "    'candidate_labels': 'positivo, negativo, neutral'},\n",
       "   {'text': 'La película empezó siendo un desastre pero en general fue bien.',\n",
       "    'candidate_labels': 'positivo, negativo, neutral'},\n",
       "   {'text': '¿A quién vas a votar en 2020?',\n",
       "    'candidate_labels': 'Europa, elecciones, política, ciencia, deportes'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'tags': ['zero-shot-classification', 'nli', 'pytorch'],\n",
       "   'datasets': ['mnli', 'xnli', 'anli'],\n",
       "   'license': 'mit',\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'widget': [{'text': 'De pugna erat fantastic. Nam Crixo decem quam dilexit et praeciderunt caput aemulus.',\n",
       "     'candidate_labels': 'violent, peaceful'},\n",
       "    {'text': 'La película empezaba bien pero terminó siendo un desastre.',\n",
       "     'candidate_labels': 'positivo, negativo, neutral'},\n",
       "    {'text': 'La película empezó siendo un desastre pero en general fue bien.',\n",
       "     'candidate_labels': 'positivo, negativo, neutral'},\n",
       "    {'text': '¿A quién vas a votar en 2020?',\n",
       "     'candidate_labels': 'Europa, elecciones, política, ciencia, deportes'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/electra-small-generator': {'modelId': 'google/electra-small-generator',\n",
       "  'sha': 'c04c64e3cca372b13615e71e51bc261f93905212',\n",
       "  'lastModified': '2021-04-29T15:23:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'electra',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['ElectraForMaskedLM'], 'model_type': 'electra'},\n",
       "  'id': 'google/electra-small-generator',\n",
       "  'downloads': 43598,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-electra': {'modelId': 'hf-internal-testing/tiny-random-electra',\n",
       "  'sha': 'e5efa6ccdff6b9cd0c387c90ff4af92686bc0c12',\n",
       "  'lastModified': '2021-09-17T19:22:20.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'electra', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'model_type': 'electra'},\n",
       "  'id': 'hf-internal-testing/tiny-random-electra',\n",
       "  'downloads': 43449,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'cointegrated/rubert-tiny': {'modelId': 'cointegrated/rubert-tiny',\n",
       "  'sha': 'f191937ca7511ae76b44b06a460f18ab1699c54b',\n",
       "  'lastModified': '2022-01-28T11:42:37.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'pretraining',\n",
       "   'ru',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'russian',\n",
       "   'fill-mask',\n",
       "   'embeddings',\n",
       "   'masked-lm',\n",
       "   'tiny',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'cointegrated',\n",
       "  'config': {'architectures': ['BertForPreTraining'], 'model_type': 'bert'},\n",
       "  'id': 'cointegrated/rubert-tiny',\n",
       "  'downloads': 43411,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Миниатюрная модель для [MASK] разных задач.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru', 'en'],\n",
       "   'tags': ['russian',\n",
       "    'fill-mask',\n",
       "    'pretraining',\n",
       "    'embeddings',\n",
       "    'masked-lm',\n",
       "    'tiny',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity'],\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': 'Миниатюрная модель для [MASK] разных задач.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-albert': {'modelId': 'hf-internal-testing/tiny-random-albert',\n",
       "  'sha': '213e37e1c19981fbcee4f5f9ebefee8db74c8db1',\n",
       "  'lastModified': '2021-09-17T19:23:59.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'albert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'model_type': 'albert'},\n",
       "  'id': 'hf-internal-testing/tiny-random-albert',\n",
       "  'downloads': 43398,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'cross-encoder/qnli-distilroberta-base': {'modelId': 'cross-encoder/qnli-distilroberta-base',\n",
       "  'sha': 'c7102de981e15ca7ef131517b94ff770d9e3c166',\n",
       "  'lastModified': '2021-08-05T08:41:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'arxiv:1804.07461',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cross-encoder/qnli-distilroberta-base',\n",
       "  'downloads': 43242,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'vennify/t5-base-grammar-correction': {'modelId': 'vennify/t5-base-grammar-correction',\n",
       "  'sha': '9e4a09d21dca1072a69302df9261289d03c3ed78',\n",
       "  'lastModified': '2022-01-14T16:35:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:jfleg',\n",
       "   'arxiv:1702.04066',\n",
       "   'transformers',\n",
       "   'grammar',\n",
       "   'license:cc-by-nc-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'vennify',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'vennify/t5-base-grammar-correction',\n",
       "  'downloads': 43151,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['grammar', 'text2text-generation'],\n",
       "   'license': 'cc-by-nc-sa-4.0',\n",
       "   'datasets': ['jfleg']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/multi-qa-mpnet-base-cos-v1': {'modelId': 'sentence-transformers/multi-qa-mpnet-base-cos-v1',\n",
       "  'sha': 'bd0b4f6d767d5cb937b4c1a9611df492a80e891a',\n",
       "  'lastModified': '2021-08-24T21:07:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mpnet',\n",
       "   'fill-mask',\n",
       "   'sentence-transformers',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['MPNetForMaskedLM'], 'model_type': 'mpnet'},\n",
       "  'id': 'sentence-transformers/multi-qa-mpnet-base-cos-v1',\n",
       "  'downloads': 43087,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/graphcodebert-base': {'modelId': 'microsoft/graphcodebert-base',\n",
       "  'sha': '2ff24803553d2274dd118c7ea20e9b37a5804b11',\n",
       "  'lastModified': '2021-07-21T16:26:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'microsoft/graphcodebert-base',\n",
       "  'downloads': 42870,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-large-cased-whole-word-masking-finetuned-squad': {'modelId': 'bert-large-cased-whole-word-masking-finetuned-squad',\n",
       "  'sha': 'ba9ccd18e456b6c6a63a3ea5b21776f05452d923',\n",
       "  'lastModified': '2021-05-18T16:22:37.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'bert-large-cased-whole-word-masking-finetuned-squad',\n",
       "  'downloads': 42567,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/distiluse-base-multilingual-cased-v1': {'modelId': 'sentence-transformers/distiluse-base-multilingual-cased-v1',\n",
       "  'sha': '756c7aa7d57c27bd1c71a483367c53966465f450',\n",
       "  'lastModified': '2022-06-15T20:11:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'multilingual',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/distiluse-base-multilingual-cased-v1',\n",
       "  'downloads': 42314,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'language': 'multilingual',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sshleifer/tiny-distilroberta-base': {'modelId': 'sshleifer/tiny-distilroberta-base',\n",
       "  'sha': 'd305c58110158c865cb6746c62d4511d4148a934',\n",
       "  'lastModified': '2021-10-22T16:10:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'sshleifer/tiny-distilroberta-base',\n",
       "  'downloads': 42077,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'prajjwal1/bert-medium': {'modelId': 'prajjwal1/bert-medium',\n",
       "  'sha': 'ce27ec2944bd32b66ed837edb9c77eb7301b8ecc',\n",
       "  'lastModified': '2021-10-27T18:30:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'arxiv:1908.08962',\n",
       "   'arxiv:2110.01518',\n",
       "   'transformers',\n",
       "   'BERT',\n",
       "   'MNLI',\n",
       "   'NLI',\n",
       "   'transformer',\n",
       "   'pre-training',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'prajjwal1',\n",
       "  'config': {},\n",
       "  'id': 'prajjwal1/bert-medium',\n",
       "  'downloads': 41519,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'license': ['mit'],\n",
       "   'tags': ['BERT', 'MNLI', 'NLI', 'transformer', 'pre-training']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'antoiloui/belgpt2': {'modelId': 'antoiloui/belgpt2',\n",
       "  'sha': 'af72b5d53d2be0e47fac2df7367d7205cd73e8dd',\n",
       "  'lastModified': '2021-05-21T13:21:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'fr',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'antoiloui',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': None},\n",
       "  'id': 'antoiloui/belgpt2',\n",
       "  'downloads': 41380,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hier, Elon Musk a'},\n",
       "   {'text': 'Pourquoi a-t-il'},\n",
       "   {'text': 'Tout à coup, elle'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['fr'],\n",
       "   'license': ['mit'],\n",
       "   'widget': [{'text': 'Hier, Elon Musk a'},\n",
       "    {'text': 'Pourquoi a-t-il'},\n",
       "    {'text': 'Tout à coup, elle'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'distilbert-base-german-cased': {'modelId': 'distilbert-base-german-cased',\n",
       "  'sha': '06b1dc5ba050ddbf462d060df38f906eedb31b01',\n",
       "  'lastModified': '2022-06-03T09:46:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'distilbert',\n",
       "   'fill-mask',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['DistilBertForMaskedLM'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'distilbert-base-german-cased',\n",
       "  'downloads': 41176,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de', 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'distilbert-base-uncased-distilled-squad': {'modelId': 'distilbert-base-uncased-distilled-squad',\n",
       "  'sha': '5fd13b8fe336f632c44f0e02c27cad425c56b3b1',\n",
       "  'lastModified': '2020-12-11T21:24:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'tflite',\n",
       "   'distilbert',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'dataset:squad',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['DistilBertForQuestionAnswering'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'distilbert-base-uncased-distilled-squad',\n",
       "  'downloads': 40995,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'},\n",
       "   {'text': 'How many square kilometers of rainforest is covered in the basin?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['squad'],\n",
       "   'widget': [{'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "     'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'},\n",
       "    {'text': 'How many square kilometers of rainforest is covered in the basin?',\n",
       "     'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nlpaueb/legal-bert-base-uncased': {'modelId': 'nlpaueb/legal-bert-base-uncased',\n",
       "  'sha': '15b570cbf88259610b082a167dacc190124f60f6',\n",
       "  'lastModified': '2022-04-28T14:42:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'pretraining',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'legal',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'fill-mask'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'nlpaueb',\n",
       "  'config': {'architectures': ['BertForPreTraining'], 'model_type': 'bert'},\n",
       "  'id': 'nlpaueb/legal-bert-base-uncased',\n",
       "  'downloads': 40895,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'The applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of police.'}],\n",
       "  'likes': 23,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'thumbnail': 'https://i.ibb.co/p3kQ7Rw/Screenshot-2020-10-06-at-12-16-36-PM.png',\n",
       "   'tags': ['legal'],\n",
       "   'widget': [{'text': 'The applicant submitted that her husband was subjected to treatment amounting to [MASK] whilst in the custody of police.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/quora-distilroberta-base': {'modelId': 'cross-encoder/quora-distilroberta-base',\n",
       "  'sha': '2f10e5b229ecdb2ca204717607c7635897fd645b',\n",
       "  'lastModified': '2021-08-05T08:41:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cross-encoder/quora-distilroberta-base',\n",
       "  'downloads': 40842,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/chinese-electra-180g-small-discriminator': {'modelId': 'hfl/chinese-electra-180g-small-discriminator',\n",
       "  'sha': '19998e79c480fa7f3892b3c035e4362fe497efcf',\n",
       "  'lastModified': '2021-03-03T01:04:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'electra',\n",
       "   'pretraining',\n",
       "   'zh',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['ElectraForPreTraining'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'hfl/chinese-electra-180g-small-discriminator',\n",
       "  'downloads': 39659,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sberbank-ai/ruRoberta-large': {'modelId': 'sberbank-ai/ruRoberta-large',\n",
       "  'sha': '29b46edec511391c384dfd0bbd3892cb72495c5f',\n",
       "  'lastModified': '2021-09-21T19:45:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'ru',\n",
       "   'transformers',\n",
       "   'PyTorch',\n",
       "   'Transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'sberbank-ai',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'sberbank-ai/ruRoberta-large',\n",
       "  'downloads': 39573,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Меня зовут <mask> и я инженер живущий в Нью-Йорке.'}],\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru'],\n",
       "   'tags': ['PyTorch', 'Transformers'],\n",
       "   'thumbnail': 'https://github.com/sberbank-ai/model-zoo'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/blenderbot_small-90M': {'modelId': 'facebook/blenderbot_small-90M',\n",
       "  'sha': 'a2a23a425b397872915db19bdee2522877eddc14',\n",
       "  'lastModified': '2021-12-02T08:09:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'blenderbot-small',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:blended_skill_talk',\n",
       "   'arxiv:1907.06616',\n",
       "   'transformers',\n",
       "   'convAI',\n",
       "   'conversational',\n",
       "   'facebook',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['BlenderbotSmallForConditionalGeneration'],\n",
       "   'model_type': 'blenderbot-small'},\n",
       "  'id': 'facebook/blenderbot_small-90M',\n",
       "  'downloads': 39242,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'thumbnail': None,\n",
       "   'tags': ['convAI', 'conversational', 'facebook'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['blended_skill_talk'],\n",
       "   'metrics': ['perplexity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ThatSkyFox/DialoGPT-small-joshua': {'modelId': 'ThatSkyFox/DialoGPT-small-joshua',\n",
       "  'sha': 'b42987a9651745cfa1112354b16a1c454492045f',\n",
       "  'lastModified': '2021-10-24T17:12:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'ThatSkyFox',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'ThatSkyFox/DialoGPT-small-joshua',\n",
       "  'downloads': 38983,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Rostlab/prot_bert_bfd': {'modelId': 'Rostlab/prot_bert_bfd',\n",
       "  'sha': '6c5c8a55a52ff08a664dfd584aa1773f125a0487',\n",
       "  'lastModified': '2020-12-11T21:30:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'fill-mask',\n",
       "   'protein',\n",
       "   'dataset:BFD',\n",
       "   'transformers',\n",
       "   'protein language model',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Rostlab',\n",
       "  'config': {'architectures': ['BertForMaskedLM']},\n",
       "  'id': 'Rostlab/prot_bert_bfd',\n",
       "  'downloads': 38328,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'protein',\n",
       "   'tags': ['protein language model'],\n",
       "   'datasets': ['BFD']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask'}},\n",
       " 'sberbank-ai/rugpt3large_based_on_gpt2': {'modelId': 'sberbank-ai/rugpt3large_based_on_gpt2',\n",
       "  'sha': 'aa2b602c1939938541eed9283347d6e08536f6f8',\n",
       "  'lastModified': '2021-09-21T19:33:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'ru',\n",
       "   'transformers',\n",
       "   'PyTorch',\n",
       "   'Transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'sberbank-ai',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'], 'model_type': 'gpt2'},\n",
       "  'id': 'sberbank-ai/rugpt3large_based_on_gpt2',\n",
       "  'downloads': 38289,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Меня зовут Жюльен и'},\n",
       "   {'text': 'Меня зовут Томас и мой основной'},\n",
       "   {'text': 'Однажды'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru'],\n",
       "   'tags': ['PyTorch', 'Transformers'],\n",
       "   'thumbnail': 'https://github.com/sberbank-ai/ru-gpts'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'princeton-nlp/sup-simcse-roberta-large': {'modelId': 'princeton-nlp/sup-simcse-roberta-large',\n",
       "  'sha': 'd34da58f734b9cc9e617cc37a2321badffdd0ecf',\n",
       "  'lastModified': '2021-05-20T19:36:20.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'roberta', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'princeton-nlp',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'princeton-nlp/sup-simcse-roberta-large',\n",
       "  'downloads': 38244,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Davlan/xlm-roberta-large-ner-hrl': {'modelId': 'Davlan/xlm-roberta-large-ner-hrl',\n",
       "  'sha': '690312971788985228c98683d4b816fbf026b346',\n",
       "  'lastModified': '2022-06-27T10:49:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'token-classification',\n",
       "   'ar',\n",
       "   'de',\n",
       "   'en',\n",
       "   'es',\n",
       "   'fr',\n",
       "   'it',\n",
       "   'lv',\n",
       "   'nl',\n",
       "   'pt',\n",
       "   'zh',\n",
       "   'multilingual',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'Davlan',\n",
       "  'config': {'architectures': ['XLMRobertaForTokenClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'Davlan/xlm-roberta-large-ner-hrl',\n",
       "  'downloads': 38211,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'إسمي محمد وأسكن في برلين'},\n",
       "   {'text': 'إسمي ساره وأسكن في لندن'},\n",
       "   {'text': 'إسمي سامي وأسكن في القدس في فلسطين.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ar',\n",
       "    'de',\n",
       "    'en',\n",
       "    'es',\n",
       "    'fr',\n",
       "    'it',\n",
       "    'lv',\n",
       "    'nl',\n",
       "    'pt',\n",
       "    'zh',\n",
       "    'multilingual']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flaubert/flaubert_base_cased': {'modelId': 'flaubert/flaubert_base_cased',\n",
       "  'sha': '86dac38e2ee7dcefac08dfb2c5c901e8c1cf401e',\n",
       "  'lastModified': '2021-05-19T16:54:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'flaubert',\n",
       "   'fill-mask',\n",
       "   'fr',\n",
       "   'dataset:flaubert',\n",
       "   'transformers',\n",
       "   'bert',\n",
       "   'language-model',\n",
       "   'flue',\n",
       "   'french',\n",
       "   'bert-base',\n",
       "   'flaubert-base',\n",
       "   'cased',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'flaubert',\n",
       "  'config': {'architectures': ['FlaubertWithLMHeadModel'],\n",
       "   'model_type': 'flaubert'},\n",
       "  'id': 'flaubert/flaubert_base_cased',\n",
       "  'downloads': 38032,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<special1>',\n",
       "  'widgetData': [{'text': 'Paris est la <special1> de la France.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['flaubert'],\n",
       "   'metrics': ['flue'],\n",
       "   'tags': ['bert',\n",
       "    'language-model',\n",
       "    'flaubert',\n",
       "    'flue',\n",
       "    'french',\n",
       "    'bert-base',\n",
       "    'flaubert-base',\n",
       "    'cased']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pysentimiento/robertuito-emotion-analysis': {'modelId': 'pysentimiento/robertuito-emotion-analysis',\n",
       "  'sha': '2a1fb82f525912c23a8187eeea418751049d5056',\n",
       "  'lastModified': '2021-12-10T13:30:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'es',\n",
       "   'arxiv:2106.09462',\n",
       "   'arxiv:2111.09453',\n",
       "   'transformers',\n",
       "   'emotion-analysis',\n",
       "   'twitter'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'pysentimiento',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'pysentimiento/robertuito-emotion-analysis',\n",
       "  'downloads': 37926,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Te quiero. Te amo.'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['es'], 'tags': ['emotion-analysis', 'twitter']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'DeepPavlov/bert-base-multilingual-cased-sentence': {'modelId': 'DeepPavlov/bert-base-multilingual-cased-sentence',\n",
       "  'sha': '403febddd8959ecc1a8d140a83d461a1261c7935',\n",
       "  'lastModified': '2021-05-18T18:16:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'multilingual',\n",
       "   'arxiv:1704.05426',\n",
       "   'arxiv:1809.05053',\n",
       "   'arxiv:1908.10084',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'DeepPavlov',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'DeepPavlov/bert-base-multilingual-cased-sentence',\n",
       "  'downloads': 37676,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'M-CLIP/M-BERT-Base-ViT-B': {'modelId': 'M-CLIP/M-BERT-Base-ViT-B',\n",
       "  'sha': '5da718394f8f62314bb080b1e989e61f5e3ce026',\n",
       "  'lastModified': '2021-05-18T21:34:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'M-CLIP',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'M-CLIP/M-BERT-Base-ViT-B',\n",
       "  'downloads': 37519,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cardiffnlp/twitter-xlm-roberta-base': {'modelId': 'cardiffnlp/twitter-xlm-roberta-base',\n",
       "  'sha': '152d26b5e474a09a99599ed33a41b9cf9d85556d',\n",
       "  'lastModified': '2021-04-28T16:24:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'multilingual',\n",
       "   'arxiv:2104.12250',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'cardiffnlp',\n",
       "  'config': {'architectures': ['XLMRobertaForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'cardiffnlp/twitter-xlm-roberta-base',\n",
       "  'downloads': 37403,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': '🤗🤗🤗<mask>'},\n",
       "   {'text': '🔥The goal of life is <mask> . 🔥'},\n",
       "   {'text': 'Il segreto della vita è l’<mask> . ❤️'},\n",
       "   {'text': 'Hasta <mask> 👋!'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'widget': [{'text': '🤗🤗🤗<mask>'},\n",
       "    {'text': '🔥The goal of life is <mask> . 🔥'},\n",
       "    {'text': 'Il segreto della vita è l’<mask> . ❤️'},\n",
       "    {'text': 'Hasta <mask> 👋!'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/byt5-small': {'modelId': 'google/byt5-small',\n",
       "  'sha': 'ce8f3a48ed7676af36476a01fb01f95ea529599c',\n",
       "  'lastModified': '2022-05-27T15:06:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'az',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'ca',\n",
       "   'ceb',\n",
       "   'co',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'eo',\n",
       "   'es',\n",
       "   'et',\n",
       "   'eu',\n",
       "   'fa',\n",
       "   'fi',\n",
       "   'fil',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'haw',\n",
       "   'hi',\n",
       "   'hmn',\n",
       "   'ht',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'ig',\n",
       "   'is',\n",
       "   'it',\n",
       "   'iw',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'ku',\n",
       "   'ky',\n",
       "   'la',\n",
       "   'lb',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mi',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'mt',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'ny',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sd',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'sm',\n",
       "   'sn',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'st',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'tg',\n",
       "   'th',\n",
       "   'tr',\n",
       "   'uk',\n",
       "   'und',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'xh',\n",
       "   'yi',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'zu',\n",
       "   'dataset:mc4',\n",
       "   'arxiv:1907.06292',\n",
       "   'arxiv:2105.13626',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/byt5-small',\n",
       "  'downloads': 36714,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'az',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'ca',\n",
       "    'ceb',\n",
       "    'co',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'eo',\n",
       "    'es',\n",
       "    'et',\n",
       "    'eu',\n",
       "    'fa',\n",
       "    'fi',\n",
       "    'fil',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'haw',\n",
       "    'hi',\n",
       "    'hmn',\n",
       "    'ht',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'ig',\n",
       "    'is',\n",
       "    'it',\n",
       "    'iw',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'ku',\n",
       "    'ky',\n",
       "    'la',\n",
       "    'lb',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mi',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'mt',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'ny',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sd',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'sm',\n",
       "    'sn',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'st',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'tg',\n",
       "    'th',\n",
       "    'tr',\n",
       "    'uk',\n",
       "    'und',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'xh',\n",
       "    'yi',\n",
       "    'yo',\n",
       "    'zh',\n",
       "    'zu'],\n",
       "   'datasets': ['mc4'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'YituTech/conv-bert-base': {'modelId': 'YituTech/conv-bert-base',\n",
       "  'sha': '5cb451936b5c4a96562d8b146de85f64f9cf2c22',\n",
       "  'lastModified': '2021-02-24T11:26:14.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'convbert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'YituTech',\n",
       "  'config': {'architectures': ['ConvBertModel'], 'model_type': 'convbert'},\n",
       "  'id': 'YituTech/conv-bert-base',\n",
       "  'downloads': 36228,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'prithivida/parrot_fluency_model': {'modelId': 'prithivida/parrot_fluency_model',\n",
       "  'sha': 'e5224ff5b4109cd949ce25b0a6dff8d8cbdec7be',\n",
       "  'lastModified': '2022-06-24T09:54:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'prithivida',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'prithivida/parrot_fluency_model',\n",
       "  'downloads': 36009,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Rostlab/prot_t5_xl_uniref50': {'modelId': 'Rostlab/prot_t5_xl_uniref50',\n",
       "  'sha': 'd604cdc190f7df5186404c8729934f0ee9a4b0e4',\n",
       "  'lastModified': '2021-03-29T11:47:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'protein',\n",
       "   'dataset:UniRef50',\n",
       "   'transformers',\n",
       "   'protein language model',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'Rostlab',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'Rostlab/prot_t5_xl_uniref50',\n",
       "  'downloads': 35826,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'protein',\n",
       "   'tags': ['protein language model'],\n",
       "   'datasets': ['UniRef50']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dmis-lab/biosyn-sapbert-bc5cdr-disease': {'modelId': 'dmis-lab/biosyn-sapbert-bc5cdr-disease',\n",
       "  'sha': '53d4525fccf15663f19f0d0846c50286a0a01f1e',\n",
       "  'lastModified': '2021-10-25T14:46:40.000Z',\n",
       "  'tags': ['pytorch', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'dmis-lab',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'dmis-lab/biosyn-sapbert-bc5cdr-disease',\n",
       "  'downloads': 35676,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'GroNLP/bert-base-dutch-cased': {'modelId': 'GroNLP/bert-base-dutch-cased',\n",
       "  'sha': '484ff5cec2ad42b434537dadd901d9b8e2b64cd2',\n",
       "  'lastModified': '2021-08-25T15:20:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'nl',\n",
       "   'arxiv:1912.09582',\n",
       "   'transformers',\n",
       "   'BERTje',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'GroNLP',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'GroNLP/bert-base-dutch-cased',\n",
       "  'downloads': 34801,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'nl',\n",
       "   'thumbnail': 'https://raw.githubusercontent.com/wietsedv/bertje/master/bertje.png',\n",
       "   'tags': ['BERTje']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/deberta-xlarge-mnli': {'modelId': 'microsoft/deberta-xlarge-mnli',\n",
       "  'sha': '5b07a9086c1dbb79981ff7b05b4d1ad83b3af51c',\n",
       "  'lastModified': '2022-06-27T15:47:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'deberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'deberta-v1',\n",
       "   'deberta-mnli',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['DebertaForSequenceClassification'],\n",
       "   'model_type': 'deberta'},\n",
       "  'id': 'microsoft/deberta-xlarge-mnli',\n",
       "  'downloads': 34578,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '[CLS] I love you. [SEP] I like you. [SEP]'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['deberta-v1', 'deberta-mnli'],\n",
       "   'tasks': 'mnli',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': '[CLS] I love you. [SEP] I like you. [SEP]'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'knkarthick/MEETING_SUMMARY': {'modelId': 'knkarthick/MEETING_SUMMARY',\n",
       "  'sha': '1d9f2261609ed4970abf4f3659c080783beaf09e',\n",
       "  'lastModified': '2022-06-29T07:16:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:cnndaily/newyorkdaily/xsum/samsum/dialogsum/AMI',\n",
       "   'transformers',\n",
       "   'seq2seq',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'knkarthick',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {}},\n",
       "  'id': 'knkarthick/MEETING_SUMMARY',\n",
       "  'downloads': 34335,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': \"Hi, I'm David and I'm supposed to be an industrial designer. Um, I just got the project announcement about what the project is. Designing a remote control. That's about it, didn't get anything else. Did you get the same thing? Cool. There's too much gear. Okay. Can't draw. Um. Yeah. Um, well anyway, I don't know, it's just the first animal I can think off the top of my head. Um. Yes. Big reason is 'cause I'm allergic to most animals. Allergic to animal fur, so um fish was a natural choice. Um, yeah, and I kind of like whales. They come in and go eat everything in sight. And they're quite harmless and mild and interesting. Tail's a bit big, I think. It's an after dinner dog then. Hmm. It does make sense from maybe the design point of view 'cause you have more complicated characters like European languages, then you need more buttons. So, possibly. Hmm. Yeah. And you keep losing them. Finding them is really a pain, you know. I mean it's usually quite small, or when you want it right, it slipped behind the couch or it's kicked under the table. You know. Yep. Mm-hmm. I think one factor would be production cost. Because there's a cap there, so um depends on how much you can cram into that price. Um. I think that that's the main factor. Cool.\\nOkay. Right. Um well this is the kick-off meeting for our our project. Um and um this is just what we're gonna be doing over the next twenty five minutes. Um so first of all, just to kind of make sure that we all know each other, I'm Laura and I'm the project manager. Do you want to introduce yourself again? Okay. Great. Okay. Um so we're designing a new remote control and um Oh I have to record who's here actually. So that's David, Andrew and Craig, isn't it? And you all arrived on time. Um yeah so des uh design a new remote control. Um, as you can see it's supposed to be original, trendy and user friendly. Um so that's kind of our our brief, as it were. Um and so there are three different stages to the design. Um I'm not really sure what what you guys have already received um in your emails. What did you get? Mm-hmm. Is that what everybody got? Okay. Um. So we're gonna have like individual work and then a meeting about it. And repeat that process three times. Um and at this point we get try out the whiteboard over there. Um. So uh you get to draw your favourite animal and sum up your favourite characteristics of it. So who would like to go first? Very good. Mm-hmm. Yeah. Yeah. Right. Lovely. Right. You can take as long over this as you like, because we haven't got an awful lot to discuss. Ok oh we do we do. Don't feel like you're in a rush, anyway. Ach why not We might have to get you up again then. I don't know what mine is. I'm gonna have to think on the spot now. Is that a whale? Ah. Okay. God, I still don't know what I'm gonna write about. Um. I was gonna choose a dog as well. But I'll just draw a different kind of dog. M my favourite animal is my own dog at home. Um That doesn't really look like him, actually. He looks more like a pig, actually. Ah well. Do you? Oh that's very good of you. Uh. Um he's a mixture of uh various things. Um and what do I like about him, um That's just to suggest that his tail wags. Um he's very friendly and cheery and always pleased to see you, and very kind of affectionate and um uh and he's quite quite wee as well so you know he can doesn't take up too much space. Um and uh And he does a funny thing where he chases his tail as well, which is quite amusing, so It is. I think it is. He only does it after he's had his dinner and um he'll just all of a sudden just get up and start chasing his tail 'round the living room. Yeah, so uh Yeah, maybe. Maybe. Right, um where did you find this? Just down here? Yeah. Okay. Um what are we doing next? Uh um. Okay, uh we now need to discuss the project finance. Um so according to the brief um we're gonna be selling this remote control for twenty five Euro, um and we're aiming to make fifty million Euro. Um so we're gonna be selling this on an international scale. And uh we don't want it to cost any more than uh twelve fifty Euros, so fifty percent of the selling price. Sure. All together. Um I dunno. I imagine That's a good question. I imagine it probably is our sale actually because it's probably up to the the um the retailer to uh sell it for whatever price they want. Um. But I I don't know, I mean do you think the fact that it's going to be sold internationally will have a bearing on how we design it at all? Think it will? Um. Hmm. Oh yeah, regions and stuff, yeah. Yeah. Okay. Yeah. Well for a remote control, do you think that will be I suppose it's depends on how complicated our remote control is. Yeah, yeah. Okay. What, just like in terms of like the wealth of the country? Like how much money people have to spend on things like? Aye, I see what you mean, yeah. Marketing. Good marketing thoughts. Oh gosh, I should be writing all this down. Um. Mm. Yeah. Yeah, yeah. Like how much does, you know, a remote control cost. Well twenty five Euro, I mean that's um that's about like eighteen pounds or something, isn't it? Or no, is it as much as that? Sixteen seventeen eighteen pounds. Um, I dunno, I've never bought a remote control, so I don't know how how good a remote control that would get you. Um. But yeah, I suppose it has to look kind of cool and gimmicky. Um right, okay. Let me just scoot on ahead here. Okay. Um well d Does anybody have anything to add to uh to the finance issue at all? Thin No, actually. That would be useful, though, wouldn't it, if you knew like what your money would get you now. Mm-hmm. Yeah, yeah. Oh. Five minutes to end of meeting. Oh, okay. We're a bit behind. Yeah. Right, so do you think that should be like a main design aim of our remote control d you know, do your your satellite and your regular telly and your V_C_R_ and everything? Mm-hmm. Yeah. Or even like, you know, notes about um what you wanna watch. Like you might put in there oh I want to watch such and such and look a Oh that's a good idea. So extra functionalities. Mm-hmm. Hmm. Um okay, uh I'd wel we're gonna have to wrap up pretty quickly in the next couple of minutes. Um I'll just check we've nothing else. Okay. Um so anything else anybody wants to add about what they don't like about remote controls they've used, what they would really like to be part of this new one at all? You keep losing them. Okay. Yeah. W You get those ones where you can, if you like, whistle or make a really high pitched noise they beep. There I mean is that something we'd want to include, do you think? Dunno. Okay maybe. My goodness. Still feels quite primitive. Maybe like a touch screen or something? Okay. Uh-huh, okay. Well I guess that's up to our industrial designer. It looks better. Yeah. Okay. Okay. Right, well um so just to wrap up, the next meeting's gonna be in thirty minutes. So that's about um about ten to twelve by my watch. Um so inbetween now and then, um as the industrial designer, you're gonna be working on you know the actual working design of it so y you know what you're doing there. Um for user interface, technical functions, I guess that's you know like what we've been talking about, what it'll actually do. Um and uh marketing executive, you'll be just thinking about what it actually what, you know, what requirements it has to has to fulfil and you'll all get instructions emailed to you, I guess. Um. Yeah, so it's th the functional design stage is next, I guess. And uh and that's the end of the meeting. So I got that little message a lot sooner than I thought I would, so Mm-hmm. Uh-huh, yeah. Th Okay, well just very quickly 'cause this we're supposed to finish now. Um I guess that's up to us, I mean you probably want some kind of unique selling point of it, so um, you know Yeah. Mm-hmm. Yeah. Okay. Right, okay, we'll that's that's the end of the meeting, then. Um. So, uh thank you all for coming.\\nUm I'm Craig and I'm User Interface. Yeah. Well, my favourite animal would be a monkey. Then they're small cute and furry, and uh when planet of the apes becomes real, I'm gonna be up there with them. Yeah. I know um My parents went out and bought um remote controls because um they got fed up of having four or five different remote controls for each things the house. So um for them it was just how many devices control. Uh.\\nMm-hmm. Great. And I'm Andrew and I'm uh our marketing expert. Mm-hmm. Mm-hmm. Yeah, that's that's it. Yeah. I will go. That's fine. Alright. So This one here, right? Okay. Very nice. Alright. My favourite animal is like A beagle. Um charac favourite characteristics of it? Is that right? Uh, right, well basically um high priority for any animal for me is that they be willing to take a lot of physical affection from their family. And, yeah that they have lots of personality and uh be fit and in robust good health. So this is blue. Blue beagle. My family's beagle. I coulda told you a whole lot more about beagles. Boy, let me tell you. Impressionist. Alright. Mm. Superb sketch, by the way. Yep. I see a dog in there. Yep. Now I see a rooster. What kind is it? Is he aware that th it's his own cha tail he's chasing? Hmm. Probably when he was little he got lots of attention for doing it and has forever been conditioned. 'Kay. Um, can we just go over that again? Uh, so bas at twel Alright, yeah. Okay. So cost like production cost is twelve fifty, but selling price is is that wholesale or retail? Like on the shelf. Our sale our sale anyway. Yeah, okay okay. Okay. Mm-hmm. Alright. Yes. Mm-hmm. Mm-hmm. Well right away I'm wondering if there's um th th uh, like with D_V_D_ players, if there are zones. Um f frequencies or something um as well as uh characters, um different uh keypad styles and s symbols. Um. I don't know. Yeah. Yeah. Yeah. And then a and then al the other thing international is on top of the price. I'm thinking the price might might appeal to a certain market in one region, whereas in another it'll be different, so Just a chara just a characteristic of the Just Or just like, basic product podi positioning, the twenty five Euro remote control might be a big hit in London, might not be such a big hit in Greece, who knows, something like that, yeah. Yep. Right away I'm making some kind of assumptions about what what information we're given here, thinking, 'kay trendy probably means something other than just basic, something other than just standard. Um so I'm wondering right away, is selling twenty five Euros, is that sort of the thi is this gonna to be like the premium product kinda thing or Uh-huh. Mm-hmm. Yep. Yeah, I'd say so, yeah. No. Yeah, yeah. Mm-hmm. Do we have any other background information on like how that compares to other other Yeah. Mm-hmm. Yeah, interesting thing about discussing um production of a remote control for me is that l as you point out, I just don't think of remote controls as somethin something people consciously assess in their purchasing habits. It's just like getting shoelaces with shoes or something. It just comes along. Do you know what I mean? Like so sort of like how do you I I mean one one way of looking at it would be, well the people producing television sets, maybe they have to buy remote controls. Or another way is maybe people who have T_V_ sets are really fed up with their remote control and they really want a better one or something. But Right. Right. Okay so Right, so in function one of the priorities might be to combine as many uses I think so. Yeah, yeah. Yeah. Well like um, maybe what we could use is a sort of like a example of a successful other piece technology is palm palm pilots. They're gone from being just like little sort of scribble boards to cameras, M_P_ three players, telephones, everything, agenda. So, like, I wonder if we might add something new to the to the remote control market, such as the lighting in your house, or um Yeah, yeah. An Yeah. Like, p personally for me, at home I've I've combined the um the audio video of my television set and my D_V_D_ player and my C_D_ player. So they w all work actually function together but I have different remote controls for each of them. So it's sort of ironic that that then they're in there um you know, the sound and everything it's just one system. But each one's got its own little part. Mm. Mm. Mm. Mm-hmm. Mm-hmm. Yeah. Yeah. That's just really good id Yep. Uh, sure. I remember when the first remote control my my family had was on a cable. Actually had a cable between it and the T_V_ and big like buttons that sort of like, like on a blender or something. And um, you know, when I think about what they are now, it's better, but actually it's still kind of, I dunno, like a massive junky thing on the table. Maybe we could think about how, could be more, you know, streamlined. S Something like that, yeah. Or whatever would be technologically reasonable. 'Cause it could b it could it could be that f it could be that functionally that doesn't make it any better, but that just the appeal of of not having You know, these days there's a r pe things in people's homes are becoming more and more like chic, you know. Um, nicer materials and might be be worth exploring anyway. Okay. Um. Before we wrap up, just to make sure we're all on the same page here, um, do we We were given sort of an example of a coffee machine or something, right? Well, um are we at ma right now on the assumption that our television remote control may have features which go beyond the television? Or are we keeping sort of like a a design commitment to television features? I I don't know. Yep. Yeah, sure. Okay. Okay, yeah. Okay. Okay. Okay. Alright.\"}],\n",
       "  'likes': 13,\n",
       "  'model-index': [{'name': 'MEETING_SUMMARY',\n",
       "    'results': [{'task': {'name': 'Abstractive Text Summarization',\n",
       "       'type': 'abstractive-text-summarization'},\n",
       "      'dataset': {'name': 'samsum', 'type': 'samsum'},\n",
       "      'metrics': [{'name': 'Validation ROGUE-1',\n",
       "        'type': 'rouge-1',\n",
       "        'value': 53.8795},\n",
       "       {'name': 'Validation ROGUE-2', 'type': 'rouge-2', 'value': 28.4975},\n",
       "       {'name': 'Validation ROGUE-L', 'type': 'rouge-L', 'value': 44.1899},\n",
       "       {'name': 'Validation ROGUE-Lsum',\n",
       "        'type': 'rouge-Lsum',\n",
       "        'value': 49.4863},\n",
       "       {'name': 'Validation ROGUE-Lsum',\n",
       "        'type': 'gen-length',\n",
       "        'value': 30.088},\n",
       "       {'name': 'Test ROGUE-1', 'type': 'rouge-1', 'value': 53.2284},\n",
       "       {'name': 'Test ROGUE-2', 'type': 'rouge-2', 'value': 28.184},\n",
       "       {'name': 'Test ROGUE-L', 'type': 'rouge-L', 'value': 44.122},\n",
       "       {'name': 'Test ROGUE-Lsum', 'type': 'rouge-Lsum', 'value': 49.0301},\n",
       "       {'name': 'Test ROGUE-Lsum', 'type': 'gen-length', 'value': 29.9951}]}]},\n",
       "   {'name': 'MEETING_SUMMARY',\n",
       "    'results': [{'task': {'name': 'Abstractive Text Summarization',\n",
       "       'type': 'abstractive-text-summarization'},\n",
       "      'dataset': {'name': 'xsum', 'type': 'xsum'},\n",
       "      'metrics': [{'name': 'Validation ROGUE-1',\n",
       "        'type': 'rouge-1',\n",
       "        'value': 35.9078},\n",
       "       {'name': 'Validation ROGUE-2', 'type': 'rouge-2', 'value': 14.2497},\n",
       "       {'name': 'Validation ROGUE-L', 'type': 'rouge-L', 'value': 28.1421},\n",
       "       {'name': 'Validation ROGUE-Lsum',\n",
       "        'type': 'rouge-Lsum',\n",
       "        'value': 28.9826},\n",
       "       {'name': 'Validation ROGUE-Lsum',\n",
       "        'type': 'gen-length',\n",
       "        'value': 32.0167},\n",
       "       {'name': 'Test ROGUE-1', 'type': 'rouge-1', 'value': 36.0241},\n",
       "       {'name': 'Test ROGUE-2', 'type': 'rouge-2', 'value': 14.3715},\n",
       "       {'name': 'Test ROGUE-L', 'type': 'rouge-L', 'value': 28.1968},\n",
       "       {'name': 'Test ROGUE-Lsum', 'type': 'rouge-Lsum', 'value': 29.0527},\n",
       "       {'name': 'Test ROGUE-Lsum', 'type': 'gen-length', 'value': 31.9933}]}]},\n",
       "   {'name': 'MEETING_SUMMARY',\n",
       "    'results': [{'task': {'name': 'Abstractive Text Summarization',\n",
       "       'type': 'abstractive-text-summarization'},\n",
       "      'dataset': {'name': 'dialogsum', 'type': 'dialogsum'},\n",
       "      'metrics': [{'name': 'Validation ROGUE-1',\n",
       "        'type': 'rouge-1',\n",
       "        'value': 39.8612},\n",
       "       {'name': 'Validation ROGUE-2', 'type': 'rouge-2', 'value': 16.6917},\n",
       "       {'name': 'Validation ROGUE-L', 'type': 'rouge-L', 'value': 32.2718},\n",
       "       {'name': 'Validation ROGUE-Lsum',\n",
       "        'type': 'rouge-Lsum',\n",
       "        'value': 35.8748},\n",
       "       {'name': 'Validation ROGUE-Lsum',\n",
       "        'type': 'gen-length',\n",
       "        'value': 41.726},\n",
       "       {'name': 'Test ROGUE-1', 'type': 'rouge-1', 'value': 36.9608},\n",
       "       {'name': 'Test ROGUE-2', 'type': 'rouge-2', 'value': 14.3058},\n",
       "       {'name': 'Test ROGUE-L', 'type': 'rouge-L', 'value': 29.3261},\n",
       "       {'name': 'Test ROGUE-Lsum', 'type': 'rouge-Lsum', 'value': 32.9},\n",
       "       {'name': 'Test ROGUE-Lsum', 'type': 'gen-length', 'value': 43.086}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['bart', 'seq2seq', 'summarization'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['cnndaily/newyorkdaily/xsum/samsum/dialogsum/AMI'],\n",
       "   'metrics': ['rouge'],\n",
       "   'widget': [{'text': \"Hi, I'm David and I'm supposed to be an industrial designer. Um, I just got the project announcement about what the project is. Designing a remote control. That's about it, didn't get anything else. Did you get the same thing? Cool. There's too much gear. Okay. Can't draw. Um. Yeah. Um, well anyway, I don't know, it's just the first animal I can think off the top of my head. Um. Yes. Big reason is 'cause I'm allergic to most animals. Allergic to animal fur, so um fish was a natural choice. Um, yeah, and I kind of like whales. They come in and go eat everything in sight. And they're quite harmless and mild and interesting. Tail's a bit big, I think. It's an after dinner dog then. Hmm. It does make sense from maybe the design point of view 'cause you have more complicated characters like European languages, then you need more buttons. So, possibly. Hmm. Yeah. And you keep losing them. Finding them is really a pain, you know. I mean it's usually quite small, or when you want it right, it slipped behind the couch or it's kicked under the table. You know. Yep. Mm-hmm. I think one factor would be production cost. Because there's a cap there, so um depends on how much you can cram into that price. Um. I think that that's the main factor. Cool.\\nOkay. Right. Um well this is the kick-off meeting for our our project. Um and um this is just what we're gonna be doing over the next twenty five minutes. Um so first of all, just to kind of make sure that we all know each other, I'm Laura and I'm the project manager. Do you want to introduce yourself again? Okay. Great. Okay. Um so we're designing a new remote control and um Oh I have to record who's here actually. So that's David, Andrew and Craig, isn't it? And you all arrived on time. Um yeah so des uh design a new remote control. Um, as you can see it's supposed to be original, trendy and user friendly. Um so that's kind of our our brief, as it were. Um and so there are three different stages to the design. Um I'm not really sure what what you guys have already received um in your emails. What did you get? Mm-hmm. Is that what everybody got? Okay. Um. So we're gonna have like individual work and then a meeting about it. And repeat that process three times. Um and at this point we get try out the whiteboard over there. Um. So uh you get to draw your favourite animal and sum up your favourite characteristics of it. So who would like to go first? Very good. Mm-hmm. Yeah. Yeah. Right. Lovely. Right. You can take as long over this as you like, because we haven't got an awful lot to discuss. Ok oh we do we do. Don't feel like you're in a rush, anyway. Ach why not We might have to get you up again then. I don't know what mine is. I'm gonna have to think on the spot now. Is that a whale? Ah. Okay. God, I still don't know what I'm gonna write about. Um. I was gonna choose a dog as well. But I'll just draw a different kind of dog. M my favourite animal is my own dog at home. Um That doesn't really look like him, actually. He looks more like a pig, actually. Ah well. Do you? Oh that's very good of you. Uh. Um he's a mixture of uh various things. Um and what do I like about him, um That's just to suggest that his tail wags. Um he's very friendly and cheery and always pleased to see you, and very kind of affectionate and um uh and he's quite quite wee as well so you know he can doesn't take up too much space. Um and uh And he does a funny thing where he chases his tail as well, which is quite amusing, so It is. I think it is. He only does it after he's had his dinner and um he'll just all of a sudden just get up and start chasing his tail 'round the living room. Yeah, so uh Yeah, maybe. Maybe. Right, um where did you find this? Just down here? Yeah. Okay. Um what are we doing next? Uh um. Okay, uh we now need to discuss the project finance. Um so according to the brief um we're gonna be selling this remote control for twenty five Euro, um and we're aiming to make fifty million Euro. Um so we're gonna be selling this on an international scale. And uh we don't want it to cost any more than uh twelve fifty Euros, so fifty percent of the selling price. Sure. All together. Um I dunno. I imagine That's a good question. I imagine it probably is our sale actually because it's probably up to the the um the retailer to uh sell it for whatever price they want. Um. But I I don't know, I mean do you think the fact that it's going to be sold internationally will have a bearing on how we design it at all? Think it will? Um. Hmm. Oh yeah, regions and stuff, yeah. Yeah. Okay. Yeah. Well for a remote control, do you think that will be I suppose it's depends on how complicated our remote control is. Yeah, yeah. Okay. What, just like in terms of like the wealth of the country? Like how much money people have to spend on things like? Aye, I see what you mean, yeah. Marketing. Good marketing thoughts. Oh gosh, I should be writing all this down. Um. Mm. Yeah. Yeah, yeah. Like how much does, you know, a remote control cost. Well twenty five Euro, I mean that's um that's about like eighteen pounds or something, isn't it? Or no, is it as much as that? Sixteen seventeen eighteen pounds. Um, I dunno, I've never bought a remote control, so I don't know how how good a remote control that would get you. Um. But yeah, I suppose it has to look kind of cool and gimmicky. Um right, okay. Let me just scoot on ahead here. Okay. Um well d Does anybody have anything to add to uh to the finance issue at all? Thin No, actually. That would be useful, though, wouldn't it, if you knew like what your money would get you now. Mm-hmm. Yeah, yeah. Oh. Five minutes to end of meeting. Oh, okay. We're a bit behind. Yeah. Right, so do you think that should be like a main design aim of our remote control d you know, do your your satellite and your regular telly and your V_C_R_ and everything? Mm-hmm. Yeah. Or even like, you know, notes about um what you wanna watch. Like you might put in there oh I want to watch such and such and look a Oh that's a good idea. So extra functionalities. Mm-hmm. Hmm. Um okay, uh I'd wel we're gonna have to wrap up pretty quickly in the next couple of minutes. Um I'll just check we've nothing else. Okay. Um so anything else anybody wants to add about what they don't like about remote controls they've used, what they would really like to be part of this new one at all? You keep losing them. Okay. Yeah. W You get those ones where you can, if you like, whistle or make a really high pitched noise they beep. There I mean is that something we'd want to include, do you think? Dunno. Okay maybe. My goodness. Still feels quite primitive. Maybe like a touch screen or something? Okay. Uh-huh, okay. Well I guess that's up to our industrial designer. It looks better. Yeah. Okay. Okay. Right, well um so just to wrap up, the next meeting's gonna be in thirty minutes. So that's about um about ten to twelve by my watch. Um so inbetween now and then, um as the industrial designer, you're gonna be working on you know the actual working design of it so y you know what you're doing there. Um for user interface, technical functions, I guess that's you know like what we've been talking about, what it'll actually do. Um and uh marketing executive, you'll be just thinking about what it actually what, you know, what requirements it has to has to fulfil and you'll all get instructions emailed to you, I guess. Um. Yeah, so it's th the functional design stage is next, I guess. And uh and that's the end of the meeting. So I got that little message a lot sooner than I thought I would, so Mm-hmm. Uh-huh, yeah. Th Okay, well just very quickly 'cause this we're supposed to finish now. Um I guess that's up to us, I mean you probably want some kind of unique selling point of it, so um, you know Yeah. Mm-hmm. Yeah. Okay. Right, okay, we'll that's that's the end of the meeting, then. Um. So, uh thank you all for coming.\\nUm I'm Craig and I'm User Interface. Yeah. Well, my favourite animal would be a monkey. Then they're small cute and furry, and uh when planet of the apes becomes real, I'm gonna be up there with them. Yeah. I know um My parents went out and bought um remote controls because um they got fed up of having four or five different remote controls for each things the house. So um for them it was just how many devices control. Uh.\\nMm-hmm. Great. And I'm Andrew and I'm uh our marketing expert. Mm-hmm. Mm-hmm. Yeah, that's that's it. Yeah. I will go. That's fine. Alright. So This one here, right? Okay. Very nice. Alright. My favourite animal is like A beagle. Um charac favourite characteristics of it? Is that right? Uh, right, well basically um high priority for any animal for me is that they be willing to take a lot of physical affection from their family. And, yeah that they have lots of personality and uh be fit and in robust good health. So this is blue. Blue beagle. My family's beagle. I coulda told you a whole lot more about beagles. Boy, let me tell you. Impressionist. Alright. Mm. Superb sketch, by the way. Yep. I see a dog in there. Yep. Now I see a rooster. What kind is it? Is he aware that th it's his own cha tail he's chasing? Hmm. Probably when he was little he got lots of attention for doing it and has forever been conditioned. 'Kay. Um, can we just go over that again? Uh, so bas at twel Alright, yeah. Okay. So cost like production cost is twelve fifty, but selling price is is that wholesale or retail? Like on the shelf. Our sale our sale anyway. Yeah, okay okay. Okay. Mm-hmm. Alright. Yes. Mm-hmm. Mm-hmm. Well right away I'm wondering if there's um th th uh, like with D_V_D_ players, if there are zones. Um f frequencies or something um as well as uh characters, um different uh keypad styles and s symbols. Um. I don't know. Yeah. Yeah. Yeah. And then a and then al the other thing international is on top of the price. I'm thinking the price might might appeal to a certain market in one region, whereas in another it'll be different, so Just a chara just a characteristic of the Just Or just like, basic product podi positioning, the twenty five Euro remote control might be a big hit in London, might not be such a big hit in Greece, who knows, something like that, yeah. Yep. Right away I'm making some kind of assumptions about what what information we're given here, thinking, 'kay trendy probably means something other than just basic, something other than just standard. Um so I'm wondering right away, is selling twenty five Euros, is that sort of the thi is this gonna to be like the premium product kinda thing or Uh-huh. Mm-hmm. Yep. Yeah, I'd say so, yeah. No. Yeah, yeah. Mm-hmm. Do we have any other background information on like how that compares to other other Yeah. Mm-hmm. Yeah, interesting thing about discussing um production of a remote control for me is that l as you point out, I just don't think of remote controls as somethin something people consciously assess in their purchasing habits. It's just like getting shoelaces with shoes or something. It just comes along. Do you know what I mean? Like so sort of like how do you I I mean one one way of looking at it would be, well the people producing television sets, maybe they have to buy remote controls. Or another way is maybe people who have T_V_ sets are really fed up with their remote control and they really want a better one or something. But Right. Right. Okay so Right, so in function one of the priorities might be to combine as many uses I think so. Yeah, yeah. Yeah. Well like um, maybe what we could use is a sort of like a example of a successful other piece technology is palm palm pilots. They're gone from being just like little sort of scribble boards to cameras, M_P_ three players, telephones, everything, agenda. So, like, I wonder if we might add something new to the to the remote control market, such as the lighting in your house, or um Yeah, yeah. An Yeah. Like, p personally for me, at home I've I've combined the um the audio video of my television set and my D_V_D_ player and my C_D_ player. So they w all work actually function together but I have different remote controls for each of them. So it's sort of ironic that that then they're in there um you know, the sound and everything it's just one system. But each one's got its own little part. Mm. Mm. Mm. Mm-hmm. Mm-hmm. Yeah. Yeah. That's just really good id Yep. Uh, sure. I remember when the first remote control my my family had was on a cable. Actually had a cable between it and the T_V_ and big like buttons that sort of like, like on a blender or something. And um, you know, when I think about what they are now, it's better, but actually it's still kind of, I dunno, like a massive junky thing on the table. Maybe we could think about how, could be more, you know, streamlined. S Something like that, yeah. Or whatever would be technologically reasonable. 'Cause it could b it could it could be that f it could be that functionally that doesn't make it any better, but that just the appeal of of not having You know, these days there's a r pe things in people's homes are becoming more and more like chic, you know. Um, nicer materials and might be be worth exploring anyway. Okay. Um. Before we wrap up, just to make sure we're all on the same page here, um, do we We were given sort of an example of a coffee machine or something, right? Well, um are we at ma right now on the assumption that our television remote control may have features which go beyond the television? Or are we keeping sort of like a a design commitment to television features? I I don't know. Yep. Yeah, sure. Okay. Okay, yeah. Okay. Okay. Okay. Alright.\"}],\n",
       "   'model-index': [{'name': 'MEETING_SUMMARY',\n",
       "     'results': [{'task': {'name': 'Abstractive Text Summarization',\n",
       "        'type': 'abstractive-text-summarization'},\n",
       "       'dataset': {'name': 'samsum', 'type': 'samsum'},\n",
       "       'metrics': [{'name': 'Validation ROGUE-1',\n",
       "         'type': 'rouge-1',\n",
       "         'value': 53.8795},\n",
       "        {'name': 'Validation ROGUE-2', 'type': 'rouge-2', 'value': 28.4975},\n",
       "        {'name': 'Validation ROGUE-L', 'type': 'rouge-L', 'value': 44.1899},\n",
       "        {'name': 'Validation ROGUE-Lsum',\n",
       "         'type': 'rouge-Lsum',\n",
       "         'value': 49.4863},\n",
       "        {'name': 'Validation ROGUE-Lsum',\n",
       "         'type': 'gen-length',\n",
       "         'value': 30.088},\n",
       "        {'name': 'Test ROGUE-1', 'type': 'rouge-1', 'value': 53.2284},\n",
       "        {'name': 'Test ROGUE-2', 'type': 'rouge-2', 'value': 28.184},\n",
       "        {'name': 'Test ROGUE-L', 'type': 'rouge-L', 'value': 44.122},\n",
       "        {'name': 'Test ROGUE-Lsum', 'type': 'rouge-Lsum', 'value': 49.0301},\n",
       "        {'name': 'Test ROGUE-Lsum',\n",
       "         'type': 'gen-length',\n",
       "         'value': 29.9951}]}]},\n",
       "    {'name': 'MEETING_SUMMARY',\n",
       "     'results': [{'task': {'name': 'Abstractive Text Summarization',\n",
       "        'type': 'abstractive-text-summarization'},\n",
       "       'dataset': {'name': 'xsum', 'type': 'xsum'},\n",
       "       'metrics': [{'name': 'Validation ROGUE-1',\n",
       "         'type': 'rouge-1',\n",
       "         'value': 35.9078},\n",
       "        {'name': 'Validation ROGUE-2', 'type': 'rouge-2', 'value': 14.2497},\n",
       "        {'name': 'Validation ROGUE-L', 'type': 'rouge-L', 'value': 28.1421},\n",
       "        {'name': 'Validation ROGUE-Lsum',\n",
       "         'type': 'rouge-Lsum',\n",
       "         'value': 28.9826},\n",
       "        {'name': 'Validation ROGUE-Lsum',\n",
       "         'type': 'gen-length',\n",
       "         'value': 32.0167},\n",
       "        {'name': 'Test ROGUE-1', 'type': 'rouge-1', 'value': 36.0241},\n",
       "        {'name': 'Test ROGUE-2', 'type': 'rouge-2', 'value': 14.3715},\n",
       "        {'name': 'Test ROGUE-L', 'type': 'rouge-L', 'value': 28.1968},\n",
       "        {'name': 'Test ROGUE-Lsum', 'type': 'rouge-Lsum', 'value': 29.0527},\n",
       "        {'name': 'Test ROGUE-Lsum',\n",
       "         'type': 'gen-length',\n",
       "         'value': 31.9933}]}]},\n",
       "    {'name': 'MEETING_SUMMARY',\n",
       "     'results': [{'task': {'name': 'Abstractive Text Summarization',\n",
       "        'type': 'abstractive-text-summarization'},\n",
       "       'dataset': {'name': 'dialogsum', 'type': 'dialogsum'},\n",
       "       'metrics': [{'name': 'Validation ROGUE-1',\n",
       "         'type': 'rouge-1',\n",
       "         'value': 39.8612},\n",
       "        {'name': 'Validation ROGUE-2', 'type': 'rouge-2', 'value': 16.6917},\n",
       "        {'name': 'Validation ROGUE-L', 'type': 'rouge-L', 'value': 32.2718},\n",
       "        {'name': 'Validation ROGUE-Lsum',\n",
       "         'type': 'rouge-Lsum',\n",
       "         'value': 35.8748},\n",
       "        {'name': 'Validation ROGUE-Lsum',\n",
       "         'type': 'gen-length',\n",
       "         'value': 41.726},\n",
       "        {'name': 'Test ROGUE-1', 'type': 'rouge-1', 'value': 36.9608},\n",
       "        {'name': 'Test ROGUE-2', 'type': 'rouge-2', 'value': 14.3058},\n",
       "        {'name': 'Test ROGUE-L', 'type': 'rouge-L', 'value': 29.3261},\n",
       "        {'name': 'Test ROGUE-Lsum', 'type': 'rouge-Lsum', 'value': 32.9},\n",
       "        {'name': 'Test ROGUE-Lsum',\n",
       "         'type': 'gen-length',\n",
       "         'value': 43.086}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dmis-lab/biosyn-sapbert-bc5cdr-chemical': {'modelId': 'dmis-lab/biosyn-sapbert-bc5cdr-chemical',\n",
       "  'sha': 'f9b9daf740698ac427bb6532fd456fc18bccdd80',\n",
       "  'lastModified': '2021-10-25T14:47:09.000Z',\n",
       "  'tags': ['pytorch', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'dmis-lab',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'dmis-lab/biosyn-sapbert-bc5cdr-chemical',\n",
       "  'downloads': 34138,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'prajjwal1/bert-mini': {'modelId': 'prajjwal1/bert-mini',\n",
       "  'sha': '5e123abc2480f0c4b4cac186d3b3f09299c258fc',\n",
       "  'lastModified': '2021-10-27T18:27:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'arxiv:1908.08962',\n",
       "   'arxiv:2110.01518',\n",
       "   'transformers',\n",
       "   'BERT',\n",
       "   'MNLI',\n",
       "   'NLI',\n",
       "   'transformer',\n",
       "   'pre-training',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'prajjwal1',\n",
       "  'config': {},\n",
       "  'id': 'prajjwal1/bert-mini',\n",
       "  'downloads': 33926,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'license': ['mit'],\n",
       "   'tags': ['BERT', 'MNLI', 'NLI', 'transformer', 'pre-training']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'peterchou/simbert-chinese-base': {'modelId': 'peterchou/simbert-chinese-base',\n",
       "  'sha': '6a6ebb9f9d9b2264a8a012f96de01067f304476d',\n",
       "  'lastModified': '2021-06-07T05:21:51.000Z',\n",
       "  'tags': ['pytorch', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'peterchou',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'peterchou/simbert-chinese-base',\n",
       "  'downloads': 33339,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'BeIR/query-gen-msmarco-t5-large-v1': {'modelId': 'BeIR/query-gen-msmarco-t5-large-v1',\n",
       "  'sha': '5dd8dd401d24332c17e40015e9792ee31f3ced91',\n",
       "  'lastModified': '2021-06-23T02:12:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'BeIR',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'BeIR/query-gen-msmarco-t5-large-v1',\n",
       "  'downloads': 33017,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-bart': {'modelId': 'hf-internal-testing/tiny-random-bart',\n",
       "  'sha': 'f2efe525625e508121ef8e13b7c37e6324073378',\n",
       "  'lastModified': '2021-11-18T11:36:51.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'bart', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'model_type': 'bart'},\n",
       "  'id': 'hf-internal-testing/tiny-random-bart',\n",
       "  'downloads': 32978,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'amberoad/bert-multilingual-passage-reranking-msmarco': {'modelId': 'amberoad/bert-multilingual-passage-reranking-msmarco',\n",
       "  'sha': 'ed2597214a09ac6a3095b64c1ec49309daab5d9c',\n",
       "  'lastModified': '2021-09-21T16:00:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'multilingual',\n",
       "   'dataset:msmarco',\n",
       "   'arxiv:1901.04085',\n",
       "   'transformers',\n",
       "   'msmarco',\n",
       "   'passage reranking',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'amberoad',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'amberoad/bert-multilingual-passage-reranking-msmarco',\n",
       "  'downloads': 32935,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'query': 'What is a corporation?',\n",
       "    'passage': 'A company is incorporated in a specific nation, often within the bounds of a smaller subset of that nation, such as a state or province. The corporation is then governed by the laws of incorporation in that state. A corporation may issue stock, either private or public, or may be classified as a non-stock corporation. If stock is issued, the corporation will usually be governed by its shareholders, either directly or indirectly.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'thumbnail': 'https://amberoad.de/images/logo_text.png',\n",
       "   'tags': ['msmarco', 'multilingual', 'passage reranking'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['msmarco'],\n",
       "   'metrics': ['MRR'],\n",
       "   'widget': [{'query': 'What is a corporation?',\n",
       "     'passage': 'A company is incorporated in a specific nation, often within the bounds of a smaller subset of that nation, such as a state or province. The corporation is then governed by the laws of incorporation in that state. A corporation may issue stock, either private or public, or may be classified as a non-stock corporation. If stock is issued, the corporation will usually be governed by its shareholders, either directly or indirectly.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sshleifer/tiny-mbart': {'modelId': 'sshleifer/tiny-mbart',\n",
       "  'sha': '9d6b9b3b2774b464bb6b14eda4efe30f82846136',\n",
       "  'lastModified': '2021-08-26T10:55:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'mbart',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'mbart'},\n",
       "  'id': 'sshleifer/tiny-mbart',\n",
       "  'downloads': 32617,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-base-german-cased': {'modelId': 'dbmdz/bert-base-german-cased',\n",
       "  'sha': '56c3dce79f5d93e466f3b800d8e57cddfe13a6d4',\n",
       "  'lastModified': '2021-05-19T14:52:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-base-german-cased',\n",
       "  'downloads': 32109,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'monologg/kobert': {'modelId': 'monologg/kobert',\n",
       "  'sha': '8ebf2818cfd85570737d31ed8cd7aaa000e7056c',\n",
       "  'lastModified': '2021-05-19T23:52:30.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'monologg',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'monologg/kobert',\n",
       "  'downloads': 31895,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/t5-v1_1-large': {'modelId': 'google/t5-v1_1-large',\n",
       "  'sha': '314bc112b191ec17b625ba81438dc73d6c23659d',\n",
       "  'lastModified': '2021-06-23T01:59:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:c4',\n",
       "   'arxiv:2002.05202',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/t5-v1_1-large',\n",
       "  'downloads': 31584,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'datasets': ['c4'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/nli-mpnet-base-v2': {'modelId': 'sentence-transformers/nli-mpnet-base-v2',\n",
       "  'sha': 'c388b46d029476cd6611aa9ed44d05272bbbacfb',\n",
       "  'lastModified': '2022-06-15T20:14:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'mpnet',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['MPNetModel'], 'model_type': 'mpnet'},\n",
       "  'id': 'sentence-transformers/nli-mpnet-base-v2',\n",
       "  'downloads': 30769,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allenai/scibert_scivocab_cased': {'modelId': 'allenai/scibert_scivocab_cased',\n",
       "  'sha': '9be298ced05121c9e6e2b2cb9f508b47b8eae650',\n",
       "  'lastModified': '2021-05-19T11:40:28.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'allenai/scibert_scivocab_cased',\n",
       "  'downloads': 30692,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'beomi/KcELECTRA-base': {'modelId': 'beomi/KcELECTRA-base',\n",
       "  'sha': '686333e78646593e324d6ad5e955dfb6dc9f0f5d',\n",
       "  'lastModified': '2022-06-26T01:49:50.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'electra', 'pretraining', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'beomi',\n",
       "  'config': {'architectures': ['ElectraForPreTraining'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'beomi/KcELECTRA-base',\n",
       "  'downloads': 30652,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ufal/robeczech-base': {'modelId': 'ufal/robeczech-base',\n",
       "  'sha': 'b154a976e0241a2cfb537600fcf936a8540caeb9',\n",
       "  'lastModified': '2022-04-24T11:33:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'cs',\n",
       "   'arxiv:2105.11314',\n",
       "   'transformers',\n",
       "   'Czech',\n",
       "   'RoBERTa',\n",
       "   'ÚFAL',\n",
       "   'license:cc-by-nc-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'ufal',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'ufal/robeczech-base',\n",
       "  'downloads': 30637,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'cs',\n",
       "   'tags': ['Czech', 'RoBERTa', 'ÚFAL'],\n",
       "   'license': 'cc-by-nc-sa-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Sahajtomar/German_Zeroshot': {'modelId': 'Sahajtomar/German_Zeroshot',\n",
       "  'sha': 'd5b0a26665b8538bcb3faa1e63a634cca4c8ee1b',\n",
       "  'lastModified': '2021-05-18T22:22:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'multilingual',\n",
       "   'dataset:xnli',\n",
       "   'transformers',\n",
       "   'nli',\n",
       "   'xnli',\n",
       "   'de',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'Sahajtomar',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Sahajtomar/German_Zeroshot',\n",
       "  'downloads': 30616,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Letzte Woche gab es einen Selbstmord in einer nahe gelegenen kolonie',\n",
       "    'candidate_labels': 'Verbrechen,Tragödie,Stehlen',\n",
       "    'hypothesis_template': 'In deisem geht es um {}.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'tags': ['text-classification', 'pytorch', 'nli', 'xnli', 'de'],\n",
       "   'datasets': ['xnli'],\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'widget': [{'text': 'Letzte Woche gab es einen Selbstmord in einer nahe gelegenen kolonie',\n",
       "     'candidate_labels': 'Verbrechen,Tragödie,Stehlen',\n",
       "     'hypothesis_template': 'In deisem geht es um {}.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/deberta-large-mnli': {'modelId': 'microsoft/deberta-large-mnli',\n",
       "  'sha': '7296194b9009373def4f7c5dad292651e4b5cf4e',\n",
       "  'lastModified': '2021-05-21T20:07:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'deberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'deberta-v1',\n",
       "   'deberta-mnli',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['DebertaForSequenceClassification'],\n",
       "   'model_type': 'deberta'},\n",
       "  'id': 'microsoft/deberta-large-mnli',\n",
       "  'downloads': 30606,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '[CLS] I love you. [SEP] I like you. [SEP]'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['deberta-v1', 'deberta-mnli'],\n",
       "   'tasks': 'mnli',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': '[CLS] I love you. [SEP] I like you. [SEP]'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pyannote/embedding': {'modelId': 'pyannote/embedding',\n",
       "  'sha': '09a3ed256d0fddbf5616fd9fb5db917fcf002708',\n",
       "  'lastModified': '2022-03-23T09:24:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'dataset:voxceleb',\n",
       "   'pyannote-audio',\n",
       "   'pyannote',\n",
       "   'pyannote-audio-model',\n",
       "   'audio',\n",
       "   'voice',\n",
       "   'speech',\n",
       "   'speaker',\n",
       "   'speaker-recognition',\n",
       "   'speaker-verification',\n",
       "   'speaker-identification',\n",
       "   'speaker-embedding',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'pyannote',\n",
       "  'config': None,\n",
       "  'id': 'pyannote/embedding',\n",
       "  'downloads': 30606,\n",
       "  'library_name': 'pyannote-audio',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['pyannote',\n",
       "    'pyannote-audio',\n",
       "    'pyannote-audio-model',\n",
       "    'audio',\n",
       "    'voice',\n",
       "    'speech',\n",
       "    'speaker',\n",
       "    'speaker-recognition',\n",
       "    'speaker-verification',\n",
       "    'speaker-identification',\n",
       "    'speaker-embedding'],\n",
       "   'datasets': ['voxceleb'],\n",
       "   'license': 'mit',\n",
       "   'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'facebook/mbart-large-50': {'modelId': 'facebook/mbart-large-50',\n",
       "  'sha': 'eab25f78110b11bfbf981249a6204e258f8a3312',\n",
       "  'lastModified': '2022-06-25T17:07:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mbart',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'ar',\n",
       "   'cs',\n",
       "   'de',\n",
       "   'en',\n",
       "   'es',\n",
       "   'et',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'gu',\n",
       "   'hi',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'kk',\n",
       "   'ko',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'si',\n",
       "   'tr',\n",
       "   'vi',\n",
       "   'zh',\n",
       "   'af',\n",
       "   'az',\n",
       "   'bn',\n",
       "   'fa',\n",
       "   'he',\n",
       "   'hr',\n",
       "   'id',\n",
       "   'ka',\n",
       "   'km',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'th',\n",
       "   'tl',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'xh',\n",
       "   'gl',\n",
       "   'sl',\n",
       "   'arxiv:2008.00401',\n",
       "   'transformers',\n",
       "   'mbart-50',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['MBartForConditionalGeneration'],\n",
       "   'model_type': 'mbart'},\n",
       "  'id': 'facebook/mbart-large-50',\n",
       "  'downloads': 30094,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'ar',\n",
       "    'cs',\n",
       "    'de',\n",
       "    'en',\n",
       "    'es',\n",
       "    'et',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'gu',\n",
       "    'hi',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'kk',\n",
       "    'ko',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'si',\n",
       "    'tr',\n",
       "    'vi',\n",
       "    'zh',\n",
       "    'af',\n",
       "    'az',\n",
       "    'bn',\n",
       "    'fa',\n",
       "    'he',\n",
       "    'hr',\n",
       "    'id',\n",
       "    'ka',\n",
       "    'km',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'th',\n",
       "    'tl',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'xh',\n",
       "    'gl',\n",
       "    'sl'],\n",
       "   'license': 'mit',\n",
       "   'tags': ['mbart-50']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-ja-en': {'modelId': 'Helsinki-NLP/opus-mt-ja-en',\n",
       "  'sha': '6282eb0555cd0253dc9fac00c5fafb2825ad04b4',\n",
       "  'lastModified': '2021-09-10T13:53:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'ja',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-ja-en',\n",
       "  'downloads': 30050,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract': {'modelId': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
       "  'sha': '3be15ab62caee5db1d45f923410798cdea920010',\n",
       "  'lastModified': '2021-09-22T20:10:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'arxiv:2007.15779',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
       "  'downloads': 29715,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '[MASK] is a tyrosine kinase inhibitor.'}],\n",
       "  'likes': 16,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': '[MASK] is a tyrosine kinase inhibitor.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bigscience/T0': {'modelId': 'bigscience/T0',\n",
       "  'sha': '37f8b7565a0c9945db6a0215b0b823a55e337f4f',\n",
       "  'lastModified': '2022-06-21T01:25:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:bigscience/P3',\n",
       "   'arxiv:2110.08207',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'bigscience',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'bigscience/T0',\n",
       "  'downloads': 29343,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': \"A is the son's of B's uncle. What is the family relationship between A and B?\"},\n",
       "   {'text': 'Reorder the words in this sentence: justin and name bieber years is my am I 27 old.'},\n",
       "   {'text': 'Task: copy but say the opposite.\\n PSG won its match against Barca.'},\n",
       "   {'text': 'Is this review positive or negative? Review: Best cast iron skillet you will every buy.',\n",
       "    'example_title': 'Sentiment analysis'},\n",
       "   {'text': 'Question A: How is air traffic controlled? \\nQuestion B: How do you become an air traffic controller?\\nPick one: these questions are duplicates or not duplicates.'},\n",
       "   {'text': \"Barack Obama nominated Hilary Clinton as his secretary of state on Monday. He chose her because she had foreign affairs experience as a former First Lady. \\nIn the previous sentence, decide who 'her' is referring to.\",\n",
       "    'example_title': 'Coreference resolution'},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\\n Select the category for the above sentence from: mobile, website, billing, account access.'},\n",
       "   {'text': 'Sentence 1: Gyorgy Heizler, head of the local disaster unit, said the coach was carrying 38 passengers.\\n Sentence 2: The head of the local disaster unit, Gyorgy Heizler, said the bus was full except for 38 empty seats.\\n\\n Do sentences 1 and 2 have the same meaning?',\n",
       "    'example_title': 'Paraphrase identification'},\n",
       "   {'text': \"Here's the beginning of an article, choose a tag that best describes the topic of the article: business, cinema, politics, health, travel, sports.\\n\\n The best and worst fo 007 as 'No time to die' marks Daniel Craig's exit.\\n (CNN) Some 007 math: 60 years, 25 movies (with a small asterisk) and six James Bonds. For a Cold War creation, Ian Fleming's suave spy has certainly gotten around, but despite different guises in the tuxedo and occasional scuba gear, when it comes to Bond ratings, there really shouldn't be much argument about who wore it best.\"},\n",
       "   {'text': \"Max: Know any good websites to buy clothes from?\\n Payton: Sure :) LINK 1, LINK 2, LINK 3\\n Max: That's a lot of them!\\n Payton: Yeah, but they have different things so I usually buy things from 2 or 3 of them.\\n Max: I'll check them out. Thanks.\\n\\n Who or what are Payton and Max referring to when they say 'them'?\"},\n",
       "   {'text': \"Is the word 'table' used in the same meaning in the two following sentences?\\n\\n Sentence A: you can leave the books on the table over there.\\n Sentence B: the tables in this book are very hard to read.\"},\n",
       "   {'text': 'On a shelf, there are five books: a gray book, a red book, a purple book, a blue book, and a black book.\\n The red book is to the right of the gray book. The black book is to the left of the blue book. The blue book is to the left of the gray book. The purple book is the second from the right.\\n\\n Which book is the leftmost book?',\n",
       "    'example_title': 'Logic puzzles'},\n",
       "   {'text': \"The two men running to become New York City's next mayor will face off in their first debate Wednesday night.\\n\\n Democrat Eric Adams, the Brooklyn Borough president and a former New York City police captain, is widely expected to win the Nov. 2 election against Republican Curtis Sliwa, the founder of the 1970s-era Guardian Angels anti-crime patril.\\n\\n Who are the men running for mayor?\",\n",
       "    'example_title': 'Reading comprehension'},\n",
       "   {'text': \"The word 'binne' means any animal that is furry and has four legs, and the word 'bam' means a simple sort of dwelling.\\n\\n Which of the following best characterizes binne bams?\\n - Sentence 1: Binne bams are for pets.\\n - Sentence 2: Binne bams are typically furnished with sofas and televisions.\\n - Sentence 3: Binne bams are luxurious apartments.\\n - Sentence 4: Binne bams are places where people live.\"}],\n",
       "  'likes': 22,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['bigscience/P3'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'widget': [{'text': \"A is the son's of B's uncle. What is the family relationship between A and B?\"},\n",
       "    {'text': 'Reorder the words in this sentence: justin and name bieber years is my am I 27 old.'},\n",
       "    {'text': 'Task: copy but say the opposite.\\n PSG won its match against Barca.'},\n",
       "    {'text': 'Is this review positive or negative? Review: Best cast iron skillet you will every buy.',\n",
       "     'example_title': 'Sentiment analysis'},\n",
       "    {'text': 'Question A: How is air traffic controlled? \\nQuestion B: How do you become an air traffic controller?\\nPick one: these questions are duplicates or not duplicates.'},\n",
       "    {'text': \"Barack Obama nominated Hilary Clinton as his secretary of state on Monday. He chose her because she had foreign affairs experience as a former First Lady. \\nIn the previous sentence, decide who 'her' is referring to.\",\n",
       "     'example_title': 'Coreference resolution'},\n",
       "    {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\\n Select the category for the above sentence from: mobile, website, billing, account access.'},\n",
       "    {'text': 'Sentence 1: Gyorgy Heizler, head of the local disaster unit, said the coach was carrying 38 passengers.\\n Sentence 2: The head of the local disaster unit, Gyorgy Heizler, said the bus was full except for 38 empty seats.\\n\\n Do sentences 1 and 2 have the same meaning?',\n",
       "     'example_title': 'Paraphrase identification'},\n",
       "    {'text': \"Here's the beginning of an article, choose a tag that best describes the topic of the article: business, cinema, politics, health, travel, sports.\\n\\n The best and worst fo 007 as 'No time to die' marks Daniel Craig's exit.\\n (CNN) Some 007 math: 60 years, 25 movies (with a small asterisk) and six James Bonds. For a Cold War creation, Ian Fleming's suave spy has certainly gotten around, but despite different guises in the tuxedo and occasional scuba gear, when it comes to Bond ratings, there really shouldn't be much argument about who wore it best.\"},\n",
       "    {'text': \"Max: Know any good websites to buy clothes from?\\n Payton: Sure :) LINK 1, LINK 2, LINK 3\\n Max: That's a lot of them!\\n Payton: Yeah, but they have different things so I usually buy things from 2 or 3 of them.\\n Max: I'll check them out. Thanks.\\n\\n Who or what are Payton and Max referring to when they say 'them'?\"},\n",
       "    {'text': \"Is the word 'table' used in the same meaning in the two following sentences?\\n\\n Sentence A: you can leave the books on the table over there.\\n Sentence B: the tables in this book are very hard to read.\"},\n",
       "    {'text': 'On a shelf, there are five books: a gray book, a red book, a purple book, a blue book, and a black book.\\n The red book is to the right of the gray book. The black book is to the left of the blue book. The blue book is to the left of the gray book. The purple book is the second from the right.\\n\\n Which book is the leftmost book?',\n",
       "     'example_title': 'Logic puzzles'},\n",
       "    {'text': \"The two men running to become New York City's next mayor will face off in their first debate Wednesday night.\\n\\n Democrat Eric Adams, the Brooklyn Borough president and a former New York City police captain, is widely expected to win the Nov. 2 election against Republican Curtis Sliwa, the founder of the 1970s-era Guardian Angels anti-crime patril.\\n\\n Who are the men running for mayor?\",\n",
       "     'example_title': 'Reading comprehension'},\n",
       "    {'text': \"The word 'binne' means any animal that is furry and has four legs, and the word 'bam' means a simple sort of dwelling.\\n\\n Which of the following best characterizes binne bams?\\n - Sentence 1: Binne bams are for pets.\\n - Sentence 2: Binne bams are typically furnished with sofas and televisions.\\n - Sentence 3: Binne bams are luxurious apartments.\\n - Sentence 4: Binne bams are places where people live.\"}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mrm8488/bert-small-finetuned-squadv2': {'modelId': 'mrm8488/bert-small-finetuned-squadv2',\n",
       "  'sha': '3ffb743e93b64bc944f778292a71ebac650834ae',\n",
       "  'lastModified': '2021-05-20T00:33:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'arxiv:1908.08962',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'mrm8488/bert-small-finetuned-squadv2',\n",
       "  'downloads': 29041,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'thumbnail': None},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'snrspeaks/t5-one-line-summary': {'modelId': 'snrspeaks/t5-one-line-summary',\n",
       "  'sha': '62acf01b9c91b2ea3a84b1e83a8ee0557cc3526c',\n",
       "  'lastModified': '2021-06-23T14:20:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'dataset:arxiv',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'snrspeaks',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'snrspeaks/t5-one-line-summary',\n",
       "  'downloads': 28720,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': \"summarize: We describe a system called Overton, whose main design goal is to support engineers in building, monitoring, and improving production  machinelearning systems. Key challenges engineers face are monitoring fine-grained quality, diagnosing errors in sophisticated applications, and  handling contradictory or incomplete supervision data. Overton automates the life cycle of model construction, deployment, and monitoring by providing a set of novel high-level, declarative abstractions. Overton's vision is to shift developers to these higher-level tasks instead of lower-level machine learning tasks.  In fact, using Overton, engineers can build deep-learning-based applications without writing any code in frameworks like TensorFlow. For over a year,  Overton has been used in production to support multiple applications in both near-real-time applications and back-of-house processing.  In that time, Overton-based applications have answered billions of queries in multiple languages and processed trillions of records reducing errors  1.7-2.9 times versus production systems.\"}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['arxiv'],\n",
       "   'widget': [{'text': \"summarize: We describe a system called Overton, whose main design goal is to support engineers in building, monitoring, and improving production  machinelearning systems. Key challenges engineers face are monitoring fine-grained quality, diagnosing errors in sophisticated applications, and  handling contradictory or incomplete supervision data. Overton automates the life cycle of model construction, deployment, and monitoring by providing a set of novel high-level, declarative abstractions. Overton's vision is to shift developers to these higher-level tasks instead of lower-level machine learning tasks.  In fact, using Overton, engineers can build deep-learning-based applications without writing any code in frameworks like TensorFlow. For over a year,  Overton has been used in production to support multiple applications in both near-real-time applications and back-of-house processing.  In that time, Overton-based applications have answered billions of queries in multiple languages and processed trillions of records reducing errors  1.7-2.9 times versus production systems.\"}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-en-zh': {'modelId': 'Helsinki-NLP/opus-mt-en-zh',\n",
       "  'sha': '93db7712e4698309ac17a80605adbf54dea5c8ee',\n",
       "  'lastModified': '2021-09-09T21:40:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-zh',\n",
       "  'downloads': 28653,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 16,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'zh'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'prithivida/parrot_adequacy_model': {'modelId': 'prithivida/parrot_adequacy_model',\n",
       "  'sha': '87a35bc291d7455cfc86fc5f6a374c92de0156af',\n",
       "  'lastModified': '2022-05-27T02:47:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'prithivida',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'prithivida/parrot_adequacy_model',\n",
       "  'downloads': 28567,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/mdeberta-v3-base': {'modelId': 'microsoft/mdeberta-v3-base',\n",
       "  'sha': '7d66e84a399b78accc72e0f61cd6d50f02ee1c2c',\n",
       "  'lastModified': '2022-01-13T19:41:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'deberta-v2',\n",
       "   'multilingual',\n",
       "   'arxiv:2006.03654',\n",
       "   'arxiv:2111.09543',\n",
       "   'transformers',\n",
       "   'deberta',\n",
       "   'deberta-v3',\n",
       "   'mdeberta',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'deberta-v2'},\n",
       "  'id': 'microsoft/mdeberta-v3-base',\n",
       "  'downloads': 28528,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 32,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'tags': ['deberta', 'deberta-v3', 'mdeberta'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'julien-c/bert-xsmall-dummy': {'modelId': 'julien-c/bert-xsmall-dummy',\n",
       "  'sha': '9d3811da21adb66feb315118023f528ed10c6b18',\n",
       "  'lastModified': '2021-05-19T20:53:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'julien-c',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'julien-c/bert-xsmall-dummy',\n",
       "  'downloads': 28053,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KoboldAI/fairseq-dense-125M': {'modelId': 'KoboldAI/fairseq-dense-125M',\n",
       "  'sha': 'ee40bc27509c93cd551f2791bb9d462bbab4d450',\n",
       "  'lastModified': '2022-02-01T22:48:03.000Z',\n",
       "  'tags': ['pytorch', 'xglm', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'KoboldAI',\n",
       "  'config': {'architectures': ['XGLMForCausalLM'], 'model_type': 'xglm'},\n",
       "  'id': 'KoboldAI/fairseq-dense-125M',\n",
       "  'downloads': 27830,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'thatdramebaazguy/roberta-base-squad': {'modelId': 'thatdramebaazguy/roberta-base-squad',\n",
       "  'sha': 'a7a26fd500148b760ca89a87edcd5b5605daab09',\n",
       "  'lastModified': '2022-07-01T19:12:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'question-answering',\n",
       "   'English',\n",
       "   'dataset:SQuAD',\n",
       "   'transformers',\n",
       "   'roberta-base',\n",
       "   'qa',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'thatdramebaazguy',\n",
       "  'config': {'architectures': ['RobertaForQuestionAnswering'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'thatdramebaazguy/roberta-base-squad',\n",
       "  'downloads': 27770,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['SQuAD'],\n",
       "   'language': ['English'],\n",
       "   'thumbnail': None,\n",
       "   'tags': ['roberta', 'roberta-base', 'question-answering', 'qa'],\n",
       "   'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-gpt2': {'modelId': 'hf-internal-testing/tiny-random-gpt2',\n",
       "  'sha': '937b4d23b6648f5a1a0d1247b939b26981798903',\n",
       "  'lastModified': '2021-09-17T19:24:03.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'gpt2', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'model_type': 'gpt2'},\n",
       "  'id': 'hf-internal-testing/tiny-random-gpt2',\n",
       "  'downloads': 27601,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'cross-encoder/ms-marco-electra-base': {'modelId': 'cross-encoder/ms-marco-electra-base',\n",
       "  'sha': '69c22886dd57c67783a8f48af5b86a35657df8f6',\n",
       "  'lastModified': '2021-08-05T08:40:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'electra',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['ElectraForSequenceClassification'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'cross-encoder/ms-marco-electra-base',\n",
       "  'downloads': 27376,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'skt/kogpt2-base-v2': {'modelId': 'skt/kogpt2-base-v2',\n",
       "  'sha': 'd0c0df48bf2b2c9350dd855021a5b216f560c0c7',\n",
       "  'lastModified': '2021-09-23T16:29:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'ko',\n",
       "   'transformers',\n",
       "   'license:cc-by-nc-sa-4.0'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'skt',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'skt/kogpt2-base-v2',\n",
       "  'downloads': 27158,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ko',\n",
       "   'tags': ['gpt2'],\n",
       "   'license': 'cc-by-nc-sa-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flair/ner-english-ontonotes-fast': {'modelId': 'flair/ner-english-ontonotes-fast',\n",
       "  'sha': '38a8eb6a720791da55e15962c36a37dd8d8270b2',\n",
       "  'lastModified': '2021-03-02T22:05:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'dataset:ontonotes',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/ner-english-ontonotes-fast',\n",
       "  'downloads': 27003,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'On September 1st George Washington won 1 dollar.'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': 'en',\n",
       "   'datasets': ['ontonotes'],\n",
       "   'widget': [{'text': 'On September 1st George Washington won 1 dollar.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/msmarco-bert-base-dot-v5': {'modelId': 'sentence-transformers/msmarco-bert-base-dot-v5',\n",
       "  'sha': '668e63a378bc93d76c430af68338e550dc78df09',\n",
       "  'lastModified': '2022-06-15T20:34:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/msmarco-bert-base-dot-v5',\n",
       "  'downloads': 26906,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/blenderbot-3B': {'modelId': 'facebook/blenderbot-3B',\n",
       "  'sha': 'c468b2376f5f49d20624f31383023f2bbd360c8d',\n",
       "  'lastModified': '2021-09-21T19:45:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'blenderbot',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:blended_skill_talk',\n",
       "   'arxiv:1907.06616',\n",
       "   'transformers',\n",
       "   'convAI',\n",
       "   'conversational',\n",
       "   'facebook',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['BlenderbotForConditionalGeneration'],\n",
       "   'model_type': 'blenderbot'},\n",
       "  'id': 'facebook/blenderbot-3B',\n",
       "  'downloads': 26649,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 19,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'thumbnail': None,\n",
       "   'tags': ['convAI', 'conversational', 'facebook'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['blended_skill_talk'],\n",
       "   'metrics': ['perplexity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wav2vec2-large-960h-lv60-self': {'modelId': 'facebook/wav2vec2-large-960h-lv60-self',\n",
       "  'sha': '54074b1c16f4de6a5ad59affb4caa8f2ea03a119',\n",
       "  'lastModified': '2022-05-23T16:13:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'en',\n",
       "   'dataset:librispeech_asr',\n",
       "   'arxiv:2010.11430',\n",
       "   'arxiv:2006.11477',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'audio',\n",
       "   'hf-asr-leaderboard',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'facebook/wav2vec2-large-960h-lv60-self',\n",
       "  'downloads': 26534,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 16,\n",
       "  'model-index': [{'name': 'wav2vec2-large-960h-lv60',\n",
       "    'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'LibriSpeech (clean)',\n",
       "       'type': 'librispeech_asr',\n",
       "       'config': 'clean',\n",
       "       'split': 'test',\n",
       "       'args': {'language': 'en'}},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 1.9}]},\n",
       "     {'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'LibriSpeech (other)',\n",
       "       'type': 'librispeech_asr',\n",
       "       'config': 'other',\n",
       "       'split': 'test',\n",
       "       'args': {'language': 'en'}},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 3.9}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['librispeech_asr'],\n",
       "   'tags': ['speech',\n",
       "    'audio',\n",
       "    'automatic-speech-recognition',\n",
       "    'hf-asr-leaderboard'],\n",
       "   'license': 'apache-2.0',\n",
       "   'model-index': [{'name': 'wav2vec2-large-960h-lv60',\n",
       "     'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'LibriSpeech (clean)',\n",
       "        'type': 'librispeech_asr',\n",
       "        'config': 'clean',\n",
       "        'split': 'test',\n",
       "        'args': {'language': 'en'}},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 1.9}]},\n",
       "      {'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'LibriSpeech (other)',\n",
       "        'type': 'librispeech_asr',\n",
       "        'config': 'other',\n",
       "        'split': 'test',\n",
       "        'args': {'language': 'en'}},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 3.9}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'uklfr/gottbert-base': {'modelId': 'uklfr/gottbert-base',\n",
       "  'sha': '301ea863069cb7f7226a8bc6b1311b0bc6b7b1d4',\n",
       "  'lastModified': '2021-09-16T15:36:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'arxiv:2012.02110',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'uklfr',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'uklfr/gottbert-base',\n",
       "  'downloads': 26503,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'joeddav/bart-large-mnli-yahoo-answers': {'modelId': 'joeddav/bart-large-mnli-yahoo-answers',\n",
       "  'sha': 'd836606b3cf20652cf30283d6884ae26a11e5392',\n",
       "  'lastModified': '2021-06-14T10:44:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bart',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:yahoo-answers',\n",
       "   'arxiv:1909.00161',\n",
       "   'transformers',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'joeddav',\n",
       "  'config': {'architectures': ['BartForSequenceClassification'],\n",
       "   'model_type': 'bart'},\n",
       "  'id': 'joeddav/bart-large-mnli-yahoo-answers',\n",
       "  'downloads': 26305,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system’s largest world. Konstantin Batygin did not set out to solve one of the solar system’s most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system’s missing “Planet Nine,” spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn’t rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: “Oh! This is how Europa formed.” Europa is one of Jupiter’s four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the Côte d’Azur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system’s formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['text-classification', 'pytorch'],\n",
       "   'datasets': ['yahoo-answers'],\n",
       "   'pipeline_tag': 'zero-shot-classification'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/deberta-large': {'modelId': 'microsoft/deberta-large',\n",
       "  'sha': '822a8791fdac38e8086e2731158047e9b63e4521',\n",
       "  'lastModified': '2022-01-13T17:10:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'deberta',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'deberta-v1',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'deberta'},\n",
       "  'id': 'microsoft/deberta-large',\n",
       "  'downloads': 26046,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': 'deberta-v1',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'google/t5-small-lm-adapt': {'modelId': 'google/t5-small-lm-adapt',\n",
       "  'sha': 'ceece9332ccd73f589b2c764fa0e334c597952d4',\n",
       "  'lastModified': '2021-11-01T13:58:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:c4',\n",
       "   'arxiv:2002.05202',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   't5-lm-adapt',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/t5-small-lm-adapt',\n",
       "  'downloads': 26031,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['c4'],\n",
       "   'tags': ['t5-lm-adapt'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'chkla/roberta-argument': {'modelId': 'chkla/roberta-argument',\n",
       "  'sha': 'd5480352a5ad33b0135cc1193a62be24396e557a',\n",
       "  'lastModified': '2021-05-20T15:19:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'english',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'chkla',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'chkla/roberta-argument',\n",
       "  'downloads': 25982,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'It has been determined that the amount of greenhouse gases have decreased by almost half because of the prevalence in the utilization of nuclear power.'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'english',\n",
       "   'widget': [{'text': 'It has been determined that the amount of greenhouse gases have decreased by almost half because of the prevalence in the utilization of nuclear power.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/msmarco-distilbert-base-v4': {'modelId': 'sentence-transformers/msmarco-distilbert-base-v4',\n",
       "  'sha': '62b749054617919f8d1e8462a987edea4b998e3c',\n",
       "  'lastModified': '2022-06-15T19:32:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/msmarco-distilbert-base-v4',\n",
       "  'downloads': 25884,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Vamsi/T5_Paraphrase_Paws': {'modelId': 'Vamsi/T5_Paraphrase_Paws',\n",
       "  'sha': '3bbf07dc42d5ddc9ca77c5589ce7239b0b731832',\n",
       "  'lastModified': '2021-06-23T11:39:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'paraphrase-generation',\n",
       "   'text-generation',\n",
       "   'Conditional Generation',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'Vamsi',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'Vamsi/T5_Paraphrase_Paws',\n",
       "  'downloads': 25843,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['paraphrase-generation',\n",
       "    'text-generation',\n",
       "    'Conditional Generation'],\n",
       "   'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'unitary/multilingual-toxic-xlm-roberta': {'modelId': 'unitary/multilingual-toxic-xlm-roberta',\n",
       "  'sha': '19f5c53459ec9679c675aeead38cab87cf588944',\n",
       "  'lastModified': '2021-05-06T11:04:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'text-classification',\n",
       "   'arxiv:1703.04009',\n",
       "   'arxiv:1905.12516',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'unitary',\n",
       "  'config': {'architectures': ['XLMRobertaForSequenceClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'unitary/multilingual-toxic-xlm-roberta',\n",
       "  'downloads': 25836,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'text-classification'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'siebert/sentiment-roberta-large-english': {'modelId': 'siebert/sentiment-roberta-large-english',\n",
       "  'sha': 'acf101658300b9531e65b06504421a0d46033f86',\n",
       "  'lastModified': '2022-05-25T09:28:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'arxiv:1907.11692',\n",
       "   'transformers',\n",
       "   'sentiment',\n",
       "   'twitter',\n",
       "   'reviews',\n",
       "   'siebert'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'siebert',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'siebert/sentiment-roberta-large-english',\n",
       "  'downloads': 25831,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 23,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['sentiment', 'twitter', 'reviews', 'siebert']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/bart-large-xsum': {'modelId': 'facebook/bart-large-xsum',\n",
       "  'sha': 'e5a049949143586befac0f09a9716bde79b55e77',\n",
       "  'lastModified': '2021-06-14T07:39:59.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:1910.13461',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {}},\n",
       "  'id': 'facebook/bart-large-xsum',\n",
       "  'downloads': 25814,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['summarization'],\n",
       "   'language': ['en'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sberbank-ai/ruclip-vit-base-patch32-384': {'modelId': 'sberbank-ai/ruclip-vit-base-patch32-384',\n",
       "  'sha': '1f7f08e5437de5dd5beba7a448983b7e4135891b',\n",
       "  'lastModified': '2022-01-10T00:21:50.000Z',\n",
       "  'tags': ['pytorch', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'sberbank-ai',\n",
       "  'config': {},\n",
       "  'id': 'sberbank-ai/ruclip-vit-base-patch32-384',\n",
       "  'downloads': 25547,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'cross-encoder/qnli-electra-base': {'modelId': 'cross-encoder/qnli-electra-base',\n",
       "  'sha': '4d70c22ec2d12ec7663a70fbe3180a408c980a2a',\n",
       "  'lastModified': '2021-08-05T08:41:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'electra',\n",
       "   'text-classification',\n",
       "   'arxiv:1804.07461',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['ElectraForSequenceClassification'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'cross-encoder/qnli-electra-base',\n",
       "  'downloads': 24800,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/tapas-base': {'modelId': 'google/tapas-base',\n",
       "  'sha': '00456266840bb0a319cd6748ebf7da3caf98816b',\n",
       "  'lastModified': '2021-11-29T10:03:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'tapas',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'arxiv:2004.02349',\n",
       "   'arxiv:2010.00571',\n",
       "   'transformers',\n",
       "   'TapasModel',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['TapasModel'], 'model_type': 'tapas'},\n",
       "  'id': 'google/tapas-base',\n",
       "  'downloads': 24754,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['tapas', 'TapasModel'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dmis-lab/biosyn-sapbert-bc2gn': {'modelId': 'dmis-lab/biosyn-sapbert-bc2gn',\n",
       "  'sha': '28ef41eace90e9aa6a9db372413c145883c72902',\n",
       "  'lastModified': '2022-02-25T13:32:53.000Z',\n",
       "  'tags': ['pytorch', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'dmis-lab',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'dmis-lab/biosyn-sapbert-bc2gn',\n",
       "  'downloads': 24749,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'r3dhummingbird/DialoGPT-medium-joshua': {'modelId': 'r3dhummingbird/DialoGPT-medium-joshua',\n",
       "  'sha': 'ff22e98bcb70ae1e082f54640c5c3bafd3950125',\n",
       "  'lastModified': '2021-07-19T23:18:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'r3dhummingbird',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'r3dhummingbird/DialoGPT-medium-joshua',\n",
       "  'downloads': 24598,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://raw.githubusercontent.com/RuolinZheng08/twewy-discord-chatbot/main/gif-demo/icon.png',\n",
       "   'tags': ['conversational'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ml6team/bert-base-uncased-city-country-ner': {'modelId': 'ml6team/bert-base-uncased-city-country-ner',\n",
       "  'sha': 'e38e683af1174120b192661dbdcbc2358fe56964',\n",
       "  'lastModified': '2022-07-01T07:27:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'en',\n",
       "   'dataset:Ultra Fine Entity Typing',\n",
       "   'transformers',\n",
       "   'address-NER',\n",
       "   'NER',\n",
       "   'bert-base-uncased',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'ml6team',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'ml6team/bert-base-uncased-city-country-ner',\n",
       "  'downloads': 24563,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Hi, I am Kermit and I live in Berlin'},\n",
       "   {'text': 'It is very difficult to find a house in Berlin, Germany.'},\n",
       "   {'text': 'ML6 is a very cool company from Belgium'},\n",
       "   {'text': 'Samuel ppops in a happy plce called Berlin which happens to be Kazakhstan'},\n",
       "   {'text': 'My family and I visited Montreal, Canada last week and the flight from Amsterdam took 9 hours'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['token-classification', 'address-NER', 'NER', 'bert-base-uncased'],\n",
       "   'datasets': ['Ultra Fine Entity Typing'],\n",
       "   'metrics': ['Precision', 'Recall', 'F1 Score'],\n",
       "   'widget': [{'text': 'Hi, I am Kermit and I live in Berlin'},\n",
       "    {'text': 'It is very difficult to find a house in Berlin, Germany.'},\n",
       "    {'text': 'ML6 is a very cool company from Belgium'},\n",
       "    {'text': 'Samuel ppops in a happy plce called Berlin which happens to be Kazakhstan'},\n",
       "    {'text': 'My family and I visited Montreal, Canada last week and the flight from Amsterdam took 9 hours'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-pl-en': {'modelId': 'Helsinki-NLP/opus-mt-pl-en',\n",
       "  'sha': '361ac28538863fafa2090bf91c36d02b9c596d5b',\n",
       "  'lastModified': '2021-09-10T14:01:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'pl',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-pl-en',\n",
       "  'downloads': 24454,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KoichiYasuoka/bert-base-japanese-char-extended': {'modelId': 'KoichiYasuoka/bert-base-japanese-char-extended',\n",
       "  'sha': 'ec39844667602ffc6fc2fa1958ee683b667421f8',\n",
       "  'lastModified': '2022-06-20T22:21:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ja',\n",
       "   'transformers',\n",
       "   'japanese',\n",
       "   'masked-lm',\n",
       "   'wikipedia',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'KoichiYasuoka',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'KoichiYasuoka/bert-base-japanese-char-extended',\n",
       "  'downloads': 24216,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '酸素ボンベを充[MASK]する。'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ja'],\n",
       "   'tags': ['japanese', 'masked-lm', 'wikipedia'],\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'mask_token': '[MASK]',\n",
       "   'widget': [{'text': '酸素ボンベを充[MASK]する。'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wmt19-en-de': {'modelId': 'facebook/wmt19-en-de',\n",
       "  'sha': 'b33976783993b11baabc19313275865ee87931e3',\n",
       "  'lastModified': '2020-12-11T21:39:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'fsmt',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'de',\n",
       "   'dataset:wmt19',\n",
       "   'arxiv:1907.06616',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'wmt19',\n",
       "   'facebook',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['FSMTForConditionalGeneration'],\n",
       "   'model_type': 'fsmt'},\n",
       "  'id': 'facebook/wmt19-en-de',\n",
       "  'downloads': 24076,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'de'],\n",
       "   'tags': ['translation', 'wmt19', 'facebook'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wmt19'],\n",
       "   'metrics': ['bleu'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/paraphrase-TinyBERT-L6-v2': {'modelId': 'sentence-transformers/paraphrase-TinyBERT-L6-v2',\n",
       "  'sha': '8fe7263a517189c4a11a98f87db8ac964b235b5f',\n",
       "  'lastModified': '2022-06-15T20:12:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/paraphrase-TinyBERT-L6-v2',\n",
       "  'downloads': 23958,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-en-ar': {'modelId': 'Helsinki-NLP/opus-mt-en-ar',\n",
       "  'sha': '12f7bc254b7e475b6377f440d488063d7fb51571',\n",
       "  'lastModified': '2021-02-28T14:15:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'ar',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-ar',\n",
       "  'downloads': 23955,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'ar'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sshleifer/tiny-distilbert-base-cased-distilled-squad': {'modelId': 'sshleifer/tiny-distilbert-base-cased-distilled-squad',\n",
       "  'sha': '33a976c7ab7d41310ea4063d311dbf66c8aaa001',\n",
       "  'lastModified': '2020-05-14T16:54:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['DistilBertForQuestionAnswering'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'sshleifer/tiny-distilbert-base-cased-distilled-squad',\n",
       "  'downloads': 23866,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KoichiYasuoka/bert-base-japanese-upos': {'modelId': 'KoichiYasuoka/bert-base-japanese-upos',\n",
       "  'sha': 'e9077f5b327b42da14e3c5d60f529331a68f9eed',\n",
       "  'lastModified': '2022-05-23T21:50:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'ja',\n",
       "   'dataset:universal_dependencies',\n",
       "   'transformers',\n",
       "   'japanese',\n",
       "   'pos',\n",
       "   'wikipedia',\n",
       "   'dependency-parsing',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'KoichiYasuoka',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'KoichiYasuoka/bert-base-japanese-upos',\n",
       "  'downloads': 23800,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '国境の長いトンネルを抜けると雪国であった。'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ja'],\n",
       "   'tags': ['japanese',\n",
       "    'token-classification',\n",
       "    'pos',\n",
       "    'wikipedia',\n",
       "    'dependency-parsing'],\n",
       "   'datasets': ['universal_dependencies'],\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'widget': [{'text': '国境の長いトンネルを抜けると雪国であった。'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/opt-1.3b': {'modelId': 'facebook/opt-1.3b',\n",
       "  'sha': 'aa6ac1e23bb9a499be2b7400079cd2a7b8a1309a',\n",
       "  'lastModified': '2022-06-22T09:53:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'opt',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'arxiv:2205.01068',\n",
       "   'arxiv:2005.14165',\n",
       "   'transformers',\n",
       "   'license:other'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['OPTForCausalLM'], 'model_type': 'opt'},\n",
       "  'id': 'facebook/opt-1.3b',\n",
       "  'downloads': 23550,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'inference': False,\n",
       "   'tags': ['text-generation', 'opt'],\n",
       "   'license': 'other',\n",
       "   'commercial': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'neuralmind/bert-large-portuguese-cased': {'modelId': 'neuralmind/bert-large-portuguese-cased',\n",
       "  'sha': 'aa302f6ea73b759f7df9cad58bd272127b67ec28',\n",
       "  'lastModified': '2021-05-20T01:31:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'pt',\n",
       "   'dataset:brWaC',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'neuralmind',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'neuralmind/bert-large-portuguese-cased',\n",
       "  'downloads': 23484,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'pt',\n",
       "   'license': 'mit',\n",
       "   'tags': ['bert', 'pytorch'],\n",
       "   'datasets': ['brWaC']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-bert-for-token-classification': {'modelId': 'hf-internal-testing/tiny-bert-for-token-classification',\n",
       "  'sha': 'f89ef50d84f2959688279d4b2c09faf823da2069',\n",
       "  'lastModified': '2021-12-16T11:04:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'hf-internal-testing/tiny-bert-for-token-classification',\n",
       "  'downloads': 23114,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-base-cased-finetuned-mrpc': {'modelId': 'bert-base-cased-finetuned-mrpc',\n",
       "  'sha': 'f53cb9cb49541a34be140979efe098073a179b0f',\n",
       "  'lastModified': '2021-05-18T16:08:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-base-cased-finetuned-mrpc',\n",
       "  'downloads': 23108,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/stsb-distilroberta-base': {'modelId': 'cross-encoder/stsb-distilroberta-base',\n",
       "  'sha': '2a387f03597b030ff3dadcef7d73456ce23e3bb7',\n",
       "  'lastModified': '2021-08-05T08:41:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cross-encoder/stsb-distilroberta-base',\n",
       "  'downloads': 23067,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/mt5-large': {'modelId': 'google/mt5-large',\n",
       "  'sha': 'bdd096d7cf0fc531444a0db2e0a9a209d0a5f8c0',\n",
       "  'lastModified': '2022-05-27T15:06:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'mt5',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'az',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'ca',\n",
       "   'ceb',\n",
       "   'co',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'eo',\n",
       "   'es',\n",
       "   'et',\n",
       "   'eu',\n",
       "   'fa',\n",
       "   'fi',\n",
       "   'fil',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'haw',\n",
       "   'hi',\n",
       "   'hmn',\n",
       "   'ht',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'ig',\n",
       "   'is',\n",
       "   'it',\n",
       "   'iw',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'ku',\n",
       "   'ky',\n",
       "   'la',\n",
       "   'lb',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mi',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'mt',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'ny',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sd',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'sm',\n",
       "   'sn',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'st',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'tg',\n",
       "   'th',\n",
       "   'tr',\n",
       "   'uk',\n",
       "   'und',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'xh',\n",
       "   'yi',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'zu',\n",
       "   'dataset:mc4',\n",
       "   'arxiv:2010.11934',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['MT5ForConditionalGeneration'],\n",
       "   'model_type': 'mt5'},\n",
       "  'id': 'google/mt5-large',\n",
       "  'downloads': 22575,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'az',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'ca',\n",
       "    'ceb',\n",
       "    'co',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'eo',\n",
       "    'es',\n",
       "    'et',\n",
       "    'eu',\n",
       "    'fa',\n",
       "    'fi',\n",
       "    'fil',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'haw',\n",
       "    'hi',\n",
       "    'hmn',\n",
       "    'ht',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'ig',\n",
       "    'is',\n",
       "    'it',\n",
       "    'iw',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'ku',\n",
       "    'ky',\n",
       "    'la',\n",
       "    'lb',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mi',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'mt',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'ny',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sd',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'sm',\n",
       "    'sn',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'st',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'tg',\n",
       "    'th',\n",
       "    'tr',\n",
       "    'uk',\n",
       "    'und',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'xh',\n",
       "    'yi',\n",
       "    'yo',\n",
       "    'zh',\n",
       "    'zu'],\n",
       "   'datasets': ['mc4'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'jonatasgrosman/wav2vec2-large-xlsr-53-english': {'modelId': 'jonatasgrosman/wav2vec2-large-xlsr-53-english',\n",
       "  'sha': '5d103dcbb7fec00310f624f8d863bc3a69bf10ad',\n",
       "  'lastModified': '2022-06-22T17:04:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'en',\n",
       "   'dataset:common_voice',\n",
       "   'dataset:mozilla-foundation/common_voice_6_0',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'hf-asr-leaderboard',\n",
       "   'mozilla-foundation/common_voice_6_0',\n",
       "   'robust-speech-event',\n",
       "   'speech',\n",
       "   'xlsr-fine-tuning-week',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'jonatasgrosman',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'jonatasgrosman/wav2vec2-large-xlsr-53-english',\n",
       "  'downloads': 22574,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 15,\n",
       "  'model-index': [{'name': 'XLSR Wav2Vec2 English by Jonatas Grosman',\n",
       "    'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Common Voice en',\n",
       "       'type': 'common_voice',\n",
       "       'args': 'en'},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 19.06},\n",
       "       {'name': 'Test CER', 'type': 'cer', 'value': 7.69},\n",
       "       {'name': 'Test WER (+LM)', 'type': 'wer', 'value': 14.81},\n",
       "       {'name': 'Test CER (+LM)', 'type': 'cer', 'value': 6.84}]},\n",
       "     {'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Robust Speech Event - Dev Data',\n",
       "       'type': 'speech-recognition-community-v2/dev_data',\n",
       "       'args': 'en'},\n",
       "      'metrics': [{'name': 'Dev WER', 'type': 'wer', 'value': 27.72},\n",
       "       {'name': 'Dev CER', 'type': 'cer', 'value': 11.65},\n",
       "       {'name': 'Dev WER (+LM)', 'type': 'wer', 'value': 20.85},\n",
       "       {'name': 'Dev CER (+LM)', 'type': 'cer', 'value': 11.01}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['common_voice', 'mozilla-foundation/common_voice_6_0'],\n",
       "   'metrics': ['wer', 'cer'],\n",
       "   'tags': ['audio',\n",
       "    'automatic-speech-recognition',\n",
       "    'en',\n",
       "    'hf-asr-leaderboard',\n",
       "    'mozilla-foundation/common_voice_6_0',\n",
       "    'robust-speech-event',\n",
       "    'speech',\n",
       "    'xlsr-fine-tuning-week'],\n",
       "   'license': 'apache-2.0',\n",
       "   'model-index': [{'name': 'XLSR Wav2Vec2 English by Jonatas Grosman',\n",
       "     'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Common Voice en',\n",
       "        'type': 'common_voice',\n",
       "        'args': 'en'},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 19.06},\n",
       "        {'name': 'Test CER', 'type': 'cer', 'value': 7.69},\n",
       "        {'name': 'Test WER (+LM)', 'type': 'wer', 'value': 14.81},\n",
       "        {'name': 'Test CER (+LM)', 'type': 'cer', 'value': 6.84}]},\n",
       "      {'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Robust Speech Event - Dev Data',\n",
       "        'type': 'speech-recognition-community-v2/dev_data',\n",
       "        'args': 'en'},\n",
       "       'metrics': [{'name': 'Dev WER', 'type': 'wer', 'value': 27.72},\n",
       "        {'name': 'Dev CER', 'type': 'cer', 'value': 11.65},\n",
       "        {'name': 'Dev WER (+LM)', 'type': 'wer', 'value': 20.85},\n",
       "        {'name': 'Dev CER (+LM)', 'type': 'cer', 'value': 11.01}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'Michau/t5-base-en-generate-headline': {'modelId': 'Michau/t5-base-en-generate-headline',\n",
       "  'sha': 'f526532f788c45b6b6288286e5ef929fa768ef6a',\n",
       "  'lastModified': '2021-06-23T03:17:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'Michau',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'Michau/t5-base-en-generate-headline',\n",
       "  'downloads': 22504,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 17,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'monologg/koelectra-base-v3-discriminator': {'modelId': 'monologg/koelectra-base-v3-discriminator',\n",
       "  'sha': '68b30cd259f34a4b5aa8786392612ba2a2617fcc',\n",
       "  'lastModified': '2021-10-20T16:53:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'electra',\n",
       "   'pretraining',\n",
       "   'ko',\n",
       "   'transformers',\n",
       "   'korean',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'monologg',\n",
       "  'config': {'architectures': ['ElectraForPreTraining'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'monologg/koelectra-base-v3-discriminator',\n",
       "  'downloads': 22385,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ko', 'license': 'apache-2.0', 'tags': ['korean']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wmt19-de-en': {'modelId': 'facebook/wmt19-de-en',\n",
       "  'sha': '80d366f635721148ffa2a0a58591cb672c9b4982',\n",
       "  'lastModified': '2020-12-11T21:39:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'fsmt',\n",
       "   'text2text-generation',\n",
       "   'de',\n",
       "   'en',\n",
       "   'dataset:wmt19',\n",
       "   'arxiv:1907.06616',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'wmt19',\n",
       "   'facebook',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['FSMTForConditionalGeneration'],\n",
       "   'model_type': 'fsmt'},\n",
       "  'id': 'facebook/wmt19-de-en',\n",
       "  'downloads': 22367,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['de', 'en'],\n",
       "   'tags': ['translation', 'wmt19', 'facebook'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wmt19'],\n",
       "   'metrics': ['bleu'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-mbart': {'modelId': 'hf-internal-testing/tiny-random-mbart',\n",
       "  'sha': '63c7077c54936948aeb5a675e10489c945957824',\n",
       "  'lastModified': '2021-12-28T12:24:41.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'mbart', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'model_type': 'mbart'},\n",
       "  'id': 'hf-internal-testing/tiny-random-mbart',\n",
       "  'downloads': 22271,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'teacookies/autonlp-more_fine_tune_24465520-26265908': {'modelId': 'teacookies/autonlp-more_fine_tune_24465520-26265908',\n",
       "  'sha': 'c935bca0d7748cdbaad58100718074cedf186ae7',\n",
       "  'lastModified': '2021-10-25T09:36:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'question-answering',\n",
       "   'unk',\n",
       "   'dataset:teacookies/autonlp-data-more_fine_tune_24465520',\n",
       "   'transformers',\n",
       "   'autonlp',\n",
       "   'co2_eq_emissions',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'teacookies',\n",
       "  'config': {'architectures': ['XLMRobertaForQuestionAnswering'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'teacookies/autonlp-more_fine_tune_24465520-26265908',\n",
       "  'downloads': 21960,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Who loves AutoNLP?',\n",
       "    'context': 'Everyone loves AutoNLP'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['autonlp', 'question-answering'],\n",
       "   'language': 'unk',\n",
       "   'widget': [{'text': 'Who loves AutoNLP?',\n",
       "     'context': 'Everyone loves AutoNLP'}],\n",
       "   'datasets': ['teacookies/autonlp-data-more_fine_tune_24465520'],\n",
       "   'co2_eq_emissions': 96.32087452115675},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/t5-v1_1-small': {'modelId': 'google/t5-v1_1-small',\n",
       "  'sha': 'fb7e6cba609f7bab11c614294bc04f82f613c7b1',\n",
       "  'lastModified': '2021-06-23T00:37:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:c4',\n",
       "   'arxiv:2002.05202',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/t5-v1_1-small',\n",
       "  'downloads': 21570,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'datasets': ['c4'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-id-en': {'modelId': 'Helsinki-NLP/opus-mt-id-en',\n",
       "  'sha': 'cbdb70ef26d3c5a6585e6a810da1003bd50bb6b3',\n",
       "  'lastModified': '2021-09-09T22:11:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'id',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-id-en',\n",
       "  'downloads': 21462,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nghuyong/ernie-2.0-large-en': {'modelId': 'nghuyong/ernie-2.0-large-en',\n",
       "  'sha': '4770fb35e20abf0e2ed2ba0a70faec4fc55b5d2b',\n",
       "  'lastModified': '2021-05-20T01:45:21.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'jax', 'bert', 'arxiv:1907.12412', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'nghuyong',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'nghuyong/ernie-2.0-large-en',\n",
       "  'downloads': 21317,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'MilaNLProc/feel-it-italian-sentiment': {'modelId': 'MilaNLProc/feel-it-italian-sentiment',\n",
       "  'sha': '10879b7d7ce86e72b40910165b5d31b192d953ee',\n",
       "  'lastModified': '2021-03-19T09:20:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'camembert',\n",
       "   'text-classification',\n",
       "   'it',\n",
       "   'transformers',\n",
       "   'sentiment',\n",
       "   'Italian',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'MilaNLProc',\n",
       "  'config': {'architectures': ['CamembertForSequenceClassification'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'MilaNLProc/feel-it-italian-sentiment',\n",
       "  'downloads': 21271,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Mi piaci. Ti amo'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'it',\n",
       "   'license': 'mit',\n",
       "   'tags': ['sentiment', 'Italian']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'textattack/roberta-base-SST-2': {'modelId': 'textattack/roberta-base-SST-2',\n",
       "  'sha': 'a029a4679e8a56a958d932d1132d6a4f68803214',\n",
       "  'lastModified': '2021-05-20T22:11:39.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'roberta', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'textattack',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'textattack/roberta-base-SST-2',\n",
       "  'downloads': 21256,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mrm8488/bert-medium-finetuned-squadv2': {'modelId': 'mrm8488/bert-medium-finetuned-squadv2',\n",
       "  'sha': '881ce1995ab82387a14f63cf50c845afb8f6f724',\n",
       "  'lastModified': '2021-05-20T00:25:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'arxiv:1908.08962',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'mrm8488/bert-medium-finetuned-squadv2',\n",
       "  'downloads': 21242,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'thumbnail': None},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'EleutherAI/gpt-neox-20b': {'modelId': 'EleutherAI/gpt-neox-20b',\n",
       "  'sha': '364ae95407723fadd1d47b023c1efb92a4d891c3',\n",
       "  'lastModified': '2022-04-07T22:14:56.000Z',\n",
       "  'tags': ['pytorch', 'gpt_neox', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'EleutherAI',\n",
       "  'config': {'architectures': ['GPTNeoXForCausalLM'],\n",
       "   'model_type': 'gpt_neox'},\n",
       "  'id': 'EleutherAI/gpt-neox-20b',\n",
       "  'downloads': 21233,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 37,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cardiffnlp/twitter-roberta-base-emoji': {'modelId': 'cardiffnlp/twitter-roberta-base-emoji',\n",
       "  'sha': 'e7efb0d4f929fce6b1477405d6f59c526e4272ac',\n",
       "  'lastModified': '2021-05-20T14:59:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'arxiv:2010.12421',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cardiffnlp',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cardiffnlp/twitter-roberta-base-emoji',\n",
       "  'downloads': 21106,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'emanjavacas/MacBERTh': {'modelId': 'emanjavacas/MacBERTh',\n",
       "  'sha': 'd709ce9a3cf02bf27e580e65fa50847edf02f64d',\n",
       "  'lastModified': '2022-01-17T16:02:47.000Z',\n",
       "  'tags': ['pytorch', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'emanjavacas',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'emanjavacas/MacBERTh',\n",
       "  'downloads': 21095,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'rasa/LaBSE': {'modelId': 'rasa/LaBSE',\n",
       "  'sha': 'e615b58364f13c7be81e15ccea2ab27a6c483b76',\n",
       "  'lastModified': '2021-05-20T04:01:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'rasa',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'rasa/LaBSE',\n",
       "  'downloads': 21061,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/m2m100_1.2B': {'modelId': 'facebook/m2m100_1.2B',\n",
       "  'sha': '90301acb1353eb6623e48973520b486612a57439',\n",
       "  'lastModified': '2022-05-26T22:26:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'm2m_100',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'ast',\n",
       "   'az',\n",
       "   'ba',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'br',\n",
       "   'bs',\n",
       "   'ca',\n",
       "   'ceb',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'es',\n",
       "   'et',\n",
       "   'fa',\n",
       "   'ff',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'he',\n",
       "   'hi',\n",
       "   'hr',\n",
       "   'ht',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'id',\n",
       "   'ig',\n",
       "   'ilo',\n",
       "   'is',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'lb',\n",
       "   'lg',\n",
       "   'ln',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'ns',\n",
       "   'oc',\n",
       "   'or',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sd',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'ss',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'th',\n",
       "   'tl',\n",
       "   'tn',\n",
       "   'tr',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'wo',\n",
       "   'xh',\n",
       "   'yi',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'zu',\n",
       "   'arxiv:2010.11125',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['M2M100ForConditionalGeneration'],\n",
       "   'model_type': 'm2m_100'},\n",
       "  'id': 'facebook/m2m100_1.2B',\n",
       "  'downloads': 20984,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'ast',\n",
       "    'az',\n",
       "    'ba',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'br',\n",
       "    'bs',\n",
       "    'ca',\n",
       "    'ceb',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'es',\n",
       "    'et',\n",
       "    'fa',\n",
       "    'ff',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'he',\n",
       "    'hi',\n",
       "    'hr',\n",
       "    'ht',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'id',\n",
       "    'ig',\n",
       "    'ilo',\n",
       "    'is',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'lb',\n",
       "    'lg',\n",
       "    'ln',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'ns',\n",
       "    'oc',\n",
       "    'or',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sd',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'ss',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'th',\n",
       "    'tl',\n",
       "    'tn',\n",
       "    'tr',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'wo',\n",
       "    'xh',\n",
       "    'yi',\n",
       "    'yo',\n",
       "    'zh',\n",
       "    'zu'],\n",
       "   'license': 'mit',\n",
       "   'tags': None},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'shibing624/macbert4csc-base-chinese': {'modelId': 'shibing624/macbert4csc-base-chinese',\n",
       "  'sha': 'a3383e26cc84638663a8681b141a6fdeabf09b72',\n",
       "  'lastModified': '2022-01-29T04:00:02.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'shibing624',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'shibing624/macbert4csc-base-chinese',\n",
       "  'downloads': 20788,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '巴黎是[MASK]国的首都。'}, {'text': '生活的真谛是[MASK]。'}],\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'tags': ['bert', 'pytorch', 'zh'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'tennessejoyce/titlewave-t5-base': {'modelId': 'tennessejoyce/titlewave-t5-base',\n",
       "  'sha': 'fb30007e56801afadefbd4d60cb3b36631dce9e8',\n",
       "  'lastModified': '2021-06-23T14:26:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'summarization',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'tennessejoyce',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 40,\n",
       "     'min_length': 5,\n",
       "     'no_repeat_ngram_size': 1,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '}}},\n",
       "  'id': 'tennessejoyce/titlewave-t5-base',\n",
       "  'downloads': 20682,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Example question body.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'cc-by-4.0',\n",
       "   'pipeline_tag': 'summarization',\n",
       "   'widget': [{'text': 'Example question body.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'MilaNLProc/feel-it-italian-emotion': {'modelId': 'MilaNLProc/feel-it-italian-emotion',\n",
       "  'sha': '7006790559e68195bfd5ab76b43afee2f131347a',\n",
       "  'lastModified': '2021-03-19T09:21:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'camembert',\n",
       "   'text-classification',\n",
       "   'it',\n",
       "   'transformers',\n",
       "   'sentiment',\n",
       "   'emotion',\n",
       "   'Italian',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'MilaNLProc',\n",
       "  'config': {'architectures': ['CamembertForSequenceClassification'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'MilaNLProc/feel-it-italian-emotion',\n",
       "  'downloads': 20677,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Mi piaci. Ti amo'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'it',\n",
       "   'license': 'mit',\n",
       "   'tags': ['sentiment', 'emotion', 'Italian']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/MiniLM-L12-H384-uncased': {'modelId': 'microsoft/MiniLM-L12-H384-uncased',\n",
       "  'sha': '44acabbec0ef496f6dbc93adadea57f376b7c0ec',\n",
       "  'lastModified': '2021-05-19T23:29:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'arxiv:2002.10957',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'text-classification',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'microsoft/MiniLM-L12-H384-uncased',\n",
       "  'downloads': 20577,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'tags': ['text-classification'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'oliverguhr/fullstop-dutch-sonar-punctuation-prediction': {'modelId': 'oliverguhr/fullstop-dutch-sonar-punctuation-prediction',\n",
       "  'sha': 'e680df1f96f17bb3001789b2ba2c78b007c5e3df',\n",
       "  'lastModified': '2022-05-02T13:15:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'token-classification',\n",
       "   'nl',\n",
       "   'dataset:sonar',\n",
       "   'transformers',\n",
       "   'punctuation prediction',\n",
       "   'punctuation',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'oliverguhr',\n",
       "  'config': {'architectures': ['RobertaForTokenClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'oliverguhr/fullstop-dutch-sonar-punctuation-prediction',\n",
       "  'downloads': 20530,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'hervatting van de zitting ik verklaar de zitting van het europees parlement die op vrijdag 17 december werd onderbroken te zijn hervat',\n",
       "    'example_title': 'Euro Parl Sample'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['nl'],\n",
       "   'tags': ['punctuation prediction', 'punctuation'],\n",
       "   'datasets': 'sonar',\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': 'hervatting van de zitting ik verklaar de zitting van het europees parlement die op vrijdag 17 december werd onderbroken te zijn hervat',\n",
       "     'example_title': 'Euro Parl Sample'}],\n",
       "   'metrics': ['f1']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ramsrigouthamg/t5_squad_v1': {'modelId': 'ramsrigouthamg/t5_squad_v1',\n",
       "  'sha': '9555145a47b5794e372b2d0b5a5331cf12e1afb7',\n",
       "  'lastModified': '2021-06-23T13:48:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'ramsrigouthamg',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'ramsrigouthamg/t5_squad_v1',\n",
       "  'downloads': 20513,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'tblard/tf-allocine': {'modelId': 'tblard/tf-allocine',\n",
       "  'sha': '9ba1b0f441ba8a67b775a6f920a2b50cc73c38f7',\n",
       "  'lastModified': '2020-12-11T22:02:40.000Z',\n",
       "  'tags': ['tf', 'camembert', 'text-classification', 'fr', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'tblard',\n",
       "  'config': {'architectures': ['TFCamembertForSequenceClassification'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'tblard/tf-allocine',\n",
       "  'downloads': 20466,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': \"Je t'apprécie beaucoup. Je t'aime.\"}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr'},\n",
       "  'transformersInfo': {'auto_model': 'TF_AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-ko-en': {'modelId': 'Helsinki-NLP/opus-mt-ko-en',\n",
       "  'sha': '8bf548f19accb8fdc96055608840f5a0c194ec8d',\n",
       "  'lastModified': '2020-08-21T14:42:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'ko',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-ko-en',\n",
       "  'downloads': 20299,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ko', 'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'yiyanghkust/finbert-esg': {'modelId': 'yiyanghkust/finbert-esg',\n",
       "  'sha': '26eff66d1942e399ca3ed598894cf0a52915985b',\n",
       "  'lastModified': '2022-06-10T23:19:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'financial-text-analysis',\n",
       "   'esg',\n",
       "   'environmental-social-corporate-governance'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'yiyanghkust',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'yiyanghkust/finbert-esg',\n",
       "  'downloads': 20263,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Rhonda has been volunteering for several years for a variety of charitable community programs. '}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['financial-text-analysis',\n",
       "    'esg',\n",
       "    'environmental-social-corporate-governance'],\n",
       "   'widget': [{'text': 'Rhonda has been volunteering for several years for a variety of charitable community programs. '}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/canine-s': {'modelId': 'google/canine-s',\n",
       "  'sha': '792aaf916e56cb8470fc3162a75f2fa31f96756a',\n",
       "  'lastModified': '2021-08-13T08:23:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'canine',\n",
       "   'feature-extraction',\n",
       "   'multilingual',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:2103.06874',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['CanineModel'], 'model_type': 'canine'},\n",
       "  'id': 'google/canine-s',\n",
       "  'downloads': 20233,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'onlplab/alephbert-base': {'modelId': 'onlplab/alephbert-base',\n",
       "  'sha': '1745fb3ff5137e41e9eb4d6246e0758f63b93e46',\n",
       "  'lastModified': '2022-06-26T09:32:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'he',\n",
       "   'dataset:oscar',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:twitter',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'language model',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'onlplab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'onlplab/alephbert-base',\n",
       "  'downloads': 20113,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['he'],\n",
       "   'tags': ['language model'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['oscar', 'wikipedia', 'twitter']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KoboldAI/fairseq-dense-13B-Shinen': {'modelId': 'KoboldAI/fairseq-dense-13B-Shinen',\n",
       "  'sha': 'c6db29f4afb5ffbdc9e2251ec91914be6fcb4339',\n",
       "  'lastModified': '2022-04-07T09:10:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xglm',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'KoboldAI',\n",
       "  'config': {'architectures': ['XGLMForCausalLM'], 'model_type': 'xglm'},\n",
       "  'id': 'KoboldAI/fairseq-dense-13B-Shinen',\n",
       "  'downloads': 20062,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/quora-distilbert-multilingual': {'modelId': 'sentence-transformers/quora-distilbert-multilingual',\n",
       "  'sha': '43c541d8cdd793eed04a9c2d66c6f971198681da',\n",
       "  'lastModified': '2022-06-15T20:31:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/quora-distilbert-multilingual',\n",
       "  'downloads': 19919,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Davlan/xlm-roberta-base-wikiann-ner': {'modelId': 'Davlan/xlm-roberta-base-wikiann-ner',\n",
       "  'sha': '172d1f30d99d7de340494c6aedd6b6702f6c5021',\n",
       "  'lastModified': '2022-06-27T10:36:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'token-classification',\n",
       "   'ar',\n",
       "   'as',\n",
       "   'bn',\n",
       "   'ca',\n",
       "   'en',\n",
       "   'es',\n",
       "   'eu',\n",
       "   'fr',\n",
       "   'gu',\n",
       "   'hi',\n",
       "   'id',\n",
       "   'ig',\n",
       "   'mr',\n",
       "   'pa',\n",
       "   'pt',\n",
       "   'sw',\n",
       "   'ur',\n",
       "   'vi',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'multilingual',\n",
       "   'dataset:wikiann',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'Davlan',\n",
       "  'config': {'architectures': ['XLMRobertaForTokenClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'Davlan/xlm-roberta-base-wikiann-ner',\n",
       "  'downloads': 19832,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'إسمي محمد وأسكن في برلين'},\n",
       "   {'text': 'إسمي ساره وأسكن في لندن'},\n",
       "   {'text': 'إسمي سامي وأسكن في القدس في فلسطين.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ar',\n",
       "    'as',\n",
       "    'bn',\n",
       "    'ca',\n",
       "    'en',\n",
       "    'es',\n",
       "    'eu',\n",
       "    'fr',\n",
       "    'gu',\n",
       "    'hi',\n",
       "    'id',\n",
       "    'ig',\n",
       "    'mr',\n",
       "    'pa',\n",
       "    'pt',\n",
       "    'sw',\n",
       "    'ur',\n",
       "    'vi',\n",
       "    'yo',\n",
       "    'zh',\n",
       "    'multilingual'],\n",
       "   'datasets': ['wikiann']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'julien-c/dummy-unknown': {'modelId': 'julien-c/dummy-unknown',\n",
       "  'sha': '60b8d3fe22aebb024b573f1cca224db3126d10f3',\n",
       "  'lastModified': '2021-05-20T17:31:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'ci',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'julien-c',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'julien-c/dummy-unknown',\n",
       "  'downloads': 19815,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['ci']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/pegasus-cnn_dailymail': {'modelId': 'google/pegasus-cnn_dailymail',\n",
       "  'sha': '811b08dd23ebf40cbe121d5c49b268150604bb8f',\n",
       "  'lastModified': '2021-03-27T08:09:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'pegasus',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:1912.08777',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['PegasusForConditionalGeneration'],\n",
       "   'model_type': 'pegasus'},\n",
       "  'id': 'google/pegasus-cnn_dailymail',\n",
       "  'downloads': 19632,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'tags': ['summarization']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/rag-token-nq': {'modelId': 'facebook/rag-token-nq',\n",
       "  'sha': 'af32fa164f774a532dfb63c94b2e898e80434643',\n",
       "  'lastModified': '2021-03-12T10:55:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rag',\n",
       "   'en',\n",
       "   'dataset:wiki_dpr',\n",
       "   'arxiv:2005.11401',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['RagTokenForGeneration'], 'model_type': 'rag'},\n",
       "  'id': 'facebook/rag-token-nq',\n",
       "  'downloads': 19595,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wiki_dpr'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png'},\n",
       "  'transformersInfo': {'auto_model': 'RagTokenForGeneration',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/gelectra-base-germanquad': {'modelId': 'deepset/gelectra-base-germanquad',\n",
       "  'sha': '9e4b56ee16728fecb4efd51596384180c8d43a43',\n",
       "  'lastModified': '2022-02-17T14:11:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'electra',\n",
       "   'question-answering',\n",
       "   'de',\n",
       "   'dataset:deepset/germanquad',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['ElectraForQuestionAnswering'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'deepset/gelectra-base-germanquad',\n",
       "  'downloads': 19592,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Wo wohne ich?',\n",
       "    'context': 'Mein Name ist Wolfgang und ich lebe in Berlin'},\n",
       "   {'text': 'Welcher Name wird auch verwendet, um den Amazonas-Regenwald auf Englisch zu beschreiben?',\n",
       "    'context': 'Der Amazonas-Regenwald, auf Englisch auch als Amazonien oder Amazonas-Dschungel bekannt, ist ein feuchter Laubwald, der den größten Teil des Amazonas-Beckens Südamerikas bedeckt. Dieses Becken umfasst 7.000.000 Quadratkilometer (2.700.000 Quadratmeilen), von denen 5.500.000 Quadratkilometer (2.100.000 Quadratmeilen) vom Regenwald bedeckt sind. Diese Region umfasst Gebiete von neun Nationen. Der größte Teil des Waldes befindet sich in Brasilien mit 60% des Regenwaldes, gefolgt von Peru mit 13%, Kolumbien mit 10% und geringen Mengen in Venezuela, Ecuador, Bolivien, Guyana, Suriname und Französisch-Guayana. Staaten oder Abteilungen in vier Nationen enthalten \"Amazonas\" in ihren Namen. Der Amazonas repräsentiert mehr als die Hälfte der verbleibenden Regenwälder des Planeten und umfasst den größten und artenreichsten tropischen Regenwald der Welt mit geschätzten 390 Milliarden Einzelbäumen, die in 16.000 Arten unterteilt sind.'}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de',\n",
       "   'datasets': ['deepset/germanquad'],\n",
       "   'license': 'mit',\n",
       "   'thumbnail': 'https://thumb.tildacdn.com/tild3433-3637-4830-a533-353833613061/-/resize/720x/-/format/webp/germanquad.jpg',\n",
       "   'tags': ['exbert']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'RUCAIBox/mvp': {'modelId': 'RUCAIBox/mvp',\n",
       "  'sha': 'c1d9aeb879f3079101f716f1f6e7109fdd18b4e9',\n",
       "  'lastModified': '2022-06-27T02:27:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mvp',\n",
       "   'en',\n",
       "   'arxiv:2206.12131',\n",
       "   'transformers',\n",
       "   'text-generation',\n",
       "   'text2text-generation',\n",
       "   'summarization',\n",
       "   'conversational',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'RUCAIBox',\n",
       "  'config': {'model_type': 'mvp'},\n",
       "  'id': 'RUCAIBox/mvp',\n",
       "  'downloads': 19543,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': \"Summarize: You may want to stick it to your boss and leave your job, but don't do it if these are your reasons.\",\n",
       "    'example_title': 'Summarization'},\n",
       "   {'text': 'Given the dialog: do you like dance? [SEP] Yes I do. Did you know Bruce Lee was a cha cha dancer?',\n",
       "    'example_title': 'Dialog'},\n",
       "   {'text': 'Describe the following data: Iron Man | instance of | Superhero [SEP] Stan Lee | creator | Iron Man',\n",
       "    'example_title': 'Data-to-text'},\n",
       "   {'text': 'Given the story title: I think all public schools should have a uniform dress code.',\n",
       "    'example_title': 'Story Generation'},\n",
       "   {'text': 'Answer the following question: From which country did Angola achieve independence in 1975?',\n",
       "    'example_title': 'Question Answering'},\n",
       "   {'text': 'Generate the question based on the answer: boxing [X_SEP] A bolo punch is a punch used in martial arts . A hook is a punch in boxing .',\n",
       "    'example_title': 'Question Generaion'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'language': ['en'],\n",
       "   'tags': ['text-generation',\n",
       "    'text2text-generation',\n",
       "    'summarization',\n",
       "    'conversational'],\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'widget': [{'text': \"Summarize: You may want to stick it to your boss and leave your job, but don't do it if these are your reasons.\",\n",
       "     'example_title': 'Summarization'},\n",
       "    {'text': 'Given the dialog: do you like dance? [SEP] Yes I do. Did you know Bruce Lee was a cha cha dancer?',\n",
       "     'example_title': 'Dialog'},\n",
       "    {'text': 'Describe the following data: Iron Man | instance of | Superhero [SEP] Stan Lee | creator | Iron Man',\n",
       "     'example_title': 'Data-to-text'},\n",
       "    {'text': 'Given the story title: I think all public schools should have a uniform dress code.',\n",
       "     'example_title': 'Story Generation'},\n",
       "    {'text': 'Answer the following question: From which country did Angola achieve independence in 1975?',\n",
       "     'example_title': 'Question Answering'},\n",
       "    {'text': 'Generate the question based on the answer: boxing [X_SEP] A bolo punch is a punch used in martial arts . A hook is a punch in boxing .',\n",
       "     'example_title': 'Question Generaion'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'valhalla/t5-base-qa-qg-hl': {'modelId': 'valhalla/t5-base-qa-qg-hl',\n",
       "  'sha': '0286be61d8d9de5650fdd21ed8923a7bc226e704',\n",
       "  'lastModified': '2020-12-11T22:03:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'dataset:squad',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'question-generation',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'valhalla',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 32,\n",
       "     'num_beams': 4,\n",
       "     'prefix': ''}}},\n",
       "  'id': 'valhalla/t5-base-qa-qg-hl',\n",
       "  'downloads': 19317,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'generate question: <hl> 42 <hl> is the answer to life, the universe and everything. </s>'},\n",
       "   {'text': 'question: What is 42 context: 42 is the answer to life, the universe and everything. </s>'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['squad'],\n",
       "   'tags': ['question-generation'],\n",
       "   'widget': [{'text': 'generate question: <hl> 42 <hl> is the answer to life, the universe and everything. </s>'},\n",
       "    {'text': 'question: What is 42 context: 42 is the answer to life, the universe and everything. </s>'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'TalTechNLP/voxlingua107-epaca-tdnn': {'modelId': 'TalTechNLP/voxlingua107-epaca-tdnn',\n",
       "  'sha': 'f35eaf95daf2040cc68ececfd45bbb5e47c44b1c',\n",
       "  'lastModified': '2021-11-04T13:37:27.000Z',\n",
       "  'tags': ['multilingual',\n",
       "   'dataset:VoxLingua107',\n",
       "   'speechbrain',\n",
       "   'audio-classification',\n",
       "   'embeddings',\n",
       "   'Language',\n",
       "   'Identification',\n",
       "   'pytorch',\n",
       "   'ECAPA-TDNN',\n",
       "   'TDNN',\n",
       "   'VoxLingua107',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'audio-classification',\n",
       "  'private': False,\n",
       "  'author': 'TalTechNLP',\n",
       "  'config': {'speechbrain': {'interface': 'EncoderClassifier'}},\n",
       "  'id': 'TalTechNLP/voxlingua107-epaca-tdnn',\n",
       "  'downloads': 19299,\n",
       "  'library_name': 'speechbrain',\n",
       "  'widgetData': [{'example_title': 'English Sample',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/LibriSpeech_61-70968-0000.flac'}],\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'thumbnail': None,\n",
       "   'tags': ['audio-classification',\n",
       "    'speechbrain',\n",
       "    'embeddings',\n",
       "    'Language',\n",
       "    'Identification',\n",
       "    'pytorch',\n",
       "    'ECAPA-TDNN',\n",
       "    'TDNN',\n",
       "    'VoxLingua107'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['VoxLingua107'],\n",
       "   'metrics': ['Accuracy'],\n",
       "   'widget': [{'example_title': 'English Sample',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/LibriSpeech_61-70968-0000.flac'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'obi/deid_bert_i2b2': {'modelId': 'obi/deid_bert_i2b2',\n",
       "  'sha': '8ceb8983df9f0bf75d8c4bac345e157d80b4a5f7',\n",
       "  'lastModified': '2022-02-16T14:41:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'english',\n",
       "   'dataset:I2B2',\n",
       "   'arxiv:1904.03323',\n",
       "   'transformers',\n",
       "   'deidentification',\n",
       "   'medical notes',\n",
       "   'ehr',\n",
       "   'phi',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'obi',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'obi/deid_bert_i2b2',\n",
       "  'downloads': 19288,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Physician Discharge Summary Admit date: 10/12/1982 Discharge date: 10/22/1982 Patient Information Jack Reacher, 54 y.o. male (DOB = 1/21/1928).'},\n",
       "   {'text': 'Home Address: 123 Park Drive, San Diego, CA, 03245. Home Phone: 202-555-0199 (home).'},\n",
       "   {'text': 'Hospital Care Team Service: Orthopedics Inpatient Attending: Roger C Kelly, MD Attending phys phone: (634)743-5135 Discharge Unit: HCS843 Primary Care Physician: Hassan V Kim, MD 512-832-5025.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['english'],\n",
       "   'thumbnail': 'https://www.onebraveidea.org/wp-content/uploads/2019/07/OBI-Logo-Website.png',\n",
       "   'tags': ['deidentification', 'medical notes', 'ehr', 'phi'],\n",
       "   'datasets': ['I2B2'],\n",
       "   'metrics': ['F1', 'Recall', 'AUC'],\n",
       "   'widget': [{'text': 'Physician Discharge Summary Admit date: 10/12/1982 Discharge date: 10/22/1982 Patient Information Jack Reacher, 54 y.o. male (DOB = 1/21/1928).'},\n",
       "    {'text': 'Home Address: 123 Park Drive, San Diego, CA, 03245. Home Phone: 202-555-0199 (home).'},\n",
       "    {'text': 'Hospital Care Team Service: Orthopedics Inpatient Attending: Roger C Kelly, MD Attending phys phone: (634)743-5135 Discharge Unit: HCS843 Primary Care Physician: Hassan V Kim, MD 512-832-5025.'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'monsoon-nlp/hindi-bert': {'modelId': 'monsoon-nlp/hindi-bert',\n",
       "  'sha': '35f95927136cc5ea05db2fb2af1fb1455f5b310e',\n",
       "  'lastModified': '2020-08-26T22:14:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'electra',\n",
       "   'feature-extraction',\n",
       "   'hi',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'monsoon-nlp',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'electra'},\n",
       "  'id': 'monsoon-nlp/hindi-bert',\n",
       "  'downloads': 19243,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'hi'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'wietsedv/bert-base-dutch-cased-finetuned-sentiment': {'modelId': 'wietsedv/bert-base-dutch-cased-finetuned-sentiment',\n",
       "  'sha': 'c9802e7e1da3cbef9e33a3e745d6af1e923a4c8b',\n",
       "  'lastModified': '2022-06-23T14:05:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'wietsedv',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert',\n",
       "   'task_specific_params': None},\n",
       "  'id': 'wietsedv/bert-base-dutch-cased-finetuned-sentiment',\n",
       "  'downloads': 19141,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/chinese-electra-180g-small-ex-discriminator': {'modelId': 'hfl/chinese-electra-180g-small-ex-discriminator',\n",
       "  'sha': '01785e80a5c6601a86bb7cc8d74c20be82646cba',\n",
       "  'lastModified': '2021-03-03T01:25:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'electra',\n",
       "   'zh',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'model_type': 'electra'},\n",
       "  'id': 'hfl/chinese-electra-180g-small-ex-discriminator',\n",
       "  'downloads': 19041,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'xlm-mlm-en-2048': {'modelId': 'xlm-mlm-en-2048',\n",
       "  'sha': '081c89fe33722bbc45b410c43faa09d8eeca889a',\n",
       "  'lastModified': '2021-07-08T09:45:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:cc-by-nc-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['XLMWithLMHeadModel'], 'model_type': 'xlm'},\n",
       "  'id': 'xlm-mlm-en-2048',\n",
       "  'downloads': 19039,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<special1>',\n",
       "  'widgetData': [{'text': 'Paris is the <special1> of France.'},\n",
       "   {'text': 'The goal of life is <special1>.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['exbert'], 'license': 'cc-by-nc-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'albert-large-v2': {'modelId': 'albert-large-v2',\n",
       "  'sha': 'c76159dc6b4d18f16d303451ae64b4f34a7d0d63',\n",
       "  'lastModified': '2021-01-13T15:35:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1909.11942',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'albert-large-v2',\n",
       "  'downloads': 18970,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Filosofas/DialoGPT-medium-PALPATINE': {'modelId': 'Filosofas/DialoGPT-medium-PALPATINE',\n",
       "  'sha': '321b76cbcf40d9c9efa7776ba1eb80be7946211a',\n",
       "  'lastModified': '2022-02-08T11:50:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'Filosofas',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'Filosofas/DialoGPT-medium-PALPATINE',\n",
       "  'downloads': 18934,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'xlm-clm-ende-1024': {'modelId': 'xlm-clm-ende-1024',\n",
       "  'sha': '413761297209305ce09cdcf26596b0adef5cc87d',\n",
       "  'lastModified': '2021-07-08T09:59:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['XLMWithLMHeadModel'], 'model_type': 'xlm'},\n",
       "  'id': 'xlm-clm-ende-1024',\n",
       "  'downloads': 18928,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<special1>',\n",
       "  'widgetData': [{'text': 'Paris is the <special1> of France.'},\n",
       "   {'text': 'The goal of life is <special1>.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nlpaueb/bert-base-greek-uncased-v1': {'modelId': 'nlpaueb/bert-base-greek-uncased-v1',\n",
       "  'sha': 'ec2b8f88dd215b5246f2f850413d5bff90d7540d',\n",
       "  'lastModified': '2022-03-02T16:32:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'pretraining',\n",
       "   'el',\n",
       "   'arxiv:2008.12014',\n",
       "   'transformers',\n",
       "   'fill-mask'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'nlpaueb',\n",
       "  'config': {'architectures': ['BertForPreTraining'], 'model_type': 'bert'},\n",
       "  'id': 'nlpaueb/bert-base-greek-uncased-v1',\n",
       "  'downloads': 18787,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Σήμερα είναι μια [MASK] μέρα.'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'el',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'thumbnail': 'https://github.com/nlpaueb/GreekBERT/raw/master/greek-bert-logo.png',\n",
       "   'widget': [{'text': 'Σήμερα είναι μια [MASK] μέρα.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jean-Baptiste/camembert-ner': {'modelId': 'Jean-Baptiste/camembert-ner',\n",
       "  'sha': 'dbec8489a1c44ecad9da8a9185115bccabd799fe',\n",
       "  'lastModified': '2022-04-04T01:13:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'camembert',\n",
       "   'token-classification',\n",
       "   'fr',\n",
       "   'dataset:Jean-Baptiste/wikiner_fr',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jean-Baptiste',\n",
       "  'config': {'architectures': ['CamembertForTokenClassification'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'Jean-Baptiste/camembert-ner',\n",
       "  'downloads': 18776,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': \"Je m'appelle jean-baptiste et je vis à montréal\"},\n",
       "   {'text': 'george washington est allé à washington'}],\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr',\n",
       "   'datasets': ['Jean-Baptiste/wikiner_fr'],\n",
       "   'widget': [{'text': \"Je m'appelle jean-baptiste et je vis à montréal\"},\n",
       "    {'text': 'george washington est allé à washington'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'setu4993/smaller-LaBSE': {'modelId': 'setu4993/smaller-LaBSE',\n",
       "  'sha': 'abd4e324cf0850b32f1dbf4b08fad6022ab47c0b',\n",
       "  'lastModified': '2021-12-05T06:13:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'ar',\n",
       "   'de',\n",
       "   'en',\n",
       "   'es',\n",
       "   'fr',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'ko',\n",
       "   'nl',\n",
       "   'pl',\n",
       "   'pt',\n",
       "   'ru',\n",
       "   'th',\n",
       "   'tr',\n",
       "   'zh',\n",
       "   'dataset:CommonCrawl',\n",
       "   'dataset:Wikipedia',\n",
       "   'arxiv:2010.05609',\n",
       "   'arxiv:2007.01852',\n",
       "   'transformers',\n",
       "   'sentence_embedding',\n",
       "   'multilingual',\n",
       "   'google',\n",
       "   'sentence-similarity',\n",
       "   'labse',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'setu4993',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'setu4993/smaller-LaBSE',\n",
       "  'downloads': 18751,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ar',\n",
       "    'de',\n",
       "    'en',\n",
       "    'es',\n",
       "    'fr',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'ko',\n",
       "    'nl',\n",
       "    'pl',\n",
       "    'pt',\n",
       "    'ru',\n",
       "    'th',\n",
       "    'tr',\n",
       "    'zh'],\n",
       "   'tags': ['bert',\n",
       "    'sentence_embedding',\n",
       "    'multilingual',\n",
       "    'google',\n",
       "    'sentence-similarity',\n",
       "    'labse'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['CommonCrawl', 'Wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/opt-30b': {'modelId': 'facebook/opt-30b',\n",
       "  'sha': '463007d7da4e87fe962909a027811a8c0b32ede8',\n",
       "  'lastModified': '2022-06-23T16:42:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'opt',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'arxiv:2205.01068',\n",
       "   'arxiv:2005.14165',\n",
       "   'transformers',\n",
       "   'license:other'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['OPTForCausalLM'], 'model_type': 'opt'},\n",
       "  'id': 'facebook/opt-30b',\n",
       "  'downloads': 18728,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 68,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'inference': False,\n",
       "   'tags': ['text-generation', 'opt'],\n",
       "   'license': 'other',\n",
       "   'commercial': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-da-en': {'modelId': 'Helsinki-NLP/opus-mt-da-en',\n",
       "  'sha': '8971eb3839ec41bddd060128b9b83038bb43fd96',\n",
       "  'lastModified': '2021-09-09T21:29:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'da',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-da-en',\n",
       "  'downloads': 18710,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wav2vec2-large-xlsr-53': {'modelId': 'facebook/wav2vec2-large-xlsr-53',\n",
       "  'sha': 'c3f9d884181a224a6ac87bf8885c84d1cff3384f',\n",
       "  'lastModified': '2022-03-18T16:11:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'wav2vec2',\n",
       "   'pretraining',\n",
       "   'multilingual',\n",
       "   'dataset:common_voice',\n",
       "   'arxiv:2006.13979',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Wav2Vec2ForPreTraining'],\n",
       "   'model_type': 'wav2vec2'},\n",
       "  'id': 'facebook/wav2vec2-large-xlsr-53',\n",
       "  'downloads': 18649,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 24,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'datasets': ['common_voice'],\n",
       "   'tags': ['speech'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'indobenchmark/indobert-base-p1': {'modelId': 'indobenchmark/indobert-base-p1',\n",
       "  'sha': 'c2cd0b51ddce6580eb35263b39b0a1e5fb0a39e2',\n",
       "  'lastModified': '2021-05-19T20:22:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'id',\n",
       "   'dataset:Indo4B',\n",
       "   'arxiv:2009.05387',\n",
       "   'transformers',\n",
       "   'indobert',\n",
       "   'indobenchmark',\n",
       "   'indonlu',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'indobenchmark',\n",
       "  'config': {'architectures': ['BertModel'],\n",
       "   'model_type': 'bert',\n",
       "   'task_specific_params': None},\n",
       "  'id': 'indobenchmark/indobert-base-p1',\n",
       "  'downloads': 18610,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'id',\n",
       "   'tags': ['indobert', 'indobenchmark', 'indonlu'],\n",
       "   'license': 'mit',\n",
       "   'inference': False,\n",
       "   'datasets': ['Indo4B']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/deberta-v3-xsmall': {'modelId': 'microsoft/deberta-v3-xsmall',\n",
       "  'sha': 'a4c41b18332abc2f8df65cd0528883373cf09923',\n",
       "  'lastModified': '2022-01-13T18:28:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'deberta-v2',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'arxiv:2111.09543',\n",
       "   'transformers',\n",
       "   'deberta',\n",
       "   'deberta-v3',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'deberta-v2'},\n",
       "  'id': 'microsoft/deberta-v3-xsmall',\n",
       "  'downloads': 18443,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['deberta', 'deberta-v3'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'rinna/japanese-gpt-1b': {'modelId': 'rinna/japanese-gpt-1b',\n",
       "  'sha': 'a3c6e8478d5afa92fe5174b984555e01fe378cd3',\n",
       "  'lastModified': '2022-02-18T04:46:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'ja',\n",
       "   'dataset:cc100',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:c4',\n",
       "   'transformers',\n",
       "   'japanese',\n",
       "   'gpt',\n",
       "   'lm',\n",
       "   'nlp',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'rinna',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'], 'model_type': 'gpt2'},\n",
       "  'id': 'rinna/japanese-gpt-1b',\n",
       "  'downloads': 18277,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '西田幾多郎は、'}],\n",
       "  'likes': 16,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'thumbnail': 'https://github.com/rinnakk/japanese-pretrained-models/blob/master/rinna.png',\n",
       "   'tags': ['ja', 'japanese', 'gpt', 'text-generation', 'lm', 'nlp'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['cc100', 'wikipedia', 'c4'],\n",
       "   'widget': [{'text': '西田幾多郎は、'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ctrl': {'modelId': 'ctrl',\n",
       "  'sha': '07e9f73d6b0d0dda24506e8ad9ad4fb5cf87f4c9',\n",
       "  'lastModified': '2021-09-01T08:15:13.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'ctrl', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'model_type': 'ctrl'},\n",
       "  'id': 'ctrl',\n",
       "  'downloads': 18240,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sshleifer/tiny-dbmdz-bert-large-cased-finetuned-conll03-english': {'modelId': 'sshleifer/tiny-dbmdz-bert-large-cased-finetuned-conll03-english',\n",
       "  'sha': '1dfc430017617e79fd19a2b60b30d6b217f64d28',\n",
       "  'lastModified': '2021-05-20T07:12:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'sshleifer/tiny-dbmdz-bert-large-cased-finetuned-conll03-english',\n",
       "  'downloads': 18137,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/processor_with_lm': {'modelId': 'hf-internal-testing/processor_with_lm',\n",
       "  'sha': '5477bdaf3c221237d0859ebcd6c6aa49e4a7d804',\n",
       "  'lastModified': '2022-01-18T13:19:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'hf-internal-testing/processor_with_lm',\n",
       "  'downloads': 18132,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'moussaKam/mbarthez': {'modelId': 'moussaKam/mbarthez',\n",
       "  'sha': '303813ade47c885979189a169c1449669fdc546f',\n",
       "  'lastModified': '2021-11-15T13:01:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mbart',\n",
       "   'text2text-generation',\n",
       "   'fr',\n",
       "   'arxiv:2010.12321',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'fill-mask',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'moussaKam',\n",
       "  'config': {'architectures': ['MBartForConditionalGeneration'],\n",
       "   'model_type': 'mbart'},\n",
       "  'id': 'moussaKam/mbarthez',\n",
       "  'downloads': 18117,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Paris est la <mask> de la France.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['summarization'],\n",
       "   'language': ['fr'],\n",
       "   'license': 'apache-2.0',\n",
       "   'pipeline_tag': 'fill-mask'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext': {'modelId': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext',\n",
       "  'sha': 'c1f013fb438445557fa71a012928e233a9c5c777',\n",
       "  'lastModified': '2021-05-24T09:59:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:2010.11784',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'cambridgeltl',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'cambridgeltl/SapBERT-from-PubMedBERT-fulltext',\n",
       "  'downloads': 18098,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Maltehb/danish-bert-botxo': {'modelId': 'Maltehb/danish-bert-botxo',\n",
       "  'sha': '565d9bd5ca0872bec0b2d7af7887607a96416c2f',\n",
       "  'lastModified': '2021-11-12T08:34:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'pretraining',\n",
       "   'da',\n",
       "   'dataset:common_crawl',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:dindebat.dk',\n",
       "   'dataset:hestenettet.dk',\n",
       "   'dataset:danish OpenSubtitles',\n",
       "   'transformers',\n",
       "   'danish',\n",
       "   'masked-lm',\n",
       "   'Certainly',\n",
       "   'license:cc-by-4.0',\n",
       "   'fill-mask'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Maltehb',\n",
       "  'config': {'architectures': ['BertForPreTraining'], 'model_type': 'bert'},\n",
       "  'id': 'Maltehb/danish-bert-botxo',\n",
       "  'downloads': 18095,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'København er [MASK] i Danmark.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'da',\n",
       "   'tags': ['danish', 'bert', 'masked-lm', 'Certainly'],\n",
       "   'license': 'cc-by-4.0',\n",
       "   'datasets': ['common_crawl',\n",
       "    'wikipedia',\n",
       "    'dindebat.dk',\n",
       "    'hestenettet.dk',\n",
       "    'danish OpenSubtitles'],\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'widget': [{'text': 'København er [MASK] i Danmark.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wmt19-ru-en': {'modelId': 'facebook/wmt19-ru-en',\n",
       "  'sha': 'd145f3063a3e25e43c04c9aa64de38999b3fb2cd',\n",
       "  'lastModified': '2020-12-11T21:40:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'fsmt',\n",
       "   'text2text-generation',\n",
       "   'ru',\n",
       "   'en',\n",
       "   'dataset:wmt19',\n",
       "   'arxiv:1907.06616',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'wmt19',\n",
       "   'facebook',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['FSMTForConditionalGeneration'],\n",
       "   'model_type': 'fsmt'},\n",
       "  'id': 'facebook/wmt19-ru-en',\n",
       "  'downloads': 17988,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Меня зовут Вольфганг и я живу в Берлине'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru', 'en'],\n",
       "   'tags': ['translation', 'wmt19', 'facebook'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wmt19'],\n",
       "   'metrics': ['bleu'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ckiplab/albert-base-chinese': {'modelId': 'ckiplab/albert-base-chinese',\n",
       "  'sha': 'ed9a51e41fcf0cb4dec5aa3cbd7cdeb40b3e0099',\n",
       "  'lastModified': '2022-05-10T03:28:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'lm-head',\n",
       "   'license:gpl-3.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'ckiplab',\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'ckiplab/albert-base-chinese',\n",
       "  'downloads': 17981,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '巴黎是[MASK]国的首都。'}, {'text': '生活的真谛是[MASK]。'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'thumbnail': 'https://ckip.iis.sinica.edu.tw/files/ckip_logo.png',\n",
       "   'tags': ['pytorch', 'lm-head', 'albert', 'zh'],\n",
       "   'license': 'gpl-3.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flair/ner-german-large': {'modelId': 'flair/ner-german-large',\n",
       "  'sha': 'd8943c40a867161a5a5b7ce91f31adaea1c3a424',\n",
       "  'lastModified': '2021-05-08T15:36:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'de',\n",
       "   'dataset:conll2003',\n",
       "   'arxiv:2011.06993',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/ner-german-large',\n",
       "  'downloads': 17976,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'George Washington ging nach Washington'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': 'de',\n",
       "   'datasets': ['conll2003'],\n",
       "   'widget': [{'text': 'George Washington ging nach Washington'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'allenai/led-large-16384': {'modelId': 'allenai/led-large-16384',\n",
       "  'sha': '04472a9a5d3af2efe700dda11da6063c68cd27a4',\n",
       "  'lastModified': '2021-01-11T14:51:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'led',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:2004.05150',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'architectures': ['LEDForConditionalGeneration'],\n",
       "   'model_type': 'led'},\n",
       "  'id': 'allenai/led-large-16384',\n",
       "  'downloads': 17967,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-hi-en': {'modelId': 'Helsinki-NLP/opus-mt-hi-en',\n",
       "  'sha': '48acb8543e96768bc9fea5b3813d0e46fe1a45f3',\n",
       "  'lastModified': '2021-09-09T22:09:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'hi',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-hi-en',\n",
       "  'downloads': 17928,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wav2vec2-large-lv60': {'modelId': 'facebook/wav2vec2-large-lv60',\n",
       "  'sha': '0cde644b64dac88d8416bec1c92a4099b850ba0b',\n",
       "  'lastModified': '2021-12-28T12:45:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'wav2vec2',\n",
       "   'pretraining',\n",
       "   'en',\n",
       "   'dataset:librispeech_asr',\n",
       "   'arxiv:2006.11477',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Wav2Vec2ForPreTraining'],\n",
       "   'model_type': 'wav2vec2'},\n",
       "  'id': 'facebook/wav2vec2-large-lv60',\n",
       "  'downloads': 17866,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['librispeech_asr'],\n",
       "   'tags': ['speech'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'cardiffnlp/twitter-roberta-base': {'modelId': 'cardiffnlp/twitter-roberta-base',\n",
       "  'sha': 'aafc670a946810bfad6966fcabeb5b37b6388930',\n",
       "  'lastModified': '2021-05-20T15:13:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'arxiv:2010.12421',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'cardiffnlp',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'cardiffnlp/twitter-roberta-base',\n",
       "  'downloads': 17824,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/clip-ViT-B-32-multilingual-v1': {'modelId': 'sentence-transformers/clip-ViT-B-32-multilingual-v1',\n",
       "  'sha': '200b64f20b3cef15ade0d31b1392519a46024087',\n",
       "  'lastModified': '2022-06-15T20:17:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'multilingual',\n",
       "   'arxiv:2004.09813',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/clip-ViT-B-32-multilingual-v1',\n",
       "  'downloads': 17767,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'language': 'multilingual',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/t5-v1_1-xl': {'modelId': 'google/t5-v1_1-xl',\n",
       "  'sha': 'a9e51c46bd6f3893213c51edf9498be6f0426797',\n",
       "  'lastModified': '2020-11-19T19:55:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:c4',\n",
       "   'arxiv:2002.05202',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/t5-v1_1-xl',\n",
       "  'downloads': 17640,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'datasets': ['c4'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'fnlp/bart-base-chinese': {'modelId': 'fnlp/bart-base-chinese',\n",
       "  'sha': 'cac82280f9cafd2e7bb76fc32123717e335397ba',\n",
       "  'lastModified': '2021-10-31T15:05:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'feature-extraction',\n",
       "   'zh',\n",
       "   'arxiv:2109.05729',\n",
       "   'transformers',\n",
       "   'text2text-generation',\n",
       "   'Chinese',\n",
       "   'seq2seq',\n",
       "   'BART'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'fnlp',\n",
       "  'config': {'architectures': ['BartModel'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'length_penalty': 1,\n",
       "     'max_length': 128,\n",
       "     'min_length': 12,\n",
       "     'num_beams': 4},\n",
       "    'summarization_cnn': {'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'num_beams': 4},\n",
       "    'summarization_xsum': {'length_penalty': 1,\n",
       "     'max_length': 62,\n",
       "     'min_length': 11,\n",
       "     'num_beams': 6}}},\n",
       "  'id': 'fnlp/bart-base-chinese',\n",
       "  'downloads': 17598,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['text2text-generation', 'Chinese', 'seq2seq', 'BART'],\n",
       "   'language': 'zh'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KB/bert-base-swedish-cased': {'modelId': 'KB/bert-base-swedish-cased',\n",
       "  'sha': '81c7baa04742a30cb6732c181e678721868cb42e',\n",
       "  'lastModified': '2022-06-07T16:31:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'sv',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'KB',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'KB/bert-base-swedish-cased',\n",
       "  'downloads': 17480,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'sv'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KB/bert-base-swedish-cased-ner': {'modelId': 'KB/bert-base-swedish-cased-ner',\n",
       "  'sha': 'e1c9ae76afa22ce28d2097310ab95312d73e4e3a',\n",
       "  'lastModified': '2022-06-07T16:34:49.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'sv',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'KB',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'KB/bert-base-swedish-cased-ner',\n",
       "  'downloads': 17321,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'sv'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cl-tohoku/bert-large-japanese': {'modelId': 'cl-tohoku/bert-large-japanese',\n",
       "  'sha': '0f5fc4dcd523bed677d208cfa7279c80b1e8e8dc',\n",
       "  'lastModified': '2021-09-23T13:45:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ja',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'cl-tohoku',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'cl-tohoku/bert-large-japanese',\n",
       "  'downloads': 17254,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '東北大学で[MASK]の研究をしています。'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'datasets': ['wikipedia'],\n",
       "   'widget': [{'text': '東北大学で[MASK]の研究をしています。'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sberbank-ai/rugpt3small_based_on_gpt2': {'modelId': 'sberbank-ai/rugpt3small_based_on_gpt2',\n",
       "  'sha': 'f2f7c585b05a16726efe8974586e10b4d5939082',\n",
       "  'lastModified': '2021-09-21T19:30:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'ru',\n",
       "   'transformers',\n",
       "   'PyTorch',\n",
       "   'Transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'sberbank-ai',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'], 'model_type': 'gpt2'},\n",
       "  'id': 'sberbank-ai/rugpt3small_based_on_gpt2',\n",
       "  'downloads': 17176,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Меня зовут Жюльен и'},\n",
       "   {'text': 'Меня зовут Томас и мой основной'},\n",
       "   {'text': 'Однажды'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru'],\n",
       "   'tags': ['PyTorch', 'Transformers'],\n",
       "   'thumbnail': 'https://github.com/sberbank-ai/ru-gpts'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pyannote/overlapped-speech-detection': {'modelId': 'pyannote/overlapped-speech-detection',\n",
       "  'sha': '77439be79bd6ddddaabfe0916c146742265e9614',\n",
       "  'lastModified': '2022-03-23T09:23:32.000Z',\n",
       "  'tags': ['dataset:ami',\n",
       "   'dataset:dihard',\n",
       "   'dataset:voxconverse',\n",
       "   'pyannote-audio',\n",
       "   'pyannote',\n",
       "   'pyannote-audio-pipeline',\n",
       "   'audio',\n",
       "   'voice',\n",
       "   'speech',\n",
       "   'speaker',\n",
       "   'overlapped-speech-detection',\n",
       "   'automatic-speech-recognition',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'pyannote',\n",
       "  'config': None,\n",
       "  'id': 'pyannote/overlapped-speech-detection',\n",
       "  'downloads': 17130,\n",
       "  'library_name': 'pyannote-audio',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['pyannote',\n",
       "    'pyannote-audio',\n",
       "    'pyannote-audio-pipeline',\n",
       "    'audio',\n",
       "    'voice',\n",
       "    'speech',\n",
       "    'speaker',\n",
       "    'overlapped-speech-detection',\n",
       "    'automatic-speech-recognition'],\n",
       "   'datasets': ['ami', 'dihard', 'voxconverse'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'hfl/chinese-macbert-large': {'modelId': 'hfl/chinese-macbert-large',\n",
       "  'sha': '1cf2677c782975600ce58e2961656b1b29eddbae',\n",
       "  'lastModified': '2021-05-19T19:14:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'hfl/chinese-macbert-large',\n",
       "  'downloads': 16989,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '巴黎是[MASK]国的首都。'}, {'text': '生活的真谛是[MASK]。'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'tags': ['bert'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/deit-base-patch16-224': {'modelId': 'facebook/deit-base-patch16-224',\n",
       "  'sha': '7d4fdb12f05852e952cf14805435477bc2e4f99e',\n",
       "  'lastModified': '2022-06-23T07:49:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'vit',\n",
       "   'image-classification',\n",
       "   'dataset:imagenet-1k',\n",
       "   'arxiv:2012.12877',\n",
       "   'arxiv:2006.03677',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-classification',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['ViTForImageClassification'],\n",
       "   'model_type': 'vit'},\n",
       "  'id': 'facebook/deit-base-patch16-224',\n",
       "  'downloads': 16740,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['image-classification'],\n",
       "   'datasets': ['imagenet-1k']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageClassification',\n",
       "   'pipeline_tag': 'image-classification',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'avichr/heBERT_sentiment_analysis': {'modelId': 'avichr/heBERT_sentiment_analysis',\n",
       "  'sha': '022c0d00fc26288c25c0b9f5389d7f0991f93de2',\n",
       "  'lastModified': '2021-12-31T16:08:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'avichr',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'avichr/heBERT_sentiment_analysis',\n",
       "  'downloads': 16654,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/t5-base-lm-adapt': {'modelId': 'google/t5-base-lm-adapt',\n",
       "  'sha': '82aa560c46d415609fa3403f4e94d2c1a90923af',\n",
       "  'lastModified': '2021-11-01T14:01:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:c4',\n",
       "   'arxiv:2002.05202',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   't5-lm-adapt',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/t5-base-lm-adapt',\n",
       "  'downloads': 16589,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['c4'],\n",
       "   'tags': ['t5-lm-adapt'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'climatebert/distilroberta-base-climate-f': {'modelId': 'climatebert/distilroberta-base-climate-f',\n",
       "  'sha': '046b11a4b8c9db2c04a1f170bd48f863e0d2cf47',\n",
       "  'lastModified': '2022-03-09T16:49:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'arxiv:2110.12010',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'climatebert',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'climatebert/distilroberta-base-climate-f',\n",
       "  'downloads': 16577,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/multi-qa-distilbert-cos-v1': {'modelId': 'sentence-transformers/multi-qa-distilbert-cos-v1',\n",
       "  'sha': 'd3aaaee2b4f73c64bae6f6bb4433f8b50d620b4f',\n",
       "  'lastModified': '2021-08-23T18:22:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'distilbert',\n",
       "   'fill-mask',\n",
       "   'sentence-transformers',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertForMaskedLM'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/multi-qa-distilbert-cos-v1',\n",
       "  'downloads': 16329,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/muril-base-cased': {'modelId': 'google/muril-base-cased',\n",
       "  'sha': 'afd9f36c7923d54e97903922ff1b260d091d202f',\n",
       "  'lastModified': '2022-06-10T13:33:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'arxiv:2103.10730',\n",
       "   'arxiv:1810.04805',\n",
       "   'arxiv:1911.02116',\n",
       "   'arxiv:2003.11080',\n",
       "   'arxiv:2009.05166',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'google/muril-base-cased',\n",
       "  'downloads': 16319,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'SpanBERT/spanbert-base-cased': {'modelId': 'SpanBERT/spanbert-base-cased',\n",
       "  'sha': 'b436fe68816aa04256692ce7e27711bf6be15513',\n",
       "  'lastModified': '2021-05-19T11:30:27.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'SpanBERT',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'SpanBERT/spanbert-base-cased',\n",
       "  'downloads': 16313,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'textattack/bert-base-uncased-ag-news': {'modelId': 'textattack/bert-base-uncased-ag-news',\n",
       "  'sha': 'fe417ad660b1657142f66353a184dc0c7e6d2e48',\n",
       "  'lastModified': '2021-05-20T07:40:21.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'textattack',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'textattack/bert-base-uncased-ag-news',\n",
       "  'downloads': 16243,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/sentence_bert': {'modelId': 'deepset/sentence_bert',\n",
       "  'sha': '496b9b39b227f03c4053a9f5fdac1616773b5112',\n",
       "  'lastModified': '2021-05-19T15:34:03.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'transformers', 'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'deepset/sentence_bert',\n",
       "  'downloads': 16098,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'Seznam/small-e-czech': {'modelId': 'Seznam/small-e-czech',\n",
       "  'sha': '0e933d0b8d8dde2b3f5cc76444e221c0f407536c',\n",
       "  'lastModified': '2022-05-11T10:46:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'electra',\n",
       "   'cs',\n",
       "   'arxiv:2003.10555',\n",
       "   'arxiv:2112.01810',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'Seznam',\n",
       "  'config': {'model_type': 'electra'},\n",
       "  'id': 'Seznam/small-e-czech',\n",
       "  'downloads': 15901,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'cs', 'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'ckiplab/bert-base-chinese-ws': {'modelId': 'ckiplab/bert-base-chinese-ws',\n",
       "  'sha': '60c22ced1c0ec221242906e8f9fbdf90fb560b77',\n",
       "  'lastModified': '2022-05-10T03:28:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'license:gpl-3.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'ckiplab',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'ckiplab/bert-base-chinese-ws',\n",
       "  'downloads': 15892,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '我叫沃尔夫冈，我住在柏林。'},\n",
       "   {'text': '我叫萨拉，我住在伦敦。'},\n",
       "   {'text': '我叫克拉拉，我住在加州伯克利。'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'thumbnail': 'https://ckip.iis.sinica.edu.tw/files/ckip_logo.png',\n",
       "   'tags': ['pytorch', 'token-classification', 'bert', 'zh'],\n",
       "   'license': 'gpl-3.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pyannote/voice-activity-detection': {'modelId': 'pyannote/voice-activity-detection',\n",
       "  'sha': '1ba76fd68b766cdff0fb95ace5699757838ea6cb',\n",
       "  'lastModified': '2022-03-23T09:24:05.000Z',\n",
       "  'tags': ['dataset:ami',\n",
       "   'dataset:dihard',\n",
       "   'dataset:voxconverse',\n",
       "   'pyannote-audio',\n",
       "   'pyannote',\n",
       "   'pyannote-audio-pipeline',\n",
       "   'audio',\n",
       "   'voice',\n",
       "   'speech',\n",
       "   'speaker',\n",
       "   'voice-activity-detection',\n",
       "   'automatic-speech-recognition',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'pyannote',\n",
       "  'config': None,\n",
       "  'id': 'pyannote/voice-activity-detection',\n",
       "  'downloads': 15608,\n",
       "  'library_name': 'pyannote-audio',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['pyannote',\n",
       "    'pyannote-audio',\n",
       "    'pyannote-audio-pipeline',\n",
       "    'audio',\n",
       "    'voice',\n",
       "    'speech',\n",
       "    'speaker',\n",
       "    'voice-activity-detection',\n",
       "    'automatic-speech-recognition'],\n",
       "   'datasets': ['ami', 'dihard', 'voxconverse'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'dmis-lab/biobert-base-cased-v1.2': {'modelId': 'dmis-lab/biobert-base-cased-v1.2',\n",
       "  'sha': '67c9c25b46986521ca33df05d8540da1210b3256',\n",
       "  'lastModified': '2021-06-24T02:54:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'dmis-lab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'],\n",
       "   'model_type': 'bert',\n",
       "   'task_specific_params': None},\n",
       "  'id': 'dmis-lab/biobert-base-cased-v1.2',\n",
       "  'downloads': 15570,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mrm8488/bert-tiny-finetuned-sms-spam-detection': {'modelId': 'mrm8488/bert-tiny-finetuned-sms-spam-detection',\n",
       "  'sha': '49541ae4a47b52d4b2e6d5a3a1875c5e35aebeb1',\n",
       "  'lastModified': '2021-05-20T00:40:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:sms_spam',\n",
       "   'transformers',\n",
       "   'sms',\n",
       "   'spam',\n",
       "   'detection'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'mrm8488/bert-tiny-finetuned-sms-spam-detection',\n",
       "  'downloads': 15525,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['sms', 'spam', 'detection'],\n",
       "   'datasets': ['sms_spam'],\n",
       "   'widget': [{'text': 'Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-tr-en': {'modelId': 'Helsinki-NLP/opus-mt-tr-en',\n",
       "  'sha': '3252b40d8b9dead8012364425fd00db1a26abf85',\n",
       "  'lastModified': '2021-09-11T10:49:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'tr',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-tr-en',\n",
       "  'downloads': 15510,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'SZTAKI-HLT/hubert-base-cc': {'modelId': 'SZTAKI-HLT/hubert-base-cc',\n",
       "  'sha': 'f9b2da95ca4080247005d54b81a41bf98c65acb5',\n",
       "  'lastModified': '2021-05-19T11:29:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'hu',\n",
       "   'dataset:common_crawl',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'SZTAKI-HLT',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'SZTAKI-HLT/hubert-base-cc',\n",
       "  'downloads': 15476,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'hu',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['common_crawl', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/paraphrase-albert-small-v2': {'modelId': 'sentence-transformers/paraphrase-albert-small-v2',\n",
       "  'sha': 'e31b7cd6bb1c99f3fae6aa41bc89da1c8306c94f',\n",
       "  'lastModified': '2022-06-21T14:56:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rust',\n",
       "   'albert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['AlbertModel'], 'model_type': 'albert'},\n",
       "  'id': 'sentence-transformers/paraphrase-albert-small-v2',\n",
       "  'downloads': 15398,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dmis-lab/bern2-ner': {'modelId': 'dmis-lab/bern2-ner',\n",
       "  'sha': 'e09c2e59b9e90cdf3f5c55e3923fe2c2034070a9',\n",
       "  'lastModified': '2021-10-27T06:15:12.000Z',\n",
       "  'tags': ['pytorch', 'roberta', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dmis-lab',\n",
       "  'config': {'architectures': ['RoBERTaMultiNER2'], 'model_type': 'roberta'},\n",
       "  'id': 'dmis-lab/bern2-ner',\n",
       "  'downloads': 15396,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'RoBERTaMultiNER2',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'samrawal/bert-base-uncased_clinical-ner': {'modelId': 'samrawal/bert-base-uncased_clinical-ner',\n",
       "  'sha': 'db93d0fda8da893ad16484174f11ebc1ecb00a49',\n",
       "  'lastModified': '2022-05-28T15:56:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'samrawal',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'samrawal/bert-base-uncased_clinical-ner',\n",
       "  'downloads': 15268,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/layoutlmv3-base': {'modelId': 'microsoft/layoutlmv3-base',\n",
       "  'sha': '3ef8cc27ab279a6ef5b2b7841d4ce6e7386b1825',\n",
       "  'lastModified': '2022-05-03T07:30:33.000Z',\n",
       "  'tags': ['pytorch', 'layoutlmv3', 'arxiv:2204.08387', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'layoutlmv3'},\n",
       "  'id': 'microsoft/layoutlmv3-base',\n",
       "  'downloads': 15229,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 15,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'clips/mfaq': {'modelId': 'clips/mfaq',\n",
       "  'sha': 'a49f0160e4d170a3d12e90ace3990fd6a50e35aa',\n",
       "  'lastModified': '2021-10-15T06:21:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'feature-extraction',\n",
       "   'cs',\n",
       "   'da',\n",
       "   'de',\n",
       "   'en',\n",
       "   'es',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'he',\n",
       "   'hr',\n",
       "   'hu',\n",
       "   'id',\n",
       "   'it',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'pl',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sv',\n",
       "   'tr',\n",
       "   'vi',\n",
       "   'dataset:clips/mfaq',\n",
       "   'arxiv:2109.12870',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'clips',\n",
       "  'config': {'architectures': ['XLMRobertaModel'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'clips/mfaq',\n",
       "  'downloads': 15207,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': '<Q>How many models can I host on HuggingFace?',\n",
       "    'sentences': ['<A>All plans come with unlimited private models and datasets.',\n",
       "     '<A>AutoNLP is an automatic way to train and deploy state-of-the-art NLP models, seamlessly integrated with the Hugging Face ecosystem.',\n",
       "     '<A>Based on how much training data and model variants are created, we send you a compute cost and payment link - as low as $10 per job.']}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'language': ['cs',\n",
       "    'da',\n",
       "    'de',\n",
       "    'en',\n",
       "    'es',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'he',\n",
       "    'hr',\n",
       "    'hu',\n",
       "    'id',\n",
       "    'it',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'pl',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sv',\n",
       "    'tr',\n",
       "    'vi'],\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers'],\n",
       "   'datasets': ['clips/mfaq'],\n",
       "   'widget': {'source_sentence': '<Q>How many models can I host on HuggingFace?',\n",
       "    'sentences': ['<A>All plans come with unlimited private models and datasets.',\n",
       "     '<A>AutoNLP is an automatic way to train and deploy state-of-the-art NLP models, seamlessly integrated with the Hugging Face ecosystem.',\n",
       "     '<A>Based on how much training data and model variants are created, we send you a compute cost and payment link - as low as $10 per job.']}},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'PlanTL-GOB-ES/roberta-base-bne': {'modelId': 'PlanTL-GOB-ES/roberta-base-bne',\n",
       "  'sha': 'f4cc2aff5eaa2e1ad5add20a740d8578c833574a',\n",
       "  'lastModified': '2022-04-06T14:40:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'es',\n",
       "   'dataset:bne',\n",
       "   'arxiv:1907.11692',\n",
       "   'arxiv:2107.07253',\n",
       "   'transformers',\n",
       "   'national library of spain',\n",
       "   'spanish',\n",
       "   'bne',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'PlanTL-GOB-ES',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'PlanTL-GOB-ES/roberta-base-bne',\n",
       "  'downloads': 15131,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Este año las campanadas de La Sexta las presentará <mask>.'},\n",
       "   {'text': 'David Broncano es un presentador de La <mask>.'},\n",
       "   {'text': 'Gracias a los datos de la BNE se ha podido <mask> este modelo del lenguaje.'},\n",
       "   {'text': 'Hay base legal dentro del marco <mask> actual.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['es'],\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['national library of spain', 'spanish', 'bne'],\n",
       "   'datasets': ['bne'],\n",
       "   'metrics': ['ppl'],\n",
       "   'widget': [{'text': 'Este año las campanadas de La Sexta las presentará <mask>.'},\n",
       "    {'text': 'David Broncano es un presentador de La <mask>.'},\n",
       "    {'text': 'Gracias a los datos de la BNE se ha podido <mask> este modelo del lenguaje.'},\n",
       "    {'text': 'Hay base legal dentro del marco <mask> actual.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/gbert-base-germandpr-question_encoder': {'modelId': 'deepset/gbert-base-germandpr-question_encoder',\n",
       "  'sha': 'dcbc13b5d22e49f58549a54dc7f30edec6153c39',\n",
       "  'lastModified': '2021-10-21T12:17:20.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'dpr',\n",
       "   'feature-extraction',\n",
       "   'de',\n",
       "   'dataset:deepset/germandpr',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['DPRQuestionEncoder'], 'model_type': 'dpr'},\n",
       "  'id': 'deepset/gbert-base-germandpr-question_encoder',\n",
       "  'downloads': 15058,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de',\n",
       "   'datasets': ['deepset/germandpr'],\n",
       "   'license': 'mit',\n",
       "   'thumbnail': 'https://thumb.tildacdn.com/tild3433-3637-4830-a533-353833613061/-/resize/720x/-/format/webp/germanquad.jpg',\n",
       "   'tags': ['exbert']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/infoxlm-large': {'modelId': 'microsoft/infoxlm-large',\n",
       "  'sha': 'd616d637f0720deda963cebbfc630657d2b7d3ae',\n",
       "  'lastModified': '2021-08-04T11:43:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'arxiv:2007.07834',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['XLMRobertaForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'microsoft/infoxlm-large',\n",
       "  'downloads': 15039,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'megagonlabs/transformers-ud-japanese-electra-base-ginza-510': {'modelId': 'megagonlabs/transformers-ud-japanese-electra-base-ginza-510',\n",
       "  'sha': '8498b019a97bdf693fa138efcf7e118f099e2c4e',\n",
       "  'lastModified': '2021-12-05T12:12:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'electra',\n",
       "   'feature-extraction',\n",
       "   'ja',\n",
       "   'dataset:mC4',\n",
       "   'dataset:UD_Japanese_BCCWJ r2.8',\n",
       "   'dataset:GSK2014-A(2019)',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'PyTorch',\n",
       "   'Transformers',\n",
       "   'spaCy',\n",
       "   'ELECTRA',\n",
       "   'GiNZA',\n",
       "   'mC4',\n",
       "   'UD_Japanese-BCCWJ',\n",
       "   'GSK2014-A',\n",
       "   'MIT',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'megagonlabs',\n",
       "  'config': {'architectures': ['ElectraModel'], 'model_type': 'electra'},\n",
       "  'id': 'megagonlabs/transformers-ud-japanese-electra-base-ginza-510',\n",
       "  'downloads': 15035,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ja'],\n",
       "   'thumbnail': 'https://raw.githubusercontent.com/megagonlabs/ginza/static/docs/images/GiNZA_logo_4c_s.png',\n",
       "   'tags': ['PyTorch',\n",
       "    'Transformers',\n",
       "    'spaCy',\n",
       "    'ELECTRA',\n",
       "    'GiNZA',\n",
       "    'mC4',\n",
       "    'UD_Japanese-BCCWJ',\n",
       "    'GSK2014-A',\n",
       "    'ja',\n",
       "    'MIT'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['mC4', 'UD_Japanese_BCCWJ r2.8', 'GSK2014-A(2019)'],\n",
       "   'metrics': ['UAS', 'LAS', 'UPOS']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'valhalla/distilbart-mnli-12-6': {'modelId': 'valhalla/distilbart-mnli-12-6',\n",
       "  'sha': 'f14383bf830237f338e7d597955f156590fddc2e',\n",
       "  'lastModified': '2021-06-14T10:32:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bart',\n",
       "   'text-classification',\n",
       "   'dataset:mnli',\n",
       "   'transformers',\n",
       "   'distilbart',\n",
       "   'distilbart-mnli',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'valhalla',\n",
       "  'config': {'architectures': ['BartForSequenceClassification'],\n",
       "   'model_type': 'bart'},\n",
       "  'id': 'valhalla/distilbart-mnli-12-6',\n",
       "  'downloads': 14818,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system’s largest world. Konstantin Batygin did not set out to solve one of the solar system’s most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system’s missing “Planet Nine,” spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn’t rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: “Oh! This is how Europa formed.” Europa is one of Jupiter’s four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the Côte d’Azur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system’s formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['mnli'],\n",
       "   'tags': ['distilbart', 'distilbart-mnli'],\n",
       "   'pipeline_tag': 'zero-shot-classification'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'xlnet-large-cased': {'modelId': 'xlnet-large-cased',\n",
       "  'sha': '09792d2c42dfb606155f1bea8873260b23887edd',\n",
       "  'lastModified': '2021-09-16T09:45:20.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlnet',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1906.08237',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['XLNetLMHeadModel'],\n",
       "   'model_type': 'xlnet',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 250}}},\n",
       "  'id': 'xlnet-large-cased',\n",
       "  'downloads': 14799,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'wietsedv/bert-base-dutch-cased': {'modelId': 'wietsedv/bert-base-dutch-cased',\n",
       "  'sha': '2d09de2a6f34a25cb194c39e6281c5fbef317032',\n",
       "  'lastModified': '2021-05-20T09:12:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'wietsedv',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'wietsedv/bert-base-dutch-cased',\n",
       "  'downloads': 14705,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'uclanlp/plbart-base': {'modelId': 'uclanlp/plbart-base',\n",
       "  'sha': 'cf5287241fcff3819f6ade49635dc2d77efee032',\n",
       "  'lastModified': '2021-11-09T17:07:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'plbart',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'uclanlp',\n",
       "  'config': {'architectures': ['PLBartForConditionalGeneration'],\n",
       "   'model_type': 'plbart'},\n",
       "  'id': 'uclanlp/plbart-base',\n",
       "  'downloads': 14682,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KoboldAI/GPT-Neo-2.7B-Shinen': {'modelId': 'KoboldAI/GPT-Neo-2.7B-Shinen',\n",
       "  'sha': '551fbd85138d4f29589ab07000cca813cb8a62ea',\n",
       "  'lastModified': '2022-03-20T18:49:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt_neo',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'KoboldAI',\n",
       "  'config': {'architectures': ['GPTNeoForCausalLM'],\n",
       "   'model_type': 'gpt_neo',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50,\n",
       "     'temperature': 0.9}}},\n",
       "  'id': 'KoboldAI/GPT-Neo-2.7B-Shinen',\n",
       "  'downloads': 14649,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'huawei-noah/TinyBERT_General_4L_312D': {'modelId': 'huawei-noah/TinyBERT_General_4L_312D',\n",
       "  'sha': '34707a33cd59a94ecde241ac209bf35103691b43',\n",
       "  'lastModified': '2021-05-19T20:03:32.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'arxiv:1909.10351', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'huawei-noah',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'huawei-noah/TinyBERT_General_4L_312D',\n",
       "  'downloads': 14565,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'google/electra-large-generator': {'modelId': 'google/electra-large-generator',\n",
       "  'sha': 'bbb1b8938d38e9f5dbfaaecc869320388b4fefe2',\n",
       "  'lastModified': '2021-04-30T07:44:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'electra',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['ElectraForMaskedLM'], 'model_type': 'electra'},\n",
       "  'id': 'google/electra-large-generator',\n",
       "  'downloads': 14513,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cointegrated/rubert-tiny2': {'modelId': 'cointegrated/rubert-tiny2',\n",
       "  'sha': 'a53f0afc4b34c94012191a043c52e2271fa23f27',\n",
       "  'lastModified': '2022-06-30T14:26:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'pretraining',\n",
       "   'ru',\n",
       "   'transformers',\n",
       "   'russian',\n",
       "   'fill-mask',\n",
       "   'embeddings',\n",
       "   'masked-lm',\n",
       "   'tiny',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'cointegrated',\n",
       "  'config': {'architectures': ['BertForPreTraining'], 'model_type': 'bert'},\n",
       "  'id': 'cointegrated/rubert-tiny2',\n",
       "  'downloads': 14443,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Миниатюрная модель для [MASK] разных задач.'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru'],\n",
       "   'tags': ['russian',\n",
       "    'fill-mask',\n",
       "    'pretraining',\n",
       "    'embeddings',\n",
       "    'masked-lm',\n",
       "    'tiny',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity'],\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': 'Миниатюрная модель для [MASK] разных задач.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'castorini/tct_colbert-v2-hnp-msmarco': {'modelId': 'castorini/tct_colbert-v2-hnp-msmarco',\n",
       "  'sha': '3b46a821282996e0ada304e4bcc5d659712972a8',\n",
       "  'lastModified': '2021-08-12T01:05:56.000Z',\n",
       "  'tags': ['pytorch', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'castorini',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'castorini/tct_colbert-v2-hnp-msmarco',\n",
       "  'downloads': 14361,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli': {'modelId': 'jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli',\n",
       "  'sha': '33e5835aad1c32ac8707971141b65c3fc5ff1904',\n",
       "  'lastModified': '2022-02-25T19:07:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'jbetker',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli',\n",
       "  'downloads': 14291,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'unitary/toxic-bert': {'modelId': 'unitary/toxic-bert',\n",
       "  'sha': '5cc53435803a6e6f1ac8e4b243910d3bf26803ff',\n",
       "  'lastModified': '2021-06-07T15:20:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'arxiv:1703.04009',\n",
       "   'arxiv:1905.12516',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'unitary',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'unitary/toxic-bert',\n",
       "  'downloads': 14276,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 15,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/german-gpt2': {'modelId': 'dbmdz/german-gpt2',\n",
       "  'sha': 'f0edef6d975b1338bae533502e1dae74974cb2d2',\n",
       "  'lastModified': '2021-10-22T08:58:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'dbmdz/german-gpt2',\n",
       "  'downloads': 14234,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Heute ist sehr schönes Wetter in'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de',\n",
       "   'widget': [{'text': 'Heute ist sehr schönes Wetter in'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hustvl/yolos-tiny': {'modelId': 'hustvl/yolos-tiny',\n",
       "  'sha': '3686e65df0c914833fc8cbeca745a33b374c499b',\n",
       "  'lastModified': '2022-06-27T08:37:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'yolos',\n",
       "   'object-detection',\n",
       "   'dataset:coco',\n",
       "   'arxiv:2106.00666',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'object-detection',\n",
       "  'private': False,\n",
       "  'author': 'hustvl',\n",
       "  'config': {'architectures': ['YolosForObjectDetection'],\n",
       "   'model_type': 'yolos'},\n",
       "  'id': 'hustvl/yolos-tiny',\n",
       "  'downloads': 14078,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg',\n",
       "    'example_title': 'Savanna'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg',\n",
       "    'example_title': 'Football Match'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg',\n",
       "    'example_title': 'Airport'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['object-detection', 'vision'],\n",
       "   'datasets': ['coco'],\n",
       "   'widget': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg',\n",
       "     'example_title': 'Savanna'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg',\n",
       "     'example_title': 'Football Match'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg',\n",
       "     'example_title': 'Airport'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForObjectDetection',\n",
       "   'pipeline_tag': 'object-detection',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'facebook/detr-resnet-101': {'modelId': 'facebook/detr-resnet-101',\n",
       "  'sha': '1a655091c08729eecf4fc5063c27fa5ea82aeaa3',\n",
       "  'lastModified': '2022-06-27T08:30:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'detr',\n",
       "   'object-detection',\n",
       "   'dataset:coco',\n",
       "   'arxiv:2005.12872',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'object-detection',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['DetrForObjectDetection'],\n",
       "   'model_type': 'detr'},\n",
       "  'id': 'facebook/detr-resnet-101',\n",
       "  'downloads': 13867,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg',\n",
       "    'example_title': 'Savanna'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg',\n",
       "    'example_title': 'Football Match'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg',\n",
       "    'example_title': 'Airport'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['object-detection', 'vision'],\n",
       "   'datasets': ['coco'],\n",
       "   'widget': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/savanna.jpg',\n",
       "     'example_title': 'Savanna'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg',\n",
       "     'example_title': 'Football Match'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/airport.jpg',\n",
       "     'example_title': 'Airport'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForObjectDetection',\n",
       "   'pipeline_tag': 'object-detection',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'google/bigbird-pegasus-large-arxiv': {'modelId': 'google/bigbird-pegasus-large-arxiv',\n",
       "  'sha': '9d87686a36f9c732db7deeae1dcac5fb085d0b90',\n",
       "  'lastModified': '2022-06-29T20:40:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bigbird_pegasus',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:scientific_papers',\n",
       "   'arxiv:2007.14062',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['BigBirdPegasusForConditionalGeneration'],\n",
       "   'model_type': 'bigbird_pegasus'},\n",
       "  'id': 'google/bigbird-pegasus-large-arxiv',\n",
       "  'downloads': 13783,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 7,\n",
       "  'model-index': [{'name': 'google/bigbird-pegasus-large-arxiv',\n",
       "    'results': [{'task': {'type': 'summarization', 'name': 'Summarization'},\n",
       "      'dataset': {'name': 'scientific_papers',\n",
       "       'type': 'scientific_papers',\n",
       "       'config': 'pubmed',\n",
       "       'split': 'test'},\n",
       "      'metrics': [{'name': 'ROUGE-1',\n",
       "        'type': 'rouge',\n",
       "        'value': 36.0276,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-2',\n",
       "        'type': 'rouge',\n",
       "        'value': 13.4166,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-L',\n",
       "        'type': 'rouge',\n",
       "        'value': 21.9612,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-LSUM',\n",
       "        'type': 'rouge',\n",
       "        'value': 29.648,\n",
       "        'verified': True},\n",
       "       {'name': 'loss',\n",
       "        'type': 'loss',\n",
       "        'value': 2.774355173110962,\n",
       "        'verified': True},\n",
       "       {'name': 'meteor', 'type': 'meteor', 'value': 0.2824, 'verified': True},\n",
       "       {'name': 'gen_len',\n",
       "        'type': 'gen_len',\n",
       "        'value': 209.2537,\n",
       "        'verified': True}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['scientific_papers'],\n",
       "   'tags': ['summarization'],\n",
       "   'model-index': [{'name': 'google/bigbird-pegasus-large-arxiv',\n",
       "     'results': [{'task': {'type': 'summarization', 'name': 'Summarization'},\n",
       "       'dataset': {'name': 'scientific_papers',\n",
       "        'type': 'scientific_papers',\n",
       "        'config': 'pubmed',\n",
       "        'split': 'test'},\n",
       "       'metrics': [{'name': 'ROUGE-1',\n",
       "         'type': 'rouge',\n",
       "         'value': 36.0276,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-2',\n",
       "         'type': 'rouge',\n",
       "         'value': 13.4166,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-L',\n",
       "         'type': 'rouge',\n",
       "         'value': 21.9612,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-LSUM',\n",
       "         'type': 'rouge',\n",
       "         'value': 29.648,\n",
       "         'verified': True},\n",
       "        {'name': 'loss',\n",
       "         'type': 'loss',\n",
       "         'value': 2.774355173110962,\n",
       "         'verified': True},\n",
       "        {'name': 'meteor',\n",
       "         'type': 'meteor',\n",
       "         'value': 0.2824,\n",
       "         'verified': True},\n",
       "        {'name': 'gen_len',\n",
       "         'type': 'gen_len',\n",
       "         'value': 209.2537,\n",
       "         'verified': True}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sshleifer/bart-tiny-random': {'modelId': 'sshleifer/bart-tiny-random',\n",
       "  'sha': '69bce9237e4fa10ea015446395ec0108067890cf',\n",
       "  'lastModified': '2021-06-14T07:44:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'max_length': 10,\n",
       "     'min_length': 2,\n",
       "     'num_beams': 1}}},\n",
       "  'id': 'sshleifer/bart-tiny-random',\n",
       "  'downloads': 13764,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'transfo-xl-wt103': {'modelId': 'transfo-xl-wt103',\n",
       "  'sha': 'fe38e218e1949dbfa50ce7a8931019cc8dd20f38',\n",
       "  'lastModified': '2020-12-09T18:29:59.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'transfo-xl', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['TransfoXLLMHeadModel'],\n",
       "   'model_type': 'transfo-xl',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 250}}},\n",
       "  'id': 'transfo-xl-wt103',\n",
       "  'downloads': 13575,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'j-hartmann/sentiment-roberta-large-english-3-classes': {'modelId': 'j-hartmann/sentiment-roberta-large-english-3-classes',\n",
       "  'sha': 'f995433eb6d79d26702ab9335bfde472a9933ee4',\n",
       "  'lastModified': '2022-02-06T12:26:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'sentiment',\n",
       "   'twitter'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'j-hartmann',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'j-hartmann/sentiment-roberta-large-english-3-classes',\n",
       "  'downloads': 13568,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Oh no. This is bad..'},\n",
       "   {'text': 'To be or not to be.'},\n",
       "   {'text': 'Oh Happy Day'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['roberta', 'sentiment', 'twitter'],\n",
       "   'widget': [{'text': 'Oh no. This is bad..'},\n",
       "    {'text': 'To be or not to be.'},\n",
       "    {'text': 'Oh Happy Day'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli': {'modelId': 'MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli',\n",
       "  'sha': '4954f857233ce526f1f1f061cfbecdd23ec1019f',\n",
       "  'lastModified': '2022-02-08T21:39:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'deberta-v2',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:multi_nli',\n",
       "   'dataset:anli',\n",
       "   'dataset:fever',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'MoritzLaurer',\n",
       "  'config': {'architectures': ['DebertaV2ForSequenceClassification'],\n",
       "   'model_type': 'deberta-v2'},\n",
       "  'id': 'MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli',\n",
       "  'downloads': 13477,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system’s largest world. Konstantin Batygin did not set out to solve one of the solar system’s most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system’s missing “Planet Nine,” spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn’t rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: “Oh! This is how Europa formed.” Europa is one of Jupiter’s four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the Côte d’Azur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system’s formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['text-classification', 'zero-shot-classification'],\n",
       "   'metrics': ['accuracy'],\n",
       "   'datasets': ['multi_nli', 'anli', 'fever'],\n",
       "   'pipeline_tag': 'zero-shot-classification'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allenai/PRIMERA': {'modelId': 'allenai/PRIMERA',\n",
       "  'sha': '550385ac1f15be0309fddaa429d72a87ed60aa6b',\n",
       "  'lastModified': '2022-06-25T16:04:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'led',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'architectures': ['LEDForConditionalGeneration'],\n",
       "   'model_type': 'led',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4}}},\n",
       "  'id': 'allenai/PRIMERA',\n",
       "  'downloads': 13474,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'DeepPavlov/distilrubert-base-cased-conversational': {'modelId': 'DeepPavlov/distilrubert-base-cased-conversational',\n",
       "  'sha': '9b4f8c20bdc51934ef2ef586ef9afee85549cccb',\n",
       "  'lastModified': '2022-05-06T11:58:43.000Z',\n",
       "  'tags': ['pytorch', 'distilbert', 'ru', 'arxiv:2205.02340', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'DeepPavlov',\n",
       "  'config': {'model_type': 'distilbert'},\n",
       "  'id': 'DeepPavlov/distilrubert-base-cased-conversational',\n",
       "  'downloads': 13454,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'hf-internal-testing/tiny-random-vit': {'modelId': 'hf-internal-testing/tiny-random-vit',\n",
       "  'sha': '1870c862512fd2c5c46337626d3fec558aa816f3',\n",
       "  'lastModified': '2022-03-02T15:34:35.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'vit', 'image-classification', 'transformers'],\n",
       "  'pipeline_tag': 'image-classification',\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'architectures': ['ViTForImageClassification'],\n",
       "   'model_type': 'vit'},\n",
       "  'id': 'hf-internal-testing/tiny-random-vit',\n",
       "  'downloads': 13444,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageClassification',\n",
       "   'pipeline_tag': 'image-classification',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'HooshvareLab/albert-fa-zwnj-base-v2': {'modelId': 'HooshvareLab/albert-fa-zwnj-base-v2',\n",
       "  'sha': '2315c0d4ddd6bd68f7703c5651978706bf8aff37',\n",
       "  'lastModified': '2021-03-16T16:36:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'fa',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'HooshvareLab',\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'HooshvareLab/albert-fa-zwnj-base-v2',\n",
       "  'downloads': 13281,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'زندگی یک سوال است و این که چگونه [MASK] کنیم پاسخ این سوال!'},\n",
       "   {'text': 'زندگی از مرگ پرسید: چرا همه من را [MASK] دارند اما از تو متنفرند؟'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fa', 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'textattack/albert-base-v2-yelp-polarity': {'modelId': 'textattack/albert-base-v2-yelp-polarity',\n",
       "  'sha': 'bbb5fb3997de43eedb58f7c74b8fbd63c719b5dd',\n",
       "  'lastModified': '2020-07-06T16:37:10.000Z',\n",
       "  'tags': ['pytorch', 'albert', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'textattack',\n",
       "  'config': {'architectures': ['AlbertForSequenceClassification'],\n",
       "   'model_type': 'albert'},\n",
       "  'id': 'textattack/albert-base-v2-yelp-polarity',\n",
       "  'downloads': 13236,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/roberta-large-nli-stsb-mean-tokens': {'modelId': 'sentence-transformers/roberta-large-nli-stsb-mean-tokens',\n",
       "  'sha': '768fca01ac32ae924414f7128af28ea1d9dfcada',\n",
       "  'lastModified': '2022-06-15T20:56:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/roberta-large-nli-stsb-mean-tokens',\n",
       "  'downloads': 13219,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nlpconnect/vit-gpt2-image-captioning': {'modelId': 'nlpconnect/vit-gpt2-image-captioning',\n",
       "  'sha': '27b41be193be4c2dc238990bad1c8d874b272a83',\n",
       "  'lastModified': '2022-07-01T07:38:36.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'vision-encoder-decoder',\n",
       "   'transformers',\n",
       "   'image-to-text',\n",
       "   'image-captioning',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-to-text',\n",
       "  'private': False,\n",
       "  'author': 'nlpconnect',\n",
       "  'config': {'architectures': ['VisionEncoderDecoderModel'],\n",
       "   'model_type': 'vision-encoder-decoder'},\n",
       "  'id': 'nlpconnect/vit-gpt2-image-captioning',\n",
       "  'downloads': 13178,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['image-to-text', 'image-captioning'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Rakib/roberta-base-on-cuad': {'modelId': 'Rakib/roberta-base-on-cuad',\n",
       "  'sha': 'bc6033499692e08cef629b94b5dad636df956b24',\n",
       "  'lastModified': '2021-07-03T18:10:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'Rakib',\n",
       "  'config': {'architectures': ['RobertaForQuestionAnswering'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'Rakib/roberta-base-on-cuad',\n",
       "  'downloads': 13065,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ckiplab/bert-base-chinese': {'modelId': 'ckiplab/bert-base-chinese',\n",
       "  'sha': 'efe27bb4a9373384e0120ffe1cf327714ceb61bf',\n",
       "  'lastModified': '2022-05-10T03:28:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'lm-head',\n",
       "   'license:gpl-3.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'ckiplab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'ckiplab/bert-base-chinese',\n",
       "  'downloads': 13044,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '巴黎是[MASK]国的首都。'}, {'text': '生活的真谛是[MASK]。'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'thumbnail': 'https://ckip.iis.sinica.edu.tw/files/ckip_logo.png',\n",
       "   'tags': ['pytorch', 'lm-head', 'bert', 'zh'],\n",
       "   'license': 'gpl-3.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two': {'modelId': 'Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two',\n",
       "  'sha': '7b1a724a178c639a4b3446c0ff8f13d19be4f471',\n",
       "  'lastModified': '2022-06-24T09:45:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:hatexplain',\n",
       "   'arxiv:2012.10289',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Hate-speech-CNERG',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Hate-speech-CNERG/bert-base-uncased-hatexplain-rationale-two',\n",
       "  'downloads': 13043,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['hatexplain']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/tapas-base-finetuned-wtq': {'modelId': 'google/tapas-base-finetuned-wtq',\n",
       "  'sha': '69ceee21288ad8e3ffd045c02ce9c9ee15424abe',\n",
       "  'lastModified': '2021-11-29T13:02:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'tapas',\n",
       "   'table-question-answering',\n",
       "   'en',\n",
       "   'dataset:wtq',\n",
       "   'arxiv:2004.02349',\n",
       "   'arxiv:2010.00571',\n",
       "   'arxiv:1508.00305',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'table-question-answering',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['TapasForQuestionAnswering'],\n",
       "   'model_type': 'tapas'},\n",
       "  'id': 'google/tapas-base-finetuned-wtq',\n",
       "  'downloads': 12926,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'How many stars does the transformers repository have?',\n",
       "    'table': {'Repository': ['Transformers', 'Datasets', 'Tokenizers'],\n",
       "     'Stars': [36542, 4512, 3934],\n",
       "     'Contributors': [651, 77, 34],\n",
       "     'Programming language': ['Python',\n",
       "      'Python',\n",
       "      'Rust, Python and NodeJS']}}],\n",
       "  'likes': 21,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['tapas'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wtq']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTableQuestionAnswering',\n",
       "   'pipeline_tag': 'table-question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'textattack/bert-base-uncased-yelp-polarity': {'modelId': 'textattack/bert-base-uncased-yelp-polarity',\n",
       "  'sha': 'a4d0a85ea6c1d5bb944dcc12ea5c918863e469a4',\n",
       "  'lastModified': '2021-05-20T07:49:07.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'textattack',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'textattack/bert-base-uncased-yelp-polarity',\n",
       "  'downloads': 12693,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dumitrescustefan/bert-base-romanian-cased-v1': {'modelId': 'dumitrescustefan/bert-base-romanian-cased-v1',\n",
       "  'sha': '9718c77b8a4f402f3d2a9202e9c918f7fdcdcceb',\n",
       "  'lastModified': '2021-11-02T15:25:55.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'ro', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dumitrescustefan',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'dumitrescustefan/bert-base-romanian-cased-v1',\n",
       "  'downloads': 12687,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ro'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es': {'modelId': 'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es',\n",
       "  'sha': 'a8842522e00a16b382c875ecb2da2dd8cf7cf5b6',\n",
       "  'lastModified': '2022-03-30T20:37:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'es',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'mrm8488/distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es',\n",
       "  'downloads': 12598,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '¿Dónde vivo?',\n",
       "    'context': 'Me llamo Wolfgang y vivo en Berlin'},\n",
       "   {'text': '¿Quién inventó el submarino?',\n",
       "    'context': 'Isaac Peral fue un murciano que inventó el submarino'},\n",
       "   {'text': '¿Cuántas personas hablan español?',\n",
       "    'context': 'El español es el segundo idioma más hablado del mundo con más de 442 millones de hablantes'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'es',\n",
       "   'thumbnail': 'https://i.imgur.com/jgBdimh.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'vinai/bertweet-large': {'modelId': 'vinai/bertweet-large',\n",
       "  'sha': '67477168d449ccc8abb725e2123a0d6e44f27f4b',\n",
       "  'lastModified': '2022-06-08T04:43:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'vinai',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'vinai/bertweet-large',\n",
       "  'downloads': 12551,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/xtremedistil-l6-h256-uncased': {'modelId': 'microsoft/xtremedistil-l6-h256-uncased',\n",
       "  'sha': '8d58f0e6e83c1ab87f88d8c556ec537a111e2ee0',\n",
       "  'lastModified': '2021-08-05T17:49:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'arxiv:2106.04563',\n",
       "   'transformers',\n",
       "   'text-classification',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'microsoft/xtremedistil-l6-h256-uncased',\n",
       "  'downloads': 12540,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'tags': ['text-classification'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-en-nl': {'modelId': 'Helsinki-NLP/opus-mt-en-nl',\n",
       "  'sha': 'cd888e7c566d69f91f675d7a12d26613d1c4d826',\n",
       "  'lastModified': '2021-09-09T21:38:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'nl',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-nl',\n",
       "  'downloads': 12503,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'elastic/distilbert-base-uncased-finetuned-conll03-english': {'modelId': 'elastic/distilbert-base-uncased-finetuned-conll03-english',\n",
       "  'sha': '0e98652673725eab6929978aeb28d8dffc614818',\n",
       "  'lastModified': '2022-06-24T09:30:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'distilbert',\n",
       "   'token-classification',\n",
       "   'en',\n",
       "   'dataset:conll2003',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'elastic',\n",
       "  'config': {'architectures': ['DistilBertForTokenClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'elastic/distilbert-base-uncased-finetuned-conll03-english',\n",
       "  'downloads': 12485,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 7,\n",
       "  'model-index': [{'name': 'elastic/distilbert-base-uncased-finetuned-conll03-english',\n",
       "    'results': [{'task': {'type': 'token-classification',\n",
       "       'name': 'Token Classification'},\n",
       "      'dataset': {'name': 'conll2003',\n",
       "       'type': 'conll2003',\n",
       "       'config': 'conll2003',\n",
       "       'split': 'validation'},\n",
       "      'metrics': [{'name': 'Accuracy',\n",
       "        'type': 'accuracy',\n",
       "        'value': 0.9854480753649896,\n",
       "        'verified': True},\n",
       "       {'name': 'Precision',\n",
       "        'type': 'precision',\n",
       "        'value': 0.9880928983228512,\n",
       "        'verified': True},\n",
       "       {'name': 'Recall',\n",
       "        'type': 'recall',\n",
       "        'value': 0.9895677847945542,\n",
       "        'verified': True},\n",
       "       {'name': 'F1',\n",
       "        'type': 'f1',\n",
       "        'value': 0.9888297915932504,\n",
       "        'verified': True},\n",
       "       {'name': 'loss',\n",
       "        'type': 'loss',\n",
       "        'value': 0.06707527488470078,\n",
       "        'verified': True}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['conll2003'],\n",
       "   'model-index': [{'name': 'elastic/distilbert-base-uncased-finetuned-conll03-english',\n",
       "     'results': [{'task': {'type': 'token-classification',\n",
       "        'name': 'Token Classification'},\n",
       "       'dataset': {'name': 'conll2003',\n",
       "        'type': 'conll2003',\n",
       "        'config': 'conll2003',\n",
       "        'split': 'validation'},\n",
       "       'metrics': [{'name': 'Accuracy',\n",
       "         'type': 'accuracy',\n",
       "         'value': 0.9854480753649896,\n",
       "         'verified': True},\n",
       "        {'name': 'Precision',\n",
       "         'type': 'precision',\n",
       "         'value': 0.9880928983228512,\n",
       "         'verified': True},\n",
       "        {'name': 'Recall',\n",
       "         'type': 'recall',\n",
       "         'value': 0.9895677847945542,\n",
       "         'verified': True},\n",
       "        {'name': 'F1',\n",
       "         'type': 'f1',\n",
       "         'value': 0.9888297915932504,\n",
       "         'verified': True},\n",
       "        {'name': 'loss',\n",
       "         'type': 'loss',\n",
       "         'value': 0.06707527488470078,\n",
       "         'verified': True}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pin/senda': {'modelId': 'pin/senda',\n",
       "  'sha': '0ad8d1953f6a3a27a432a20b957d7e1129cdcbbc',\n",
       "  'lastModified': '2021-08-20T11:00:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'da',\n",
       "   'transformers',\n",
       "   'danish',\n",
       "   'sentiment',\n",
       "   'polarity',\n",
       "   'license:cc-by-4.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'pin',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'pin/senda',\n",
       "  'downloads': 12438,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Sikke en dejlig dag det er i dag'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'da',\n",
       "   'tags': ['danish', 'bert', 'sentiment', 'polarity'],\n",
       "   'license': 'cc-by-4.0',\n",
       "   'widget': [{'text': 'Sikke en dejlig dag det er i dag'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/bigbird-pegasus-large-bigpatent': {'modelId': 'google/bigbird-pegasus-large-bigpatent',\n",
       "  'sha': '623321f538339e475269fdf79a258a5a7b796f4c',\n",
       "  'lastModified': '2021-06-03T18:26:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bigbird_pegasus',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:big_patent',\n",
       "   'arxiv:2007.14062',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['BigBirdPegasusForConditionalGeneration'],\n",
       "   'model_type': 'bigbird_pegasus'},\n",
       "  'id': 'google/bigbird-pegasus-large-bigpatent',\n",
       "  'downloads': 12344,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['big_patent'],\n",
       "   'tags': ['summarization']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allenai/longformer-large-4096': {'modelId': 'allenai/longformer-large-4096',\n",
       "  'sha': 'cfa97f5f8c58c219bfea4da030a0259d5dbb28c4',\n",
       "  'lastModified': '2021-03-10T02:31:17.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'longformer', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'model_type': 'longformer'},\n",
       "  'id': 'allenai/longformer-large-4096',\n",
       "  'downloads': 12320,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'openclimatefix/nowcasting_cnn_v2': {'modelId': 'openclimatefix/nowcasting_cnn_v2',\n",
       "  'sha': '5b9c2a9194869afae91651ffdf2ba1de1952d866',\n",
       "  'lastModified': '2022-05-25T10:41:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'transformers',\n",
       "   'nowcasting',\n",
       "   'forecasting',\n",
       "   'timeseries',\n",
       "   'remote-sensing',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'openclimatefix',\n",
       "  'config': {},\n",
       "  'id': 'openclimatefix/nowcasting_cnn_v2',\n",
       "  'downloads': 12186,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'mit',\n",
       "   'tags': ['nowcasting', 'forecasting', 'timeseries', 'remote-sensing']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'Helsinki-NLP/opus-mt-en-ru': {'modelId': 'Helsinki-NLP/opus-mt-en-ru',\n",
       "  'sha': 'b4544f727b37dc7b186fa5d5a99baa74cd2f4128',\n",
       "  'lastModified': '2021-09-09T21:38:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'ru',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-ru',\n",
       "  'downloads': 12174,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'stanfordnlp/stanza-fr': {'modelId': 'stanfordnlp/stanza-fr',\n",
       "  'sha': 'f7ed3c2077c9f65e409975e8f1ba491be850b093',\n",
       "  'lastModified': '2022-05-28T08:08:45.000Z',\n",
       "  'tags': ['fr', 'stanza', 'token-classification', 'license:apache-2.0'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'stanfordnlp',\n",
       "  'config': None,\n",
       "  'id': 'stanfordnlp/stanza-fr',\n",
       "  'downloads': 12113,\n",
       "  'library_name': 'stanza',\n",
       "  'widgetData': [{'text': 'Mon nom est Wolfgang et je vis à Berlin'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['stanza', 'token-classification'],\n",
       "   'library_name': 'stanza',\n",
       "   'language': 'fr',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'google/t5-large-ssm-nq': {'modelId': 'google/t5-large-ssm-nq',\n",
       "  'sha': '000abff88a3e622c77c59e4a83fb985e79c7c6e6',\n",
       "  'lastModified': '2021-06-23T01:35:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:c4',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:natural_questions',\n",
       "   'arxiv:2002.08909',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/t5-large-ssm-nq',\n",
       "  'downloads': 12041,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['c4', 'wikipedia', 'natural_questions'],\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/gbert-base-germandpr-ctx_encoder': {'modelId': 'deepset/gbert-base-germandpr-ctx_encoder',\n",
       "  'sha': '12ac6df80a8a3a3301464a306cd412d01f43c082',\n",
       "  'lastModified': '2021-10-21T12:17:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'dpr',\n",
       "   'de',\n",
       "   'dataset:deepset/germandpr',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['DPRContextEncoder'], 'model_type': 'dpr'},\n",
       "  'id': 'deepset/gbert-base-germandpr-ctx_encoder',\n",
       "  'downloads': 11997,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de',\n",
       "   'datasets': ['deepset/germandpr'],\n",
       "   'license': 'mit',\n",
       "   'thumbnail': 'https://thumb.tildacdn.com/tild3433-3637-4830-a533-353833613061/-/resize/720x/-/format/webp/germanquad.jpg',\n",
       "   'tags': ['exbert']},\n",
       "  'transformersInfo': {'auto_model': 'DPRContextEncoder',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Recognai/bert-base-spanish-wwm-cased-xnli': {'modelId': 'Recognai/bert-base-spanish-wwm-cased-xnli',\n",
       "  'sha': '6219d7e4cc59999010c795ac26d2e014102e24ba',\n",
       "  'lastModified': '2021-10-15T15:55:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'es',\n",
       "   'dataset:xnli',\n",
       "   'transformers',\n",
       "   'zero-shot-classification',\n",
       "   'nli',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'Recognai',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Recognai/bert-base-spanish-wwm-cased-xnli',\n",
       "  'downloads': 11961,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'El autor se perfila, a los 50 años de su muerte, como uno de los grandes de su siglo',\n",
       "    'candidate_labels': 'cultura, sociedad, economia, salud, deportes'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'es',\n",
       "   'tags': ['zero-shot-classification', 'nli', 'pytorch'],\n",
       "   'datasets': ['xnli'],\n",
       "   'license': 'mit',\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'widget': [{'text': 'El autor se perfila, a los 50 años de su muerte, como uno de los grandes de su siglo',\n",
       "     'candidate_labels': 'cultura, sociedad, economia, salud, deportes'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/codebert-base-mlm': {'modelId': 'microsoft/codebert-base-mlm',\n",
       "  'sha': '5c927614b8750b556dcf569cf8a211fbe20f688a',\n",
       "  'lastModified': '2021-05-20T17:47:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'arxiv:2002.08155',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'microsoft/codebert-base-mlm',\n",
       "  'downloads': 11933,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'shahrukhx01/bert-mini-finetune-question-detection': {'modelId': 'shahrukhx01/bert-mini-finetune-question-detection',\n",
       "  'sha': 'c9088454657626f5e370b3a1ec993fdcad81aaf6',\n",
       "  'lastModified': '2021-07-11T14:27:37.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'neural-search-query-classification',\n",
       "   'neural-search'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'shahrukhx01',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'shahrukhx01/bert-mini-finetune-question-detection',\n",
       "  'downloads': 11877,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'keyword query.'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['neural-search-query-classification', 'neural-search'],\n",
       "   'widget': [{'text': 'keyword query.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'albert-xxlarge-v1': {'modelId': 'albert-xxlarge-v1',\n",
       "  'sha': '431c690c9f508b1cbcba8f475974833ac646d41c',\n",
       "  'lastModified': '2021-01-13T15:32:02.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1909.11942',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'albert-xxlarge-v1',\n",
       "  'downloads': 11875,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-base-turkish-uncased': {'modelId': 'dbmdz/bert-base-turkish-uncased',\n",
       "  'sha': '0582a4e05fd7ec5aa6b265d4bc4c81438d951593',\n",
       "  'lastModified': '2021-05-19T15:15:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'tr',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-base-turkish-uncased',\n",
       "  'downloads': 11807,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'tr', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'indolem/indobert-base-uncased': {'modelId': 'indolem/indobert-base-uncased',\n",
       "  'sha': 'b6663c19a819c04798e7a93d681f9bc34ed57b4a',\n",
       "  'lastModified': '2021-09-17T04:06:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'id',\n",
       "   'dataset:220M words (IndoWiki, IndoWC, News)]',\n",
       "   'arxiv:2011.00677',\n",
       "   'transformers',\n",
       "   'indobert',\n",
       "   'indolem',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'indolem',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'indolem/indobert-base-uncased',\n",
       "  'downloads': 11791,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'id',\n",
       "   'tags': ['indobert', 'indolem'],\n",
       "   'license': 'mit',\n",
       "   'inference': False,\n",
       "   'datasets': ['220M words (IndoWiki, IndoWC, News)]']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-wav2vec2': {'modelId': 'hf-internal-testing/tiny-random-wav2vec2',\n",
       "  'sha': '44e60a7cad2409b873242c874476c0c8ce8e98b0',\n",
       "  'lastModified': '2021-10-06T10:02:54.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'wav2vec2', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'model_type': 'wav2vec2'},\n",
       "  'id': 'hf-internal-testing/tiny-random-wav2vec2',\n",
       "  'downloads': 11790,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'beomi/kcbert-base': {'modelId': 'beomi/kcbert-base',\n",
       "  'sha': '1715c455b12198ef178917232788fc682f1f218f',\n",
       "  'lastModified': '2021-05-19T12:29:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'beomi',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'beomi/kcbert-base',\n",
       "  'downloads': 11664,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/chinese-bert-wwm': {'modelId': 'hfl/chinese-bert-wwm',\n",
       "  'sha': 'ab0aa81da273504efc8540aa4d0bbaa3016a1bb5',\n",
       "  'lastModified': '2021-05-19T19:07:49.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'arxiv:1906.08101',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'hfl/chinese-bert-wwm',\n",
       "  'downloads': 11648,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '巴黎是[MASK]国的首都。'}, {'text': '生活的真谛是[MASK]。'}],\n",
       "  'likes': 15,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'vumichien/wav2vec2-large-xlsr-japanese-hiragana': {'modelId': 'vumichien/wav2vec2-large-xlsr-japanese-hiragana',\n",
       "  'sha': '4110cbb24231daf76321af85b829a1baa686d289',\n",
       "  'lastModified': '2021-06-18T11:22:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'ja',\n",
       "   'dataset:common_voice',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'speech',\n",
       "   'xlsr-fine-tuning-week',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'vumichien',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'vumichien/wav2vec2-large-xlsr-japanese-hiragana',\n",
       "  'downloads': 11641,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': [{'name': 'XLSR Wav2Vec2 Japanese Hiragana by Chien Vu',\n",
       "    'results': [{'task': {'name': 'Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Common Voice Japanese',\n",
       "       'type': 'common_voice',\n",
       "       'args': 'ja'},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 24.74},\n",
       "       {'name': 'Test CER', 'type': 'cer', 'value': 10.99}]}]}],\n",
       "  'cardData': {'language': 'ja',\n",
       "   'datasets': ['common_voice'],\n",
       "   'metrics': ['wer'],\n",
       "   'tags': ['audio',\n",
       "    'automatic-speech-recognition',\n",
       "    'speech',\n",
       "    'xlsr-fine-tuning-week'],\n",
       "   'license': 'apache-2.0',\n",
       "   'model-index': [{'name': 'XLSR Wav2Vec2 Japanese Hiragana by Chien Vu',\n",
       "     'results': [{'task': {'name': 'Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Common Voice Japanese',\n",
       "        'type': 'common_voice',\n",
       "        'args': 'ja'},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 24.74},\n",
       "        {'name': 'Test CER', 'type': 'cer', 'value': 10.99}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'allenai/longformer-large-4096-finetuned-triviaqa': {'modelId': 'allenai/longformer-large-4096-finetuned-triviaqa',\n",
       "  'sha': '4a10c0999bd77b29f6fd122663787c770afa197e',\n",
       "  'lastModified': '2021-03-10T02:31:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'longformer',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'architectures': ['LongformerForQuestionAnswering'],\n",
       "   'model_type': 'longformer'},\n",
       "  'id': 'allenai/longformer-large-4096-finetuned-triviaqa',\n",
       "  'downloads': 11598,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-ine-en': {'modelId': 'Helsinki-NLP/opus-mt-ine-en',\n",
       "  'sha': 'bd46b80a8dfef9dcd1a1201bbe2807bdec97ad9b',\n",
       "  'lastModified': '2020-08-21T14:42:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'ca',\n",
       "   'es',\n",
       "   'os',\n",
       "   'ro',\n",
       "   'fy',\n",
       "   'cy',\n",
       "   'sc',\n",
       "   'is',\n",
       "   'yi',\n",
       "   'lb',\n",
       "   'an',\n",
       "   'sq',\n",
       "   'fr',\n",
       "   'ht',\n",
       "   'rm',\n",
       "   'ps',\n",
       "   'af',\n",
       "   'uk',\n",
       "   'sl',\n",
       "   'lt',\n",
       "   'bg',\n",
       "   'be',\n",
       "   'gd',\n",
       "   'si',\n",
       "   'en',\n",
       "   'br',\n",
       "   'mk',\n",
       "   'or',\n",
       "   'mr',\n",
       "   'ru',\n",
       "   'fo',\n",
       "   'co',\n",
       "   'oc',\n",
       "   'pl',\n",
       "   'gl',\n",
       "   'nb',\n",
       "   'bn',\n",
       "   'id',\n",
       "   'hy',\n",
       "   'da',\n",
       "   'gv',\n",
       "   'nl',\n",
       "   'pt',\n",
       "   'hi',\n",
       "   'as',\n",
       "   'kw',\n",
       "   'ga',\n",
       "   'sv',\n",
       "   'gu',\n",
       "   'wa',\n",
       "   'lv',\n",
       "   'el',\n",
       "   'it',\n",
       "   'hr',\n",
       "   'ur',\n",
       "   'nn',\n",
       "   'de',\n",
       "   'cs',\n",
       "   'ine',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-ine-en',\n",
       "  'downloads': 11557,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ca',\n",
       "    'es',\n",
       "    'os',\n",
       "    'ro',\n",
       "    'fy',\n",
       "    'cy',\n",
       "    'sc',\n",
       "    'is',\n",
       "    'yi',\n",
       "    'lb',\n",
       "    'an',\n",
       "    'sq',\n",
       "    'fr',\n",
       "    'ht',\n",
       "    'rm',\n",
       "    'ps',\n",
       "    'af',\n",
       "    'uk',\n",
       "    'sl',\n",
       "    'lt',\n",
       "    'bg',\n",
       "    'be',\n",
       "    'gd',\n",
       "    'si',\n",
       "    'en',\n",
       "    'br',\n",
       "    'mk',\n",
       "    'or',\n",
       "    'mr',\n",
       "    'ru',\n",
       "    'fo',\n",
       "    'co',\n",
       "    'oc',\n",
       "    'pl',\n",
       "    'gl',\n",
       "    'nb',\n",
       "    'bn',\n",
       "    'id',\n",
       "    'hy',\n",
       "    'da',\n",
       "    'gv',\n",
       "    'nl',\n",
       "    'pt',\n",
       "    'hi',\n",
       "    'as',\n",
       "    'kw',\n",
       "    'ga',\n",
       "    'sv',\n",
       "    'gu',\n",
       "    'wa',\n",
       "    'lv',\n",
       "    'el',\n",
       "    'it',\n",
       "    'hr',\n",
       "    'ur',\n",
       "    'nn',\n",
       "    'de',\n",
       "    'cs',\n",
       "    'ine'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/electra-large-discriminator-finetuned-conll03-english': {'modelId': 'dbmdz/electra-large-discriminator-finetuned-conll03-english',\n",
       "  'sha': '7c0a483cbc7c8c27759f5fc38fe0261ce6bda31e',\n",
       "  'lastModified': '2020-12-09T18:30:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'electra',\n",
       "   'token-classification',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'architectures': ['ElectraForTokenClassification'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'dbmdz/electra-large-discriminator-finetuned-conll03-english',\n",
       "  'downloads': 11449,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/xlm-r-bert-base-nli-stsb-mean-tokens': {'modelId': 'sentence-transformers/xlm-r-bert-base-nli-stsb-mean-tokens',\n",
       "  'sha': '9c71022cc69f395526255204c7af8bea1cc252d8',\n",
       "  'lastModified': '2022-06-15T20:21:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['XLMRobertaModel'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'sentence-transformers/xlm-r-bert-base-nli-stsb-mean-tokens',\n",
       "  'downloads': 11407,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cointegrated/roberta-large-cola-krishna2020': {'modelId': 'cointegrated/roberta-large-cola-krishna2020',\n",
       "  'sha': '8386814a366a824280df5690a810fe038d7a270b',\n",
       "  'lastModified': '2021-11-11T05:13:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'arxiv:2010.05700',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cointegrated',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cointegrated/roberta-large-cola-krishna2020',\n",
       "  'downloads': 11400,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Salesforce/codet5-base': {'modelId': 'Salesforce/codet5-base',\n",
       "  'sha': '4078456db09ba972a3532827a0b5df4da172323c',\n",
       "  'lastModified': '2021-11-23T09:53:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'dataset:code_search_net',\n",
       "   'arxiv:2109.00859',\n",
       "   'arxiv:1909.09436',\n",
       "   'transformers',\n",
       "   'codet5',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'Salesforce',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'Salesforce/codet5-base',\n",
       "  'downloads': 11383,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 22,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['codet5'],\n",
       "   'datasets': ['code_search_net'],\n",
       "   'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-bat-en': {'modelId': 'Helsinki-NLP/opus-mt-bat-en',\n",
       "  'sha': '760a39c69d5158182c320e47986306e24e96fa1f',\n",
       "  'lastModified': '2021-01-18T07:48:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'bat',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-bat-en',\n",
       "  'downloads': 11353,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['lt', 'lv', 'bat', 'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/roberta-base-nli-stsb-mean-tokens': {'modelId': 'sentence-transformers/roberta-base-nli-stsb-mean-tokens',\n",
       "  'sha': '903ef0c8897802c3209d82aa46b1c897ac56cf28',\n",
       "  'lastModified': '2022-06-15T20:49:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/roberta-base-nli-stsb-mean-tokens',\n",
       "  'downloads': 11323,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'fnlp/cpt-large': {'modelId': 'fnlp/cpt-large',\n",
       "  'sha': 'd3f5c8f08d530c93c96b85be27bd79efea80052d',\n",
       "  'lastModified': '2021-10-29T07:11:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'feature-extraction',\n",
       "   'zh',\n",
       "   'arxiv:2109.05729',\n",
       "   'transformers',\n",
       "   'fill-mask',\n",
       "   'text2text-generation',\n",
       "   'text-classification',\n",
       "   'Summarization',\n",
       "   'Chinese',\n",
       "   'CPT',\n",
       "   'BART',\n",
       "   'BERT',\n",
       "   'seq2seq'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'fnlp',\n",
       "  'config': {'architectures': ['BartModel'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'length_penalty': 1,\n",
       "     'max_length': 128,\n",
       "     'min_length': 12,\n",
       "     'num_beams': 4},\n",
       "    'summarization_cnn': {'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'num_beams': 4},\n",
       "    'summarization_xsum': {'length_penalty': 1,\n",
       "     'max_length': 62,\n",
       "     'min_length': 11,\n",
       "     'num_beams': 6}}},\n",
       "  'id': 'fnlp/cpt-large',\n",
       "  'downloads': 11295,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': '我喜欢你。 我爱你'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['fill-mask',\n",
       "    'text2text-generation',\n",
       "    'fill-mask',\n",
       "    'text-classification',\n",
       "    'Summarization',\n",
       "    'Chinese',\n",
       "    'CPT',\n",
       "    'BART',\n",
       "    'BERT',\n",
       "    'seq2seq'],\n",
       "   'language': 'zh'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KBLab/bert-base-swedish-cased': {'modelId': 'KBLab/bert-base-swedish-cased',\n",
       "  'sha': '8feda44cdf94516823ee4803c6ef1037aacd224e',\n",
       "  'lastModified': '2021-05-18T21:23:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'sv',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'KBLab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'KBLab/bert-base-swedish-cased',\n",
       "  'downloads': 11294,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'sv'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'GanjinZero/UMLSBert_ENG': {'modelId': 'GanjinZero/UMLSBert_ENG',\n",
       "  'sha': '1e4841546c6384cefa47192146a7bd368d509849',\n",
       "  'lastModified': '2022-04-27T08:18:37.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'biomedical',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'GanjinZero',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'GanjinZero/UMLSBert_ENG',\n",
       "  'downloads': 11260,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['bert', 'biomedical']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'asafaya/bert-base-arabic': {'modelId': 'asafaya/bert-base-arabic',\n",
       "  'sha': 'b61c328d130eabc1bb1b3b4d1448410a961da888',\n",
       "  'lastModified': '2021-05-19T00:05:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ar',\n",
       "   'dataset:oscar',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'asafaya',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'asafaya/bert-base-arabic',\n",
       "  'downloads': 11258,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'باريس [MASK] فرنسا.'},\n",
       "   {'text': 'فلسفة الحياة هي [MASK].'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ar', 'datasets': ['oscar', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hiiamsid/sentence_similarity_spanish_es': {'modelId': 'hiiamsid/sentence_similarity_spanish_es',\n",
       "  'sha': '2817cf8566982a08b43bc4d6f74924010bc56f65',\n",
       "  'lastModified': '2021-10-18T03:52:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'es',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'hiiamsid',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'hiiamsid/sentence_similarity_spanish_es',\n",
       "  'downloads': 11243,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'Esa es una persona feliz',\n",
       "    'sentences': ['Ese es un perro feliz',\n",
       "     'Esa es una persona muy feliz',\n",
       "     'Hoy es un día soleado']}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'language': ['es'],\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/wavlm-large': {'modelId': 'microsoft/wavlm-large',\n",
       "  'sha': 'c1423ed94bb01d80a3f5ce5bc39f6026a0f4828c',\n",
       "  'lastModified': '2022-02-02T21:21:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wavlm',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'arxiv:1912.07875',\n",
       "   'arxiv:2106.06909',\n",
       "   'arxiv:2101.00390',\n",
       "   'arxiv:2110.13900',\n",
       "   'transformers',\n",
       "   'speech'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['WavLMModel'], 'model_type': 'wavlm'},\n",
       "  'id': 'microsoft/wavlm-large',\n",
       "  'downloads': 11207,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'], 'tags': ['speech'], 'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'iarfmoose/bert-base-cased-qa-evaluator': {'modelId': 'iarfmoose/bert-base-cased-qa-evaluator',\n",
       "  'sha': 'edfb0e29d78453325a95d9a61d4d26d3598e402b',\n",
       "  'lastModified': '2021-05-19T20:15:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'iarfmoose',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'iarfmoose/bert-base-cased-qa-evaluator',\n",
       "  'downloads': 11040,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'epwalsh/bert-xsmall-dummy': {'modelId': 'epwalsh/bert-xsmall-dummy',\n",
       "  'sha': 'd36cc494a54ac76cac8c237866fe8ce540c879a6',\n",
       "  'lastModified': '2021-05-19T16:30:53.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'epwalsh',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'epwalsh/bert-xsmall-dummy',\n",
       "  'downloads': 11027,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/stsb-roberta-base': {'modelId': 'sentence-transformers/stsb-roberta-base',\n",
       "  'sha': '20afd582a7bbc12014f37486dbef9b0c990f91bd',\n",
       "  'lastModified': '2022-06-15T20:36:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/stsb-roberta-base',\n",
       "  'downloads': 10954,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'vinai/bertweet-covid19-base-uncased': {'modelId': 'vinai/bertweet-covid19-base-uncased',\n",
       "  'sha': 'fd00afc23cbc3c3dba662f913d549453f91cb4d4',\n",
       "  'lastModified': '2022-06-08T04:41:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'vinai',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'vinai/bertweet-covid19-base-uncased',\n",
       "  'downloads': 10913,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sonoisa/t5-base-japanese': {'modelId': 'sonoisa/t5-base-japanese',\n",
       "  'sha': 'c4ef50c4b768cbbb13848677f88989d0020fddc2',\n",
       "  'lastModified': '2021-10-04T01:31:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'feature-extraction',\n",
       "   'ja',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:oscar',\n",
       "   'dataset:cc100',\n",
       "   'transformers',\n",
       "   'text2text-generation',\n",
       "   'seq2seq',\n",
       "   'license:cc-by-sa-4.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'sonoisa',\n",
       "  'config': {'architectures': ['T5Model'], 'model_type': 't5'},\n",
       "  'id': 'sonoisa/t5-base-japanese',\n",
       "  'downloads': 10839,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'tags': ['t5', 'text2text-generation', 'seq2seq'],\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'datasets': ['wikipedia', 'oscar', 'cc100']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-uk-ru': {'modelId': 'Helsinki-NLP/opus-mt-uk-ru',\n",
       "  'sha': '0dbf7d9872d43f0599f41259b927e012f84b87fe',\n",
       "  'lastModified': '2020-08-21T14:42:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'uk',\n",
       "   'ru',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-uk-ru',\n",
       "  'downloads': 10833,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Мене звати Вольфґанґ і я живу в Берліні.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['uk', 'ru'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'eugenesiow/bart-paraphrase': {'modelId': 'eugenesiow/bart-paraphrase',\n",
       "  'sha': '561b9d9631d608b8c63c01ecb64b5f030cabdd73',\n",
       "  'lastModified': '2021-09-13T10:02:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:quora',\n",
       "   'dataset:paws',\n",
       "   'arxiv:1910.13461',\n",
       "   'transformers',\n",
       "   'paraphrase',\n",
       "   'seq2seq',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'eugenesiow',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'length_penalty': 1,\n",
       "     'max_length': 128,\n",
       "     'min_length': 12,\n",
       "     'num_beams': 4},\n",
       "    'summarization_cnn': {'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'num_beams': 4},\n",
       "    'summarization_xsum': {'length_penalty': 1,\n",
       "     'max_length': 62,\n",
       "     'min_length': 11,\n",
       "     'num_beams': 6}}},\n",
       "  'id': 'eugenesiow/bart-paraphrase',\n",
       "  'downloads': 10822,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['transformers', 'bart', 'paraphrase', 'seq2seq'],\n",
       "   'datasets': ['quora', 'paws']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'JorisCos/ConvTasNet_Libri2Mix_sepnoisy_16k': {'modelId': 'JorisCos/ConvTasNet_Libri2Mix_sepnoisy_16k',\n",
       "  'sha': '98b76c842d1fae9868f74b331b298a92eee3c12e',\n",
       "  'lastModified': '2021-09-23T15:48:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'dataset:Libri2Mix',\n",
       "   'dataset:sep_noisy',\n",
       "   'asteroid',\n",
       "   'audio',\n",
       "   'ConvTasNet',\n",
       "   'audio-to-audio',\n",
       "   'license:cc-by-sa-4.0'],\n",
       "  'pipeline_tag': 'audio-to-audio',\n",
       "  'private': False,\n",
       "  'author': 'JorisCos',\n",
       "  'config': None,\n",
       "  'id': 'JorisCos/ConvTasNet_Libri2Mix_sepnoisy_16k',\n",
       "  'downloads': 10805,\n",
       "  'library_name': 'asteroid',\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['asteroid', 'audio', 'ConvTasNet', 'audio-to-audio'],\n",
       "   'datasets': ['Libri2Mix', 'sep_noisy'],\n",
       "   'license': 'cc-by-sa-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'Salesforce/codet5-small': {'modelId': 'Salesforce/codet5-small',\n",
       "  'sha': 'a642dc934e5475185369d09ac07091dfe72a31fc',\n",
       "  'lastModified': '2021-11-23T09:45:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'dataset:code_search_net',\n",
       "   'arxiv:2109.00859',\n",
       "   'arxiv:1909.09436',\n",
       "   'transformers',\n",
       "   'codet5',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'Salesforce',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'Salesforce/codet5-small',\n",
       "  'downloads': 10803,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['codet5'],\n",
       "   'datasets': ['code_search_net'],\n",
       "   'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'JorisCos/ConvTasNet_Libri2Mix_sepclean_16k': {'modelId': 'JorisCos/ConvTasNet_Libri2Mix_sepclean_16k',\n",
       "  'sha': 'e1ef95ab7a037950f3a606b9a56760cf94701d3d',\n",
       "  'lastModified': '2021-09-23T15:48:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'dataset:Libri2Mix',\n",
       "   'dataset:sep_clean',\n",
       "   'asteroid',\n",
       "   'audio',\n",
       "   'ConvTasNet',\n",
       "   'audio-to-audio',\n",
       "   'license:cc-by-sa-4.0'],\n",
       "  'pipeline_tag': 'audio-to-audio',\n",
       "  'private': False,\n",
       "  'author': 'JorisCos',\n",
       "  'config': None,\n",
       "  'id': 'JorisCos/ConvTasNet_Libri2Mix_sepclean_16k',\n",
       "  'downloads': 10757,\n",
       "  'library_name': 'asteroid',\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['asteroid', 'audio', 'ConvTasNet', 'audio-to-audio'],\n",
       "   'datasets': ['Libri2Mix', 'sep_clean'],\n",
       "   'license': 'cc-by-sa-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/msmarco-distilbert-base-tas-b': {'modelId': 'sentence-transformers/msmarco-distilbert-base-tas-b',\n",
       "  'sha': '1de916fb3ec96493f57c9f0349a0d9e338ed64c2',\n",
       "  'lastModified': '2022-06-15T21:37:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/msmarco-distilbert-base-tas-b',\n",
       "  'downloads': 10652,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/tinyroberta-squad2': {'modelId': 'deepset/tinyroberta-squad2',\n",
       "  'sha': '45149c89ea8692c8441820a3b11deb7adaaa2967',\n",
       "  'lastModified': '2022-05-27T08:43:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'dataset:squad_v2',\n",
       "   'arxiv:1909.10351',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['RobertaForQuestionAnswering'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'deepset/tinyroberta-squad2',\n",
       "  'downloads': 10560,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['squad_v2'],\n",
       "   'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'princeton-nlp/sup-simcse-roberta-base': {'modelId': 'princeton-nlp/sup-simcse-roberta-base',\n",
       "  'sha': '4bf73c6b5df517f74188c5e9ec159b2208c89c08',\n",
       "  'lastModified': '2021-05-20T19:33:45.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'roberta', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'princeton-nlp',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'princeton-nlp/sup-simcse-roberta-base',\n",
       "  'downloads': 10523,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/prophetnet-large-uncased': {'modelId': 'microsoft/prophetnet-large-uncased',\n",
       "  'sha': 'f8218576b32128d7623ad24f3f25dce10f3d1b01',\n",
       "  'lastModified': '2021-03-04T20:24:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'prophetnet',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:2001.04063',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['ProphetNetForConditionalGeneration'],\n",
       "   'model_type': 'prophetnet',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4}}},\n",
       "  'id': 'microsoft/prophetnet-large-uncased',\n",
       "  'downloads': 10505,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/opt-2.7b': {'modelId': 'facebook/opt-2.7b',\n",
       "  'sha': 'c9c15109b9dac40871c063892227d45b85cb3952',\n",
       "  'lastModified': '2022-06-22T09:54:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'opt',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'arxiv:2205.01068',\n",
       "   'arxiv:2005.14165',\n",
       "   'transformers',\n",
       "   'license:other'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['OPTForCausalLM'], 'model_type': 'opt'},\n",
       "  'id': 'facebook/opt-2.7b',\n",
       "  'downloads': 10435,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'inference': False,\n",
       "   'tags': ['text-generation', 'opt'],\n",
       "   'license': 'other',\n",
       "   'commercial': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'textattack/bert-base-uncased-imdb': {'modelId': 'textattack/bert-base-uncased-imdb',\n",
       "  'sha': 'c70b9f391af2067f7eff69a03940218bba9b8d39',\n",
       "  'lastModified': '2021-05-20T07:42:02.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'textattack',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'textattack/bert-base-uncased-imdb',\n",
       "  'downloads': 10431,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sonoisa/sentence-bert-base-ja-mean-tokens': {'modelId': 'sonoisa/sentence-bert-base-ja-mean-tokens',\n",
       "  'sha': 'c5db458007569cea1374d4b5766193832c3fc285',\n",
       "  'lastModified': '2021-12-14T11:43:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'fill-mask',\n",
       "   'ja',\n",
       "   'sentence-transformers',\n",
       "   'sentence-bert',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity',\n",
       "   'license:cc-by-sa-4.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'sonoisa',\n",
       "  'config': {'architectures': ['BertForMaskedLM']},\n",
       "  'id': 'sonoisa/sentence-bert-base-ja-mean-tokens',\n",
       "  'downloads': 10417,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'sentence-bert',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask'}},\n",
       " 'sberbank-ai/ruT5-base': {'modelId': 'sberbank-ai/ruT5-base',\n",
       "  'sha': '8940daf014ad31b5b619cb07429dd50c884882f1',\n",
       "  'lastModified': '2021-09-21T19:41:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'ru',\n",
       "   'transformers',\n",
       "   'PyTorch',\n",
       "   'Transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'sberbank-ai',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'sberbank-ai/ruT5-base',\n",
       "  'downloads': 10415,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru'],\n",
       "   'tags': ['PyTorch', 'Transformers'],\n",
       "   'thumbnail': 'https://github.com/sberbank-ai/model-zoo'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KoboldAI/GPT-Neo-2.7B-Horni': {'modelId': 'KoboldAI/GPT-Neo-2.7B-Horni',\n",
       "  'sha': '6024c30cd1984b12fa35c50e1490c73e42cf4823',\n",
       "  'lastModified': '2021-12-30T11:43:31.000Z',\n",
       "  'tags': ['pytorch', 'gpt_neo', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'KoboldAI',\n",
       "  'config': {'architectures': ['GPTNeoForCausalLM'],\n",
       "   'model_type': 'gpt_neo',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50,\n",
       "     'temperature': 0.9}}},\n",
       "  'id': 'KoboldAI/GPT-Neo-2.7B-Horni',\n",
       "  'downloads': 10397,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sshleifer/distilbart-cnn-6-6': {'modelId': 'sshleifer/distilbart-cnn-6-6',\n",
       "  'sha': 'd2fde4ca965ba893255479612e4b801aa6500029',\n",
       "  'lastModified': '2021-06-14T07:53:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:cnn_dailymail',\n",
       "   'dataset:xsum',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4}}},\n",
       "  'id': 'sshleifer/distilbart-cnn-6-6',\n",
       "  'downloads': 10392,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['summarization'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['cnn_dailymail', 'xsum'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/distilbart_medium.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bigscience/T0pp': {'modelId': 'bigscience/T0pp',\n",
       "  'sha': 'cd850304a7a82b39522a4a9b36f55c287ed72995',\n",
       "  'lastModified': '2022-06-21T01:20:49.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:bigscience/P3',\n",
       "   'arxiv:2110.08207',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'bigscience',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'bigscience/T0pp',\n",
       "  'downloads': 10365,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': \"A is the son's of B's uncle. What is the family relationship between A and B?\"},\n",
       "   {'text': 'Reorder the words in this sentence: justin and name bieber years is my am I 27 old.'},\n",
       "   {'text': 'Task: copy but say the opposite.\\n PSG won its match against Barca.'},\n",
       "   {'text': 'Is this review positive or negative? Review: Best cast iron skillet you will every buy.',\n",
       "    'example_title': 'Sentiment analysis'},\n",
       "   {'text': 'Question A: How is air traffic controlled? \\nQuestion B: How do you become an air traffic controller?\\nPick one: these questions are duplicates or not duplicates.'},\n",
       "   {'text': \"Barack Obama nominated Hilary Clinton as his secretary of state on Monday. He chose her because she had foreign affairs experience as a former First Lady. \\nIn the previous sentence, decide who 'her' is referring to.\",\n",
       "    'example_title': 'Coreference resolution'},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\\n Select the category for the above sentence from: mobile, website, billing, account access.'},\n",
       "   {'text': 'Sentence 1: Gyorgy Heizler, head of the local disaster unit, said the coach was carrying 38 passengers.\\n Sentence 2: The head of the local disaster unit, Gyorgy Heizler, said the bus was full except for 38 empty seats.\\n\\n Do sentences 1 and 2 have the same meaning?',\n",
       "    'example_title': 'Paraphrase identification'},\n",
       "   {'text': \"Here's the beginning of an article, choose a tag that best describes the topic of the article: business, cinema, politics, health, travel, sports.\\n\\n The best and worst fo 007 as 'No time to die' marks Daniel Craig's exit.\\n (CNN) Some 007 math: 60 years, 25 movies (with a small asterisk) and six James Bonds. For a Cold War creation, Ian Fleming's suave spy has certainly gotten around, but despite different guises in the tuxedo and occasional scuba gear, when it comes to Bond ratings, there really shouldn't be much argument about who wore it best.\"},\n",
       "   {'text': \"Max: Know any good websites to buy clothes from?\\n Payton: Sure :) LINK 1, LINK 2, LINK 3\\n Max: That's a lot of them!\\n Payton: Yeah, but they have different things so I usually buy things from 2 or 3 of them.\\n Max: I'll check them out. Thanks.\\n\\n Who or what are Payton and Max referring to when they say 'them'?\"},\n",
       "   {'text': \"Is the word 'table' used in the same meaning in the two following sentences?\\n\\n Sentence A: you can leave the books on the table over there.\\n Sentence B: the tables in this book are very hard to read.\"},\n",
       "   {'text': 'On a shelf, there are five books: a gray book, a red book, a purple book, a blue book, and a black book.\\n The red book is to the right of the gray book. The black book is to the left of the blue book. The blue book is to the left of the gray book. The purple book is the second from the right.\\n\\n Which book is the leftmost book?',\n",
       "    'example_title': 'Logic puzzles'},\n",
       "   {'text': \"The two men running to become New York City's next mayor will face off in their first debate Wednesday night.\\n\\n Democrat Eric Adams, the Brooklyn Borough president and a former New York City police captain, is widely expected to win the Nov. 2 election against Republican Curtis Sliwa, the founder of the 1970s-era Guardian Angels anti-crime patril.\\n\\n Who are the men running for mayor?\",\n",
       "    'example_title': 'Reading comprehension'},\n",
       "   {'text': \"The word 'binne' means any animal that is furry and has four legs, and the word 'bam' means a simple sort of dwelling.\\n\\n Which of the following best characterizes binne bams?\\n - Sentence 1: Binne bams are for pets.\\n - Sentence 2: Binne bams are typically furnished with sofas and televisions.\\n - Sentence 3: Binne bams are luxurious apartments.\\n - Sentence 4: Binne bams are places where people live.\"}],\n",
       "  'likes': 251,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['bigscience/P3'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'widget': [{'text': \"A is the son's of B's uncle. What is the family relationship between A and B?\"},\n",
       "    {'text': 'Reorder the words in this sentence: justin and name bieber years is my am I 27 old.'},\n",
       "    {'text': 'Task: copy but say the opposite.\\n PSG won its match against Barca.'},\n",
       "    {'text': 'Is this review positive or negative? Review: Best cast iron skillet you will every buy.',\n",
       "     'example_title': 'Sentiment analysis'},\n",
       "    {'text': 'Question A: How is air traffic controlled? \\nQuestion B: How do you become an air traffic controller?\\nPick one: these questions are duplicates or not duplicates.'},\n",
       "    {'text': \"Barack Obama nominated Hilary Clinton as his secretary of state on Monday. He chose her because she had foreign affairs experience as a former First Lady. \\nIn the previous sentence, decide who 'her' is referring to.\",\n",
       "     'example_title': 'Coreference resolution'},\n",
       "    {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.\\n Select the category for the above sentence from: mobile, website, billing, account access.'},\n",
       "    {'text': 'Sentence 1: Gyorgy Heizler, head of the local disaster unit, said the coach was carrying 38 passengers.\\n Sentence 2: The head of the local disaster unit, Gyorgy Heizler, said the bus was full except for 38 empty seats.\\n\\n Do sentences 1 and 2 have the same meaning?',\n",
       "     'example_title': 'Paraphrase identification'},\n",
       "    {'text': \"Here's the beginning of an article, choose a tag that best describes the topic of the article: business, cinema, politics, health, travel, sports.\\n\\n The best and worst fo 007 as 'No time to die' marks Daniel Craig's exit.\\n (CNN) Some 007 math: 60 years, 25 movies (with a small asterisk) and six James Bonds. For a Cold War creation, Ian Fleming's suave spy has certainly gotten around, but despite different guises in the tuxedo and occasional scuba gear, when it comes to Bond ratings, there really shouldn't be much argument about who wore it best.\"},\n",
       "    {'text': \"Max: Know any good websites to buy clothes from?\\n Payton: Sure :) LINK 1, LINK 2, LINK 3\\n Max: That's a lot of them!\\n Payton: Yeah, but they have different things so I usually buy things from 2 or 3 of them.\\n Max: I'll check them out. Thanks.\\n\\n Who or what are Payton and Max referring to when they say 'them'?\"},\n",
       "    {'text': \"Is the word 'table' used in the same meaning in the two following sentences?\\n\\n Sentence A: you can leave the books on the table over there.\\n Sentence B: the tables in this book are very hard to read.\"},\n",
       "    {'text': 'On a shelf, there are five books: a gray book, a red book, a purple book, a blue book, and a black book.\\n The red book is to the right of the gray book. The black book is to the left of the blue book. The blue book is to the left of the gray book. The purple book is the second from the right.\\n\\n Which book is the leftmost book?',\n",
       "     'example_title': 'Logic puzzles'},\n",
       "    {'text': \"The two men running to become New York City's next mayor will face off in their first debate Wednesday night.\\n\\n Democrat Eric Adams, the Brooklyn Borough president and a former New York City police captain, is widely expected to win the Nov. 2 election against Republican Curtis Sliwa, the founder of the 1970s-era Guardian Angels anti-crime patril.\\n\\n Who are the men running for mayor?\",\n",
       "     'example_title': 'Reading comprehension'},\n",
       "    {'text': \"The word 'binne' means any animal that is furry and has four legs, and the word 'bam' means a simple sort of dwelling.\\n\\n Which of the following best characterizes binne bams?\\n - Sentence 1: Binne bams are for pets.\\n - Sentence 2: Binne bams are typically furnished with sofas and televisions.\\n - Sentence 3: Binne bams are luxurious apartments.\\n - Sentence 4: Binne bams are places where people live.\"}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dandelin/vilt-b32-mlm': {'modelId': 'dandelin/vilt-b32-mlm',\n",
       "  'sha': '7051201a80eb7fd4f78a1a39f0e507a0204517ff',\n",
       "  'lastModified': '2022-01-23T09:42:02.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'vilt',\n",
       "   'fill-mask',\n",
       "   'arxiv:2102.03334',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'dandelin',\n",
       "  'config': {'architectures': ['ViltForMaskedLM'], 'model_type': 'vilt'},\n",
       "  'id': 'dandelin/vilt-b32-mlm',\n",
       "  'downloads': 10318,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0', 'tags': None},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'google/bert_uncased_L-4_H-256_A-4': {'modelId': 'google/bert_uncased_L-4_H-256_A-4',\n",
       "  'sha': '387825ce42dbb39b87911cdf8e383ee3b25184f8',\n",
       "  'lastModified': '2021-05-19T17:30:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'arxiv:1908.08962',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'google/bert_uncased_L-4_H-256_A-4',\n",
       "  'downloads': 10286,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens': {'modelId': 'sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens',\n",
       "  'sha': 'e614409446a9b8cc7eb7ac1087e11af9e99ab895',\n",
       "  'lastModified': '2022-06-15T20:39:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['XLMRobertaModel'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens',\n",
       "  'downloads': 10230,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cahya/xlm-roberta-large-indonesian-NER': {'modelId': 'cahya/xlm-roberta-large-indonesian-NER',\n",
       "  'sha': 'd0ef1c27f757b1c21ab299ccfb25fe858ac77ed4',\n",
       "  'lastModified': '2020-09-23T15:55:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'token-classification',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'cahya',\n",
       "  'config': {'architectures': ['XLMRobertaForTokenClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'cahya/xlm-roberta-large-indonesian-NER',\n",
       "  'downloads': 10188,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-base-french-europeana-cased': {'modelId': 'dbmdz/bert-base-french-europeana-cased',\n",
       "  'sha': 'b895c3cf291f7bf4c15639078a6bee0b3e272c5b',\n",
       "  'lastModified': '2021-09-13T21:03:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fr',\n",
       "   'transformers',\n",
       "   'historic french',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-base-french-europeana-cased',\n",
       "  'downloads': 10120,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr',\n",
       "   'license': 'mit',\n",
       "   'tags': ['historic french']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'deepset/roberta-base-squad2-distilled': {'modelId': 'deepset/roberta-base-squad2-distilled',\n",
       "  'sha': '299ce1fd69aa868bb76820746a82b73f4ac4d463',\n",
       "  'lastModified': '2022-01-06T16:19:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'dataset:squad_v2',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['RobertaForQuestionAnswering'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'deepset/roberta-base-squad2-distilled',\n",
       "  'downloads': 10118,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['squad_v2'],\n",
       "   'license': 'mit',\n",
       "   'thumbnail': 'https://thumb.tildacdn.com/tild3433-3637-4830-a533-353833613061/-/resize/720x/-/format/webp/germanquad.jpg',\n",
       "   'tags': ['exbert']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-bert': {'modelId': 'hf-internal-testing/tiny-bert',\n",
       "  'sha': '8a4db81d7e7ef2296d71ceb206e048c5734ce42f',\n",
       "  'lastModified': '2021-07-08T18:23:09.000Z',\n",
       "  'tags': ['pytorch', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {},\n",
       "  'id': 'hf-internal-testing/tiny-bert',\n",
       "  'downloads': 10038,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/stsb-roberta-large': {'modelId': 'sentence-transformers/stsb-roberta-large',\n",
       "  'sha': 'b6cd86898ba049a6d160dab42e298f677c5e63b6',\n",
       "  'lastModified': '2022-06-15T20:28:37.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/stsb-roberta-large',\n",
       "  'downloads': 10012,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sshleifer/distilbart-xsum-12-6': {'modelId': 'sshleifer/distilbart-xsum-12-6',\n",
       "  'sha': '5b2e376c845c201ddc34ec0e55fd1ad9890ba5ee',\n",
       "  'lastModified': '2021-06-14T07:58:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:cnn_dailymail',\n",
       "   'dataset:xsum',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {}},\n",
       "  'id': 'sshleifer/distilbart-xsum-12-6',\n",
       "  'downloads': 10004,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['summarization'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['cnn_dailymail', 'xsum'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/distilbart_medium.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-base-turkish-128k-cased': {'modelId': 'dbmdz/bert-base-turkish-128k-cased',\n",
       "  'sha': 'ee962e2ecfaafa8cf708fa961f5cbc4346b1c367',\n",
       "  'lastModified': '2021-05-19T15:10:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'tr',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-base-turkish-128k-cased',\n",
       "  'downloads': 9986,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'tr', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'facebook/wav2vec2-large-it-voxpopuli': {'modelId': 'facebook/wav2vec2-large-it-voxpopuli',\n",
       "  'sha': '06983d0205d75ad2b6ff6b31ef0cff420091ec85',\n",
       "  'lastModified': '2021-07-06T02:18:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'wav2vec2',\n",
       "   'pretraining',\n",
       "   'it',\n",
       "   'arxiv:2101.00390',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'automatic-speech-recognition',\n",
       "   'voxpopuli',\n",
       "   'license:cc-by-nc-4.0'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Wav2Vec2ForPreTraining'],\n",
       "   'model_type': 'wav2vec2'},\n",
       "  'id': 'facebook/wav2vec2-large-it-voxpopuli',\n",
       "  'downloads': 9952,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'it',\n",
       "   'tags': ['audio', 'automatic-speech-recognition', 'voxpopuli'],\n",
       "   'license': 'cc-by-nc-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'textattack/bert-base-uncased-SST-2': {'modelId': 'textattack/bert-base-uncased-SST-2',\n",
       "  'sha': '95f0f6f859b35c8ff0863ae3cd4e2dbc702c0ae2',\n",
       "  'lastModified': '2021-05-20T07:37:12.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'textattack',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'textattack/bert-base-uncased-SST-2',\n",
       "  'downloads': 9790,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'valhalla/t5-small-qg-hl': {'modelId': 'valhalla/t5-small-qg-hl',\n",
       "  'sha': '9fdee3255929ba5f0b9d45e76e1e0184f664a368',\n",
       "  'lastModified': '2021-06-23T14:43:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'dataset:squad',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'question-generation',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'valhalla',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 32,\n",
       "     'num_beams': 4,\n",
       "     'prefix': ''}}},\n",
       "  'id': 'valhalla/t5-small-qg-hl',\n",
       "  'downloads': 9781,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '<hl> 42 <hl> is the answer to life, the universe and everything. </s>'},\n",
       "   {'text': 'Python is a programming language. It is developed by <hl> Guido Van Rossum <hl>. </s>'},\n",
       "   {'text': 'Simple is better than <hl> complex <hl>. </s>'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['squad'],\n",
       "   'tags': ['question-generation'],\n",
       "   'widget': [{'text': '<hl> 42 <hl> is the answer to life, the universe and everything. </s>'},\n",
       "    {'text': 'Python is a programming language. It is developed by <hl> Guido Van Rossum <hl>. </s>'},\n",
       "    {'text': 'Simple is better than <hl> complex <hl>. </s>'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cardiffnlp/twitter-roberta-base-hate': {'modelId': 'cardiffnlp/twitter-roberta-base-hate',\n",
       "  'sha': '82cfd645130b125b8e8de63d90b0376620a0bfbd',\n",
       "  'lastModified': '2021-05-20T15:02:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'arxiv:2010.12421',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cardiffnlp',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cardiffnlp/twitter-roberta-base-hate',\n",
       "  'downloads': 9705,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pdelobelle/robbert-v2-dutch-base': {'modelId': 'pdelobelle/robbert-v2-dutch-base',\n",
       "  'sha': 'e28720e1a6cdf68ed3418c67a1964392905a7c8a',\n",
       "  'lastModified': '2022-05-19T20:45:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'nl',\n",
       "   'dataset:oscar',\n",
       "   'dataset:oscar (NL)',\n",
       "   'dataset:dbrd',\n",
       "   'dataset:lassy-ud',\n",
       "   'dataset:europarl-mono',\n",
       "   'dataset:conll2002',\n",
       "   'arxiv:2001.06286',\n",
       "   'arxiv:2004.02814',\n",
       "   'arxiv:2010.13652',\n",
       "   'arxiv:2101.05716',\n",
       "   'arxiv:1907.11692',\n",
       "   'arxiv:2001.02943',\n",
       "   'arxiv:1909.11942',\n",
       "   'transformers',\n",
       "   'Dutch',\n",
       "   'Flemish',\n",
       "   'RoBERTa',\n",
       "   'RobBERT',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'pdelobelle',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'pdelobelle/robbert-v2-dutch-base',\n",
       "  'downloads': 9696,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Hallo, ik ben RobBERT, een <mask> taalmodel van de KU Leuven.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'nl',\n",
       "   'thumbnail': 'https://github.com/iPieter/RobBERT/raw/master/res/robbert_logo.png',\n",
       "   'tags': ['Dutch', 'Flemish', 'RoBERTa', 'RobBERT'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['oscar',\n",
       "    'oscar (NL)',\n",
       "    'dbrd',\n",
       "    'lassy-ud',\n",
       "    'europarl-mono',\n",
       "    'conll2002'],\n",
       "   'widget': [{'text': 'Hallo, ik ben RobBERT, een <mask> taalmodel van de KU Leuven.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'studio-ousia/luke-base': {'modelId': 'studio-ousia/luke-base',\n",
       "  'sha': '7438924defd9f3c2018d63c16073bf4bcb6a70aa',\n",
       "  'lastModified': '2022-04-13T08:59:59.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'luke',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'arxiv:1906.08237',\n",
       "   'arxiv:1903.07785',\n",
       "   'arxiv:2002.01808',\n",
       "   'transformers',\n",
       "   'named entity recognition',\n",
       "   'entity typing',\n",
       "   'relation classification',\n",
       "   'question answering',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'studio-ousia',\n",
       "  'config': {'architectures': ['LukeForMaskedLM'], 'model_type': 'luke'},\n",
       "  'id': 'studio-ousia/luke-base',\n",
       "  'downloads': 9687,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://github.com/studio-ousia/luke/raw/master/resources/luke_logo.png',\n",
       "   'tags': ['luke',\n",
       "    'named entity recognition',\n",
       "    'entity typing',\n",
       "    'relation classification',\n",
       "    'question answering'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/Multilingual-MiniLM-L12-H384': {'modelId': 'microsoft/Multilingual-MiniLM-L12-H384',\n",
       "  'sha': 'f8a8e5023cbd4f94f1debed2578c65964dd4846b',\n",
       "  'lastModified': '2021-06-01T14:33:36.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'arxiv:2002.10957',\n",
       "   'arxiv:1809.05053',\n",
       "   'arxiv:1911.02116',\n",
       "   'arxiv:1910.07475',\n",
       "   'transformers',\n",
       "   'text-classification',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'microsoft/Multilingual-MiniLM-L12-H384',\n",
       "  'downloads': 9686,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 14,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'tags': ['text-classification'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/nq-distilbert-base-v1': {'modelId': 'sentence-transformers/nq-distilbert-base-v1',\n",
       "  'sha': 'a3dd10344d84c37c1d4a8be5d5021317900a2d19',\n",
       "  'lastModified': '2022-06-15T21:49:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/nq-distilbert-base-v1',\n",
       "  'downloads': 9682,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/chinese-electra-180g-base-discriminator': {'modelId': 'hfl/chinese-electra-180g-base-discriminator',\n",
       "  'sha': '693c1a7e58307777ad4cdf6b80b47c777c028572',\n",
       "  'lastModified': '2021-03-03T01:26:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'electra',\n",
       "   'zh',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'model_type': 'electra'},\n",
       "  'id': 'hfl/chinese-electra-180g-base-discriminator',\n",
       "  'downloads': 9601,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'deepset/gbert-large': {'modelId': 'deepset/gbert-large',\n",
       "  'sha': 'f6bca479ebb46e62ac99c03282a5030139e302f4',\n",
       "  'lastModified': '2022-02-17T14:05:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'fill-mask',\n",
       "   'de',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:OPUS',\n",
       "   'dataset:OpenLegalData',\n",
       "   'dataset:oscar',\n",
       "   'arxiv:2010.10906',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['BertForMaskedLM']},\n",
       "  'id': 'deepset/gbert-large',\n",
       "  'downloads': 9530,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['wikipedia', 'OPUS', 'OpenLegalData', 'oscar']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask'}},\n",
       " 'google/bert_uncased_L-4_H-512_A-8': {'modelId': 'google/bert_uncased_L-4_H-512_A-8',\n",
       "  'sha': '606e4d55252882ac25ba1f1d1a182075830f5a90',\n",
       "  'lastModified': '2021-05-19T17:30:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'arxiv:1908.08962',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'google/bert_uncased_L-4_H-512_A-8',\n",
       "  'downloads': 9500,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'NbAiLab/nb-bert-base': {'modelId': 'NbAiLab/nb-bert-base',\n",
       "  'sha': '82b194c0b3ea1fcad65f1eceee04adb26f9f71ac',\n",
       "  'lastModified': '2021-11-26T12:02:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'no',\n",
       "   'transformers',\n",
       "   'norwegian',\n",
       "   'license:cc-by-4.0',\n",
       "   'fill-mask'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'NbAiLab',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'NbAiLab/nb-bert-base',\n",
       "  'downloads': 9499,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'På biblioteket kan du [MASK] en bok.'},\n",
       "   {'text': 'Dette er et [MASK] eksempel.'},\n",
       "   {'text': 'Av og til kan en språkmodell gi et [MASK] resultat.'},\n",
       "   {'text': 'Som ansat får du [MASK] for at bidrage til borgernes adgang til dansk kulturarv, til forskning og til samfundets demokratiske udvikling.'}],\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'no',\n",
       "   'license': 'cc-by-4.0',\n",
       "   'tags': ['norwegian', 'bert'],\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'widget': [{'text': 'På biblioteket kan du [MASK] en bok.'},\n",
       "    {'text': 'Dette er et [MASK] eksempel.'},\n",
       "    {'text': 'Av og til kan en språkmodell gi et [MASK] resultat.'},\n",
       "    {'text': 'Som ansat får du [MASK] for at bidrage til borgernes adgang til dansk kulturarv, til forskning og til samfundets demokratiske udvikling.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'seyonec/ChemBERTa_zinc250k_v2_40k': {'modelId': 'seyonec/ChemBERTa_zinc250k_v2_40k',\n",
       "  'sha': '460f88413c1b572509b46fbd957a4296ea71e19f',\n",
       "  'lastModified': '2021-05-20T20:57:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'seyonec',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'seyonec/ChemBERTa_zinc250k_v2_40k',\n",
       "  'downloads': 9489,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KoboldAI/fairseq-dense-13B-Nerys': {'modelId': 'KoboldAI/fairseq-dense-13B-Nerys',\n",
       "  'sha': '2dea07a2017f755e1906cf6f0bf16fd13ff3e814',\n",
       "  'lastModified': '2022-06-25T11:22:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xglm',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'KoboldAI',\n",
       "  'config': {'architectures': ['XGLMForCausalLM'], 'model_type': 'xglm'},\n",
       "  'id': 'KoboldAI/fairseq-dense-13B-Nerys',\n",
       "  'downloads': 9459,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mfeb/albert-xxlarge-v2-squad2': {'modelId': 'mfeb/albert-xxlarge-v2-squad2',\n",
       "  'sha': 'e734d3529f402760605a24af13e02f6f092e96a0',\n",
       "  'lastModified': '2020-04-24T16:10:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'albert',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'mfeb',\n",
       "  'config': {'architectures': ['AlbertForQuestionAnswering'],\n",
       "   'model_type': 'albert'},\n",
       "  'id': 'mfeb/albert-xxlarge-v2-squad2',\n",
       "  'downloads': 9452,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'csarron/bert-base-uncased-squad-v1': {'modelId': 'csarron/bert-base-uncased-squad-v1',\n",
       "  'sha': '31129bdd485e06a6bec8fd2e045256369db34b0b',\n",
       "  'lastModified': '2021-05-19T14:32:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'dataset:squad',\n",
       "   'transformers',\n",
       "   'bert-base',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'csarron',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'csarron/bert-base-uncased-squad-v1',\n",
       "  'downloads': 9426,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'},\n",
       "   {'text': 'How many square kilometers of rainforest is covered in the basin?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': None,\n",
       "   'license': 'mit',\n",
       "   'tags': ['question-answering', 'bert', 'bert-base'],\n",
       "   'datasets': ['squad'],\n",
       "   'metrics': ['squad'],\n",
       "   'widget': [{'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "     'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'},\n",
       "    {'text': 'How many square kilometers of rainforest is covered in the basin?',\n",
       "     'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'DeepPavlov/distilrubert-tiny-cased-conversational': {'modelId': 'DeepPavlov/distilrubert-tiny-cased-conversational',\n",
       "  'sha': 'a70de709fe33d6879bd82337162bdd2ea19442bd',\n",
       "  'lastModified': '2022-06-28T17:10:33.000Z',\n",
       "  'tags': ['pytorch', 'distilbert', 'ru', 'arxiv:2205.02340', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'DeepPavlov',\n",
       "  'config': {'model_type': 'distilbert'},\n",
       "  'id': 'DeepPavlov/distilrubert-tiny-cased-conversational',\n",
       "  'downloads': 9315,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'Yanhao/simcse-bert-for-patent': {'modelId': 'Yanhao/simcse-bert-for-patent',\n",
       "  'sha': 'e46b5526806e0a2cc868ea06ffff6e113fb62a20',\n",
       "  'lastModified': '2022-05-04T21:32:00.000Z',\n",
       "  'tags': ['pytorch', 'roberta', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'Yanhao',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'Yanhao/simcse-bert-for-patent',\n",
       "  'downloads': 9307,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/trocr-base-printed': {'modelId': 'microsoft/trocr-base-printed',\n",
       "  'sha': '191a64dd5078db39e975a76b798b6fd026a96fa6',\n",
       "  'lastModified': '2022-07-01T07:35:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'vision-encoder-decoder',\n",
       "   'arxiv:2109.10282',\n",
       "   'transformers',\n",
       "   'trocr',\n",
       "   'image-to-text'],\n",
       "  'pipeline_tag': 'image-to-text',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['VisionEncoderDecoderModel'],\n",
       "   'model_type': 'vision-encoder-decoder'},\n",
       "  'id': 'microsoft/trocr-base-printed',\n",
       "  'downloads': 9263,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['trocr', 'image-to-text']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/dit-base-finetuned-rvlcdip': {'modelId': 'microsoft/dit-base-finetuned-rvlcdip',\n",
       "  'sha': '27841b0cfbb6d626f7b53c4fd4daaf41a45ea5f5',\n",
       "  'lastModified': '2022-03-08T10:39:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'beit',\n",
       "   'image-classification',\n",
       "   'arxiv:2203.02378',\n",
       "   'transformers',\n",
       "   'dit',\n",
       "   'vision'],\n",
       "  'pipeline_tag': 'image-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['BeitForImageClassification'],\n",
       "   'model_type': 'beit'},\n",
       "  'id': 'microsoft/dit-base-finetuned-rvlcdip',\n",
       "  'downloads': 9234,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'src': 'https://huggingface.co/microsoft/dit-base-finetuned-rvlcdip/resolve/main/coca_cola_advertisement.png',\n",
       "    'example_title': 'Advertisement'},\n",
       "   {'src': 'https://huggingface.co/microsoft/dit-base-finetuned-rvlcdip/resolve/main/scientific_publication.png',\n",
       "    'example_title': 'Scientific publication'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['dit', 'vision', 'image-classification'],\n",
       "   'widget': [{'src': 'https://huggingface.co/microsoft/dit-base-finetuned-rvlcdip/resolve/main/coca_cola_advertisement.png',\n",
       "     'example_title': 'Advertisement'},\n",
       "    {'src': 'https://huggingface.co/microsoft/dit-base-finetuned-rvlcdip/resolve/main/scientific_publication.png',\n",
       "     'example_title': 'Scientific publication'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageClassification',\n",
       "   'pipeline_tag': 'image-classification',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'dmis-lab/biobert-large-cased-v1.1-squad': {'modelId': 'dmis-lab/biobert-large-cased-v1.1-squad',\n",
       "  'sha': '2b17f30cda1efcbe0d6ab3b977856c7898f934b1',\n",
       "  'lastModified': '2021-05-19T16:01:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'dmis-lab',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'dmis-lab/biobert-large-cased-v1.1-squad',\n",
       "  'downloads': 9207,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ckiplab/bert-base-chinese-ner': {'modelId': 'ckiplab/bert-base-chinese-ner',\n",
       "  'sha': '50c5afc0a0131e8ab93f54d9ebf9575af04c22d5',\n",
       "  'lastModified': '2022-05-10T03:28:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'license:gpl-3.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'ckiplab',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'ckiplab/bert-base-chinese-ner',\n",
       "  'downloads': 9203,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '我叫沃尔夫冈，我住在柏林。'},\n",
       "   {'text': '我叫萨拉，我住在伦敦。'},\n",
       "   {'text': '我叫克拉拉，我住在加州伯克利。'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'thumbnail': 'https://ckip.iis.sinica.edu.tw/files/ckip_logo.png',\n",
       "   'tags': ['pytorch', 'token-classification', 'bert', 'zh'],\n",
       "   'license': 'gpl-3.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Babelscape/rebel-large': {'modelId': 'Babelscape/rebel-large',\n",
       "  'sha': 'd24237e8ab9c1ad2cbdf53fd54b0d7cda1da8018',\n",
       "  'lastModified': '2022-05-27T10:20:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:Babelscape/rebel-dataset',\n",
       "   'transformers',\n",
       "   'seq2seq',\n",
       "   'relation-extraction',\n",
       "   'license:cc-by-nc-sa-4.0',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'Babelscape',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'relation_extraction': {'length_penalty': 0,\n",
       "     'max_length': 256,\n",
       "     'min_length': 12,\n",
       "     'no_repeat_ngram_size': 0,\n",
       "     'num_beams': 4}}},\n",
       "  'id': 'Babelscape/rebel-large',\n",
       "  'downloads': 9200,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic'}],\n",
       "  'likes': 19,\n",
       "  'model-index': [{'name': 'REBEL',\n",
       "    'results': [{'task': {'name': 'Relation Extraction',\n",
       "       'type': 'Relation-Extraction'},\n",
       "      'dataset': {'name': 'CoNLL04', 'type': 'CoNLL04'},\n",
       "      'metrics': [{'name': 'RE+ Macro F1',\n",
       "        'type': 're+ macro f1',\n",
       "        'value': 76.65}]},\n",
       "     {'task': {'name': 'Relation Extraction', 'type': 'Relation-Extraction'},\n",
       "      'dataset': {'name': 'NYT', 'type': 'NYT'},\n",
       "      'metrics': [{'name': 'F1', 'type': 'f1', 'value': 93.4}]}]}],\n",
       "  'cardData': {'language': ['en'],\n",
       "   'widget': [{'text': 'Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic'}],\n",
       "   'tags': ['seq2seq', 'relation-extraction'],\n",
       "   'datasets': ['Babelscape/rebel-dataset'],\n",
       "   'model-index': [{'name': 'REBEL',\n",
       "     'results': [{'task': {'name': 'Relation Extraction',\n",
       "        'type': 'Relation-Extraction'},\n",
       "       'dataset': {'name': 'CoNLL04', 'type': 'CoNLL04'},\n",
       "       'metrics': [{'name': 'RE+ Macro F1',\n",
       "         'type': 're+ macro f1',\n",
       "         'value': 76.65}]},\n",
       "      {'task': {'name': 'Relation Extraction', 'type': 'Relation-Extraction'},\n",
       "       'dataset': {'name': 'NYT', 'type': 'NYT'},\n",
       "       'metrics': [{'name': 'F1', 'type': 'f1', 'value': 93.4}]}]}],\n",
       "   'license': 'cc-by-nc-sa-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ALINEAR/albert-japanese-v2': {'modelId': 'ALINEAR/albert-japanese-v2',\n",
       "  'sha': '102cec3c2b7bc8483cd9281b6a029279861df66d',\n",
       "  'lastModified': '2020-05-04T13:20:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'ALINEAR',\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'ALINEAR/albert-japanese-v2',\n",
       "  'downloads': 9152,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/distilbert-base-nli-stsb-quora-ranking': {'modelId': 'sentence-transformers/distilbert-base-nli-stsb-quora-ranking',\n",
       "  'sha': 'f39736041df2a9460ef1525cc9052c3fa39bebc2',\n",
       "  'lastModified': '2022-06-15T22:01:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/distilbert-base-nli-stsb-quora-ranking',\n",
       "  'downloads': 9132,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Hate-speech-CNERG/bert-base-uncased-hatexplain': {'modelId': 'Hate-speech-CNERG/bert-base-uncased-hatexplain',\n",
       "  'sha': 'e487c81b768c7532bf474bd5e486dedea4cf3848',\n",
       "  'lastModified': '2021-05-25T09:53:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:hatexplain',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Hate-speech-CNERG',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Hate-speech-CNERG/bert-base-uncased-hatexplain',\n",
       "  'downloads': 9026,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['hatexplain']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pysentimiento/robertuito-hate-speech': {'modelId': 'pysentimiento/robertuito-hate-speech',\n",
       "  'sha': '272493f45c85fd9b6590716d0206443f2ce79731',\n",
       "  'lastModified': '2021-12-02T21:50:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'es',\n",
       "   'arxiv:2106.09462',\n",
       "   'arxiv:2111.09453',\n",
       "   'transformers',\n",
       "   'twitter',\n",
       "   'hate-speech'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'pysentimiento',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'pysentimiento/robertuito-hate-speech',\n",
       "  'downloads': 8825,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Te quiero. Te amo.'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['es'], 'tags': ['twitter', 'hate-speech']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'papluca/xlm-roberta-base-language-detection': {'modelId': 'papluca/xlm-roberta-base-language-detection',\n",
       "  'sha': 'f793746a8fff7a83f266fa0df91064727c8c76a2',\n",
       "  'lastModified': '2021-11-25T12:41:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'text-classification',\n",
       "   'arxiv:1911.02116',\n",
       "   'transformers',\n",
       "   'generated_from_trainer',\n",
       "   'license:mit',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'papluca',\n",
       "  'config': {'architectures': ['XLMRobertaForSequenceClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'papluca/xlm-roberta-base-language-detection',\n",
       "  'downloads': 8709,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 8,\n",
       "  'model-index': [{'name': 'xlm-roberta-base-language-detection',\n",
       "    'results': []}],\n",
       "  'cardData': {'license': 'mit',\n",
       "   'tags': ['generated_from_trainer'],\n",
       "   'metrics': ['accuracy', 'f1'],\n",
       "   'model-index': [{'name': 'xlm-roberta-base-language-detection',\n",
       "     'results': []}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dlb/electra-base-portuguese-uncased-brwac': {'modelId': 'dlb/electra-base-portuguese-uncased-brwac',\n",
       "  'sha': '60cd1eb5aa58c9468200271ae1b4f55cd1ee2036',\n",
       "  'lastModified': '2021-12-10T12:33:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'pt',\n",
       "   'dataset:brwac',\n",
       "   'transformers',\n",
       "   'electra',\n",
       "   'pretraining'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dlb',\n",
       "  'config': {},\n",
       "  'id': 'dlb/electra-base-portuguese-uncased-brwac',\n",
       "  'downloads': 8689,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'pt',\n",
       "   'tags': ['electra', 'pretraining', 'pytorch'],\n",
       "   'datasets': ['brwac']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'DeepPavlov/distilrubert-tiny-cased-conversational-v1': {'modelId': 'DeepPavlov/distilrubert-tiny-cased-conversational-v1',\n",
       "  'sha': '2033d0d1de807e8181ebfa0e53d2a8e526412b0f',\n",
       "  'lastModified': '2022-05-06T11:57:05.000Z',\n",
       "  'tags': ['pytorch', 'distilbert', 'ru', 'arxiv:2205.02340', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'DeepPavlov',\n",
       "  'config': {'model_type': 'distilbert'},\n",
       "  'id': 'DeepPavlov/distilrubert-tiny-cased-conversational-v1',\n",
       "  'downloads': 8686,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'ArthurZ/flax-tiny-random-bert-sharded': {'modelId': 'ArthurZ/flax-tiny-random-bert-sharded',\n",
       "  'sha': '52b09e4c2b7081b8027dc2d05c14f79c90bd501f',\n",
       "  'lastModified': '2022-06-17T16:08:48.000Z',\n",
       "  'tags': ['jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'ArthurZ',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'ArthurZ/flax-tiny-random-bert-sharded',\n",
       "  'downloads': 8684,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'HooshvareLab/bert-fa-base-uncased': {'modelId': 'HooshvareLab/bert-fa-base-uncased',\n",
       "  'sha': 'a04aa40c97bcdde570ae11986a534542c2995a62',\n",
       "  'lastModified': '2021-05-18T21:02:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'fa',\n",
       "   'arxiv:2005.12515',\n",
       "   'transformers',\n",
       "   'bert-fa',\n",
       "   'bert-persian',\n",
       "   'persian-lm',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'HooshvareLab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'HooshvareLab/bert-fa-base-uncased',\n",
       "  'downloads': 8662,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'زندگی یک سوال است و این که چگونه [MASK] کنیم پاسخ این سوال!'},\n",
       "   {'text': 'زندگی از مرگ پرسید: چرا همه من را [MASK] دارند اما از تو متنفرند؟'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fa',\n",
       "   'tags': ['bert-fa', 'bert-persian', 'persian-lm'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ckiplab/bert-base-chinese-pos': {'modelId': 'ckiplab/bert-base-chinese-pos',\n",
       "  'sha': 'c3f173670d4793f00ce5d23381cbeffa17e4e197',\n",
       "  'lastModified': '2022-05-10T03:28:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'license:gpl-3.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'ckiplab',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'ckiplab/bert-base-chinese-pos',\n",
       "  'downloads': 8629,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '我叫沃尔夫冈，我住在柏林。'},\n",
       "   {'text': '我叫萨拉，我住在伦敦。'},\n",
       "   {'text': '我叫克拉拉，我住在加州伯克利。'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'thumbnail': 'https://ckip.iis.sinica.edu.tw/files/ckip_logo.png',\n",
       "   'tags': ['pytorch', 'token-classification', 'bert', 'zh'],\n",
       "   'license': 'gpl-3.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'jhgan/ko-sroberta-multitask': {'modelId': 'jhgan/ko-sroberta-multitask',\n",
       "  'sha': 'ab957ae6a91e99c4cad36d52063a2a9cf1bf4419',\n",
       "  'lastModified': '2022-06-13T16:34:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'ko',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'jhgan',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'jhgan/ko-sroberta-multitask',\n",
       "  'downloads': 8578,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers'],\n",
       "   'language': 'ko'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'kssteven/ibert-roberta-base': {'modelId': 'kssteven/ibert-roberta-base',\n",
       "  'sha': '4f98e9110b04a8958444d3af8ed39287834fbb90',\n",
       "  'lastModified': '2021-11-22T10:09:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'ibert',\n",
       "   'fill-mask',\n",
       "   'arxiv:1907.11692',\n",
       "   'arxiv:2101.01321',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'kssteven',\n",
       "  'config': {'architectures': ['IBertForMaskedLM'], 'model_type': 'ibert'},\n",
       "  'id': 'kssteven/ibert-roberta-base',\n",
       "  'downloads': 8550,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/infoxlm-base': {'modelId': 'microsoft/infoxlm-base',\n",
       "  'sha': 'c67f260d5635cdeef35864fd2ce369d24eca1b34',\n",
       "  'lastModified': '2021-08-04T11:42:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'arxiv:2007.07834',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['XLMRobertaForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'microsoft/infoxlm-base',\n",
       "  'downloads': 8539,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'TODBERT/TOD-BERT-JNT-V1': {'modelId': 'TODBERT/TOD-BERT-JNT-V1',\n",
       "  'sha': '903797e92f97b5e61a1142636b2d604682a1032c',\n",
       "  'lastModified': '2021-05-18T22:44:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'TODBERT',\n",
       "  'config': {'architectures': ['BertForMaskedLM'],\n",
       "   'model_type': 'bert',\n",
       "   'task_specific_params': None},\n",
       "  'id': 'TODBERT/TOD-BERT-JNT-V1',\n",
       "  'downloads': 8532,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'M-CLIP/M-BERT-Distil-40': {'modelId': 'M-CLIP/M-BERT-Distil-40',\n",
       "  'sha': 'ff20c09c1a088589cb65a169d165b5ddcbe792ca',\n",
       "  'lastModified': '2021-03-21T15:39:15.000Z',\n",
       "  'tags': ['pytorch', 'distilbert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'M-CLIP',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'M-CLIP/M-BERT-Distil-40',\n",
       "  'downloads': 8527,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'jonatasgrosman/wav2vec2-large-xlsr-53-russian': {'modelId': 'jonatasgrosman/wav2vec2-large-xlsr-53-russian',\n",
       "  'sha': 'ce7dbfba0ef183421eded7358a6643f030f8c4cb',\n",
       "  'lastModified': '2022-06-22T17:05:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'ru',\n",
       "   'dataset:common_voice',\n",
       "   'dataset:mozilla-foundation/common_voice_6_0',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'hf-asr-leaderboard',\n",
       "   'mozilla-foundation/common_voice_6_0',\n",
       "   'robust-speech-event',\n",
       "   'speech',\n",
       "   'xlsr-fine-tuning-week',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'jonatasgrosman',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'jonatasgrosman/wav2vec2-large-xlsr-53-russian',\n",
       "  'downloads': 8518,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 9,\n",
       "  'model-index': [{'name': 'XLSR Wav2Vec2 Russian by Jonatas Grosman',\n",
       "    'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Common Voice ru',\n",
       "       'type': 'common_voice',\n",
       "       'args': 'ru'},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 13.3},\n",
       "       {'name': 'Test CER', 'type': 'cer', 'value': 2.88},\n",
       "       {'name': 'Test WER (+LM)', 'type': 'wer', 'value': 9.57},\n",
       "       {'name': 'Test CER (+LM)', 'type': 'cer', 'value': 2.24}]},\n",
       "     {'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Robust Speech Event - Dev Data',\n",
       "       'type': 'speech-recognition-community-v2/dev_data',\n",
       "       'args': 'ru'},\n",
       "      'metrics': [{'name': 'Dev WER', 'type': 'wer', 'value': 40.22},\n",
       "       {'name': 'Dev CER', 'type': 'cer', 'value': 14.8},\n",
       "       {'name': 'Dev WER (+LM)', 'type': 'wer', 'value': 33.61},\n",
       "       {'name': 'Dev CER (+LM)', 'type': 'cer', 'value': 13.5}]}]}],\n",
       "  'cardData': {'language': 'ru',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['common_voice', 'mozilla-foundation/common_voice_6_0'],\n",
       "   'metrics': ['wer', 'cer'],\n",
       "   'tags': ['audio',\n",
       "    'automatic-speech-recognition',\n",
       "    'hf-asr-leaderboard',\n",
       "    'mozilla-foundation/common_voice_6_0',\n",
       "    'robust-speech-event',\n",
       "    'ru',\n",
       "    'speech',\n",
       "    'xlsr-fine-tuning-week'],\n",
       "   'model-index': [{'name': 'XLSR Wav2Vec2 Russian by Jonatas Grosman',\n",
       "     'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Common Voice ru',\n",
       "        'type': 'common_voice',\n",
       "        'args': 'ru'},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 13.3},\n",
       "        {'name': 'Test CER', 'type': 'cer', 'value': 2.88},\n",
       "        {'name': 'Test WER (+LM)', 'type': 'wer', 'value': 9.57},\n",
       "        {'name': 'Test CER (+LM)', 'type': 'cer', 'value': 2.24}]},\n",
       "      {'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Robust Speech Event - Dev Data',\n",
       "        'type': 'speech-recognition-community-v2/dev_data',\n",
       "        'args': 'ru'},\n",
       "       'metrics': [{'name': 'Dev WER', 'type': 'wer', 'value': 40.22},\n",
       "        {'name': 'Dev CER', 'type': 'cer', 'value': 14.8},\n",
       "        {'name': 'Dev WER (+LM)', 'type': 'wer', 'value': 33.61},\n",
       "        {'name': 'Dev CER (+LM)', 'type': 'cer', 'value': 13.5}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'klue/roberta-base': {'modelId': 'klue/roberta-base',\n",
       "  'sha': '67dd433d36ebc66a42c9aaa85abcf8d2620e41d9',\n",
       "  'lastModified': '2021-10-20T16:10:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'ko',\n",
       "   'arxiv:2105.09680',\n",
       "   'transformers',\n",
       "   'korean',\n",
       "   'klue',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'klue',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'klue/roberta-base',\n",
       "  'downloads': 8517,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '대한민국의 수도는 [MASK] 입니다.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ko',\n",
       "   'tags': ['korean', 'klue'],\n",
       "   'mask_token': '[MASK]',\n",
       "   'widget': [{'text': '대한민국의 수도는 [MASK] 입니다.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'camembert/camembert-large': {'modelId': 'camembert/camembert-large',\n",
       "  'sha': 'df7dbf53dd70551faa6b4ec45deb4a566445c7cc',\n",
       "  'lastModified': '2020-12-11T21:35:25.000Z',\n",
       "  'tags': ['pytorch', 'camembert', 'fr', 'arxiv:1911.03894', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'camembert',\n",
       "  'config': {'model_type': 'camembert'},\n",
       "  'id': 'camembert/camembert-large',\n",
       "  'downloads': 8484,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'ai4bharat/indic-bert': {'modelId': 'ai4bharat/indic-bert',\n",
       "  'sha': '97ae2d6440dbd1a2698540223dc00b43075c69c9',\n",
       "  'lastModified': '2021-04-12T09:06:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'albert',\n",
       "   'en',\n",
       "   'dataset:AI4Bharat IndicNLP Corpora',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'ai4bharat',\n",
       "  'config': {'model_type': 'albert'},\n",
       "  'id': 'ai4bharat/indic-bert',\n",
       "  'downloads': 8483,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['AI4Bharat IndicNLP Corpora']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'iarfmoose/t5-base-question-generator': {'modelId': 'iarfmoose/t5-base-question-generator',\n",
       "  'sha': '1bfc9d4b2b0078e0b65cf40c6e2e2e974fbab6b0',\n",
       "  'lastModified': '2022-02-24T08:41:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'iarfmoose',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'iarfmoose/t5-base-question-generator',\n",
       "  'downloads': 8483,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 16,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-base-turkish-128k-uncased': {'modelId': 'dbmdz/bert-base-turkish-128k-uncased',\n",
       "  'sha': 'f5287aecee60f0c597c11c34341cb92d31c0e71b',\n",
       "  'lastModified': '2021-05-19T15:13:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'tr',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-base-turkish-128k-uncased',\n",
       "  'downloads': 8391,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'tr', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sberbank-ai/mGPT': {'modelId': 'sberbank-ai/mGPT',\n",
       "  'sha': '9f49a85776d5ec166120ea81719987fe0f643574',\n",
       "  'lastModified': '2022-04-21T18:06:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'az',\n",
       "   'sw',\n",
       "   'af',\n",
       "   'ar',\n",
       "   'ba',\n",
       "   'be',\n",
       "   'bxr',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'cv',\n",
       "   'hy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'es',\n",
       "   'eu',\n",
       "   'fa',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'he',\n",
       "   'hi',\n",
       "   'hu',\n",
       "   'kk',\n",
       "   'id',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'ka',\n",
       "   'ky',\n",
       "   'ko',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mn',\n",
       "   'ml',\n",
       "   'os',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'my',\n",
       "   'nl',\n",
       "   'ro',\n",
       "   'pl',\n",
       "   'pt',\n",
       "   'sah',\n",
       "   'ru',\n",
       "   'tg',\n",
       "   'sv',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'tk',\n",
       "   'th',\n",
       "   'tr',\n",
       "   'tl',\n",
       "   'tt',\n",
       "   'tyv',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'vi',\n",
       "   'uz',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'xal',\n",
       "   'dataset:mc4',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:2112.10668',\n",
       "   'arxiv:2204.07580',\n",
       "   'transformers',\n",
       "   'multilingual',\n",
       "   'PyTorch',\n",
       "   'Transformers',\n",
       "   'gpt3',\n",
       "   'Deepspeed',\n",
       "   'Megatron',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'sberbank-ai',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'], 'model_type': 'gpt2'},\n",
       "  'id': 'sberbank-ai/mGPT',\n",
       "  'downloads': 8362,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 53,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'language': ['en',\n",
       "    'az',\n",
       "    'sw',\n",
       "    'af',\n",
       "    'ar',\n",
       "    'ba',\n",
       "    'be',\n",
       "    'bxr',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'cv',\n",
       "    'hy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'es',\n",
       "    'eu',\n",
       "    'fa',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'he',\n",
       "    'hi',\n",
       "    'hu',\n",
       "    'kk',\n",
       "    'id',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'ka',\n",
       "    'ky',\n",
       "    'ko',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mn',\n",
       "    'ml',\n",
       "    'os',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'my',\n",
       "    'nl',\n",
       "    'ro',\n",
       "    'pl',\n",
       "    'pt',\n",
       "    'sah',\n",
       "    'ru',\n",
       "    'tg',\n",
       "    'sv',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'tk',\n",
       "    'th',\n",
       "    'tr',\n",
       "    'tl',\n",
       "    'tt',\n",
       "    'tyv',\n",
       "    'uk',\n",
       "    'en',\n",
       "    'ur',\n",
       "    'vi',\n",
       "    'uz',\n",
       "    'yo',\n",
       "    'zh',\n",
       "    'xal'],\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'tags': ['multilingual',\n",
       "    'PyTorch',\n",
       "    'Transformers',\n",
       "    'gpt3',\n",
       "    'gpt2',\n",
       "    'Deepspeed',\n",
       "    'Megatron'],\n",
       "   'datasets': ['mc4', 'wikipedia'],\n",
       "   'thumbnail': 'https://github.com/sberbank-ai/mgpt'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large': {'modelId': 'nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large',\n",
       "  'sha': '160deb78aca30f63754e512a93337ce8013a32ca',\n",
       "  'lastModified': '2021-06-20T19:03:02.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'nreimers',\n",
       "  'config': {'architectures': ['XLMRobertaForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'nreimers/mMiniLMv2-L6-H384-distilled-from-XLMR-Large',\n",
       "  'downloads': 8355,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Harveenchadha/vakyansh-wav2vec2-hindi-him-4200': {'modelId': 'Harveenchadha/vakyansh-wav2vec2-hindi-him-4200',\n",
       "  'sha': 'e2568c3f7868d8aa3aaabcf28fa100d10d54c170',\n",
       "  'lastModified': '2022-01-29T06:03:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'hi',\n",
       "   'arxiv:2107.07402',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'speech',\n",
       "   'license:mit',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'Harveenchadha',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'Harveenchadha/vakyansh-wav2vec2-hindi-him-4200',\n",
       "  'downloads': 8339,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 0,\n",
       "  'model-index': [{'name': 'Wav2Vec2 Vakyansh Hindi Model by Harveen Chadha',\n",
       "    'results': [{'task': {'name': 'Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Common Voice hi',\n",
       "       'type': 'common_voice',\n",
       "       'args': 'hi'},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 33.17}]}]}],\n",
       "  'cardData': {'language': 'hi',\n",
       "   'metrics': ['wer'],\n",
       "   'tags': ['audio', 'automatic-speech-recognition', 'speech'],\n",
       "   'license': 'mit',\n",
       "   'model-index': [{'name': 'Wav2Vec2 Vakyansh Hindi Model by Harveen Chadha',\n",
       "     'results': [{'task': {'name': 'Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Common Voice hi',\n",
       "        'type': 'common_voice',\n",
       "        'args': 'hi'},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 33.17}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'nghuyong/ernie-1.0': {'modelId': 'nghuyong/ernie-1.0',\n",
       "  'sha': 'b06176bf30ecf544330ab008933c9ac1012f1a6d',\n",
       "  'lastModified': '2021-05-20T01:40:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'zh',\n",
       "   'arxiv:1904.09223',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'nghuyong',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'nghuyong/ernie-1.0',\n",
       "  'downloads': 8337,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'zh'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli': {'modelId': 'ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli',\n",
       "  'sha': '5b605abab9b75bc87ab66cfc049ef58d9d64b8ed',\n",
       "  'lastModified': '2021-05-20T23:17:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'dataset:snli',\n",
       "   'dataset:anli',\n",
       "   'dataset:multi_nli',\n",
       "   'dataset:multi_nli_mismatch',\n",
       "   'dataset:fever',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'ynie',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli',\n",
       "  'downloads': 8331,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['snli',\n",
       "    'anli',\n",
       "    'multi_nli',\n",
       "    'multi_nli_mismatch',\n",
       "    'fever'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'kamalkraj/deberta-base': {'modelId': 'kamalkraj/deberta-base',\n",
       "  'sha': 'e6f7f722f429327e65a625ccd1f1ab9b1bfa9abc',\n",
       "  'lastModified': '2021-08-08T09:12:57.000Z',\n",
       "  'tags': ['tf',\n",
       "   'deberta',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'deberta-v1',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'kamalkraj',\n",
       "  'config': {'architectures': ['DebertaModel'], 'model_type': 'deberta'},\n",
       "  'id': 'kamalkraj/deberta-base',\n",
       "  'downloads': 8320,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': 'deberta-v1',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/deit-small-distilled-patch16-224': {'modelId': 'facebook/deit-small-distilled-patch16-224',\n",
       "  'sha': '167101edd3b696f0030f70bfc752bc6408a0153d',\n",
       "  'lastModified': '2022-03-18T14:38:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'deit',\n",
       "   'image-classification',\n",
       "   'dataset:imagenet',\n",
       "   'arxiv:2012.12877',\n",
       "   'arxiv:2006.03677',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-classification',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['DeiTForImageClassificationWithTeacher'],\n",
       "   'model_type': 'deit'},\n",
       "  'id': 'facebook/deit-small-distilled-patch16-224',\n",
       "  'downloads': 8263,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['image-classification', 'vision'],\n",
       "   'datasets': ['imagenet']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageClassification',\n",
       "   'pipeline_tag': 'image-classification',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'cl-tohoku/bert-base-japanese-char-whole-word-masking': {'modelId': 'cl-tohoku/bert-base-japanese-char-whole-word-masking',\n",
       "  'sha': 'fa5374ac8a5b2c6d3f5f8e156a9892cdf687201d',\n",
       "  'lastModified': '2021-09-23T13:45:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ja',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'cl-tohoku',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'cl-tohoku/bert-base-japanese-char-whole-word-masking',\n",
       "  'downloads': 8164,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '仙台は「[MASK]の都」と呼ばれている。'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'datasets': ['wikipedia'],\n",
       "   'widget': [{'text': '仙台は「[MASK]の都」と呼ばれている。'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/opt-6.7b': {'modelId': 'facebook/opt-6.7b',\n",
       "  'sha': '8dc17cdd7b9381612e631064e569f4142d776d88',\n",
       "  'lastModified': '2022-06-24T05:22:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'opt',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'arxiv:2205.01068',\n",
       "   'arxiv:2005.14165',\n",
       "   'transformers',\n",
       "   'license:other'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['OPTForCausalLM'], 'model_type': 'opt'},\n",
       "  'id': 'facebook/opt-6.7b',\n",
       "  'downloads': 8150,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'inference': False,\n",
       "   'tags': ['text-generation', 'opt'],\n",
       "   'license': 'other',\n",
       "   'commercial': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/tapex-base': {'modelId': 'microsoft/tapex-base',\n",
       "  'sha': '968109c940c8b270a3eaec1532d596ba6c923b6a',\n",
       "  'lastModified': '2022-05-17T08:25:49.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:2107.07653',\n",
       "   'transformers',\n",
       "   'tapex',\n",
       "   'table-question-answering',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'table-question-answering',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart'},\n",
       "  'id': 'microsoft/tapex-base',\n",
       "  'downloads': 8143,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'How many stars does the transformers repository have?',\n",
       "    'table': {'Repository': ['Transformers', 'Datasets', 'Tokenizers'],\n",
       "     'Stars': [36542, 4512, 3934],\n",
       "     'Contributors': [651, 77, 34],\n",
       "     'Programming language': ['Python',\n",
       "      'Python',\n",
       "      'Rust, Python and NodeJS']}}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['tapex', 'table-question-answering'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nlpaueb/bert-base-uncased-eurlex': {'modelId': 'nlpaueb/bert-base-uncased-eurlex',\n",
       "  'sha': '77adac5a840314c4d688f565a925db1bdd2e87e7',\n",
       "  'lastModified': '2022-04-28T14:44:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'legal',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'fill-mask'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'nlpaueb',\n",
       "  'config': {'architectures': None, 'model_type': 'bert'},\n",
       "  'id': 'nlpaueb/bert-base-uncased-eurlex',\n",
       "  'downloads': 8008,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Establishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'thumbnail': 'https://i.ibb.co/p3kQ7Rw/Screenshot-2020-10-06-at-12-16-36-PM.png',\n",
       "   'tags': ['legal'],\n",
       "   'widget': [{'text': 'Establishing a system for the identification and registration of [MASK] animals and regarding the labelling of beef and beef products.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'vblagoje/bart_lfqa': {'modelId': 'vblagoje/bart_lfqa',\n",
       "  'sha': '5493d5be6812cdb4835e004ce17ea2082cc25b03',\n",
       "  'lastModified': '2022-02-14T15:54:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:vblagoje/lfqa',\n",
       "   'dataset:vblagoje/lfqa_support_docs',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'vblagoje',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4}}},\n",
       "  'id': 'vblagoje/bart_lfqa',\n",
       "  'downloads': 7991,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['vblagoje/lfqa', 'vblagoje/lfqa_support_docs'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/mbart-large-50-many-to-one-mmt': {'modelId': 'facebook/mbart-large-50-many-to-one-mmt',\n",
       "  'sha': 'aadfc3b9db11b773f823ad936d5d683c470e7683',\n",
       "  'lastModified': '2022-05-26T22:28:02.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'mbart',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'ar',\n",
       "   'cs',\n",
       "   'de',\n",
       "   'en',\n",
       "   'es',\n",
       "   'et',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'gu',\n",
       "   'hi',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'kk',\n",
       "   'ko',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'si',\n",
       "   'tr',\n",
       "   'vi',\n",
       "   'zh',\n",
       "   'af',\n",
       "   'az',\n",
       "   'bn',\n",
       "   'fa',\n",
       "   'he',\n",
       "   'hr',\n",
       "   'id',\n",
       "   'ka',\n",
       "   'km',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'th',\n",
       "   'tl',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'xh',\n",
       "   'gl',\n",
       "   'sl',\n",
       "   'arxiv:2008.00401',\n",
       "   'transformers',\n",
       "   'mbart-50',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['MBartForConditionalGeneration'],\n",
       "   'model_type': 'mbart',\n",
       "   'task_specific_params': {'translation_en_to_ro': {'decoder_start_token_id': 250020}}},\n",
       "  'id': 'facebook/mbart-large-50-many-to-one-mmt',\n",
       "  'downloads': 7960,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'ar',\n",
       "    'cs',\n",
       "    'de',\n",
       "    'en',\n",
       "    'es',\n",
       "    'et',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'gu',\n",
       "    'hi',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'kk',\n",
       "    'ko',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'si',\n",
       "    'tr',\n",
       "    'vi',\n",
       "    'zh',\n",
       "    'af',\n",
       "    'az',\n",
       "    'bn',\n",
       "    'fa',\n",
       "    'he',\n",
       "    'hr',\n",
       "    'id',\n",
       "    'ka',\n",
       "    'km',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'th',\n",
       "    'tl',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'xh',\n",
       "    'gl',\n",
       "    'sl'],\n",
       "   'tags': ['mbart-50']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ahotrod/electra_large_discriminator_squad2_512': {'modelId': 'ahotrod/electra_large_discriminator_squad2_512',\n",
       "  'sha': 'fe230d41e74248eceb366a91e4f8572f358f201c',\n",
       "  'lastModified': '2020-12-11T21:31:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'electra',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'ahotrod',\n",
       "  'config': {'architectures': ['ElectraForQuestionAnswering'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'ahotrod/electra_large_discriminator_squad2_512',\n",
       "  'downloads': 7903,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ctl/wav2vec2-large-xlsr-cantonese': {'modelId': 'ctl/wav2vec2-large-xlsr-cantonese',\n",
       "  'sha': '6a6119ab39ec2a0c8d16edfbf91db45334540315',\n",
       "  'lastModified': '2021-07-06T01:16:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'zh-HK',\n",
       "   'yue',\n",
       "   'dataset:common_voice',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'speech',\n",
       "   'xlsr-fine-tuning-week',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'ctl',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'ctl/wav2vec2-large-xlsr-cantonese',\n",
       "  'downloads': 7876,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': [{'name': 'wav2vec2-large-xlsr-cantonese',\n",
       "    'results': [{'task': {'name': 'Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Common Voice zh-HK',\n",
       "       'type': 'common_voice',\n",
       "       'args': 'zh-HK'},\n",
       "      'metrics': [{'name': 'Test CER', 'type': 'cer', 'value': 15.36}]}]}],\n",
       "  'cardData': {'language': ['zh-HK', 'yue'],\n",
       "   'datasets': ['common_voice'],\n",
       "   'metrics': ['cer'],\n",
       "   'tags': ['audio',\n",
       "    'automatic-speech-recognition',\n",
       "    'speech',\n",
       "    'xlsr-fine-tuning-week'],\n",
       "   'license': 'apache-2.0',\n",
       "   'model-index': [{'name': 'wav2vec2-large-xlsr-cantonese',\n",
       "     'results': [{'task': {'name': 'Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Common Voice zh-HK',\n",
       "        'type': 'common_voice',\n",
       "        'args': 'zh-HK'},\n",
       "       'metrics': [{'name': 'Test CER', 'type': 'cer', 'value': 15.36}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'sshleifer/tiny-marian-en-de': {'modelId': 'sshleifer/tiny-marian-en-de',\n",
       "  'sha': '7a6b5b34785930445aeb20a9a34543b72de6e267',\n",
       "  'lastModified': '2020-06-25T02:27:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'sshleifer/tiny-marian-en-de',\n",
       "  'downloads': 7872,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/blenderbot-1B-distill': {'modelId': 'facebook/blenderbot-1B-distill',\n",
       "  'sha': 'a10c0a628734b4cd46da57520bd35c7ca8965201',\n",
       "  'lastModified': '2021-06-17T12:02:20.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'blenderbot',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:blended_skill_talk',\n",
       "   'arxiv:1907.06616',\n",
       "   'transformers',\n",
       "   'convAI',\n",
       "   'conversational',\n",
       "   'facebook',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['BlenderbotForConditionalGeneration'],\n",
       "   'model_type': 'blenderbot'},\n",
       "  'id': 'facebook/blenderbot-1B-distill',\n",
       "  'downloads': 7829,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'thumbnail': None,\n",
       "   'tags': ['convAI', 'conversational', 'facebook'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['blended_skill_talk'],\n",
       "   'metrics': ['perplexity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/rag-sequence-nq': {'modelId': 'facebook/rag-sequence-nq',\n",
       "  'sha': 'c0d9c6ceda8a69c78091abb7aa734a97b75b89fd',\n",
       "  'lastModified': '2021-03-12T11:04:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'rag',\n",
       "   'en',\n",
       "   'dataset:wiki_dpr',\n",
       "   'arxiv:2005.11401',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['RagSequenceForGeneration'],\n",
       "   'model_type': 'rag'},\n",
       "  'id': 'facebook/rag-sequence-nq',\n",
       "  'downloads': 7783,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wiki_dpr'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png'},\n",
       "  'transformersInfo': {'auto_model': 'RagSequenceForGeneration',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Grossmend/rudialogpt3_medium_based_on_gpt2': {'modelId': 'Grossmend/rudialogpt3_medium_based_on_gpt2',\n",
       "  'sha': 'a2f8ac89182e36e352ea921de30cf2b0e9b30b89',\n",
       "  'lastModified': '2021-08-02T13:43:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'ru',\n",
       "   'transformers',\n",
       "   'convAI',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'Grossmend',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'], 'model_type': 'gpt2'},\n",
       "  'id': 'Grossmend/rudialogpt3_medium_based_on_gpt2',\n",
       "  'downloads': 7779,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru'],\n",
       "   'thumbnail': None,\n",
       "   'tags': ['convAI', 'conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/stsb-xlm-r-multilingual': {'modelId': 'sentence-transformers/stsb-xlm-r-multilingual',\n",
       "  'sha': 'bc1a68705f2e397259207e96349a36ccbc7e6493',\n",
       "  'lastModified': '2022-06-15T21:42:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['XLMRobertaModel'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'sentence-transformers/stsb-xlm-r-multilingual',\n",
       "  'downloads': 7631,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/blenderbot-90M': {'modelId': 'facebook/blenderbot-90M',\n",
       "  'sha': '3e5344952a74d2017762fa8428c45edd07f3dea7',\n",
       "  'lastModified': '2021-03-12T06:17:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'blenderbot-small',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:blended_skill_talk',\n",
       "   'arxiv:1907.06616',\n",
       "   'transformers',\n",
       "   'convAI',\n",
       "   'conversational',\n",
       "   'facebook',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['BlenderbotSmallForConditionalGeneration'],\n",
       "   'model_type': 'blenderbot-small'},\n",
       "  'id': 'facebook/blenderbot-90M',\n",
       "  'downloads': 7576,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'thumbnail': None,\n",
       "   'tags': ['convAI', 'conversational', 'facebook'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['blended_skill_talk'],\n",
       "   'metrics': ['perplexity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/mt5-xxl': {'modelId': 'google/mt5-xxl',\n",
       "  'sha': 'd4ac5e6d5125f8d30cba8763cd0ad71e5d34c17b',\n",
       "  'lastModified': '2022-05-27T15:06:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'mt5',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'az',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'ca',\n",
       "   'ceb',\n",
       "   'co',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'eo',\n",
       "   'es',\n",
       "   'et',\n",
       "   'eu',\n",
       "   'fa',\n",
       "   'fi',\n",
       "   'fil',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'haw',\n",
       "   'hi',\n",
       "   'hmn',\n",
       "   'ht',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'ig',\n",
       "   'is',\n",
       "   'it',\n",
       "   'iw',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'ku',\n",
       "   'ky',\n",
       "   'la',\n",
       "   'lb',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mi',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'mt',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'ny',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sd',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'sm',\n",
       "   'sn',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'st',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'tg',\n",
       "   'th',\n",
       "   'tr',\n",
       "   'uk',\n",
       "   'und',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'xh',\n",
       "   'yi',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'zu',\n",
       "   'dataset:mc4',\n",
       "   'arxiv:2010.11934',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 'mt5'},\n",
       "  'id': 'google/mt5-xxl',\n",
       "  'downloads': 7571,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'az',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'ca',\n",
       "    'ceb',\n",
       "    'co',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'eo',\n",
       "    'es',\n",
       "    'et',\n",
       "    'eu',\n",
       "    'fa',\n",
       "    'fi',\n",
       "    'fil',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'haw',\n",
       "    'hi',\n",
       "    'hmn',\n",
       "    'ht',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'ig',\n",
       "    'is',\n",
       "    'it',\n",
       "    'iw',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'ku',\n",
       "    'ky',\n",
       "    'la',\n",
       "    'lb',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mi',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'mt',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'ny',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sd',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'sm',\n",
       "    'sn',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'st',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'tg',\n",
       "    'th',\n",
       "    'tr',\n",
       "    'uk',\n",
       "    'und',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'xh',\n",
       "    'yi',\n",
       "    'yo',\n",
       "    'zh',\n",
       "    'zu'],\n",
       "   'datasets': ['mc4'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'kamalkraj/deberta-v2-xlarge': {'modelId': 'kamalkraj/deberta-v2-xlarge',\n",
       "  'sha': 'ee3a2dc756e780f6bfeb1e251d84bc7d7dc2da9c',\n",
       "  'lastModified': '2021-08-13T08:44:43.000Z',\n",
       "  'tags': ['tf',\n",
       "   'deberta-v2',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'deberta',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'kamalkraj',\n",
       "  'config': {'architectures': ['DebertaV2Model'], 'model_type': 'deberta-v2'},\n",
       "  'id': 'kamalkraj/deberta-v2-xlarge',\n",
       "  'downloads': 7540,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': 'deberta',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/DialogRPT-human-vs-rand': {'modelId': 'microsoft/DialogRPT-human-vs-rand',\n",
       "  'sha': '7206b425c2c016dd5533e2a99e665ba3546e5ce0',\n",
       "  'lastModified': '2021-05-23T09:18:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-classification',\n",
       "   'arxiv:2009.06978',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['GPT2ForSequenceClassification'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'microsoft/DialogRPT-human-vs-rand',\n",
       "  'downloads': 7522,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'UBC-NLP/MARBERT': {'modelId': 'UBC-NLP/MARBERT',\n",
       "  'sha': 'ef5bf8d54e104731fc045d5c76e72af8a23988cf',\n",
       "  'lastModified': '2022-01-19T20:37:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ar',\n",
       "   'transformers',\n",
       "   'Arabic BERT',\n",
       "   'MSA',\n",
       "   'Twitter',\n",
       "   'Masked Langauge Model',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'UBC-NLP',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'UBC-NLP/MARBERT',\n",
       "  'downloads': 7506,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'اللغة العربية هي لغة [MASK].'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ar'],\n",
       "   'tags': ['Arabic BERT', 'MSA', 'Twitter', 'Masked Langauge Model'],\n",
       "   'widget': [{'text': 'اللغة العربية هي لغة [MASK].'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/t5-xl-lm-adapt': {'modelId': 'google/t5-xl-lm-adapt',\n",
       "  'sha': '6e644c517fc8a8a79a657e528be5c60777d87652',\n",
       "  'lastModified': '2021-11-01T13:59:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:c4',\n",
       "   'arxiv:2002.05202',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   't5-lm-adapt',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/t5-xl-lm-adapt',\n",
       "  'downloads': 7412,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['c4'],\n",
       "   'tags': ['t5-lm-adapt'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wav2vec2-xls-r-300m': {'modelId': 'facebook/wav2vec2-xls-r-300m',\n",
       "  'sha': 'e842f378fdbdb09aabc11d87c52f26b8f2dde333',\n",
       "  'lastModified': '2021-11-18T16:32:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wav2vec2',\n",
       "   'pretraining',\n",
       "   'multilingual',\n",
       "   'dataset:common_voice',\n",
       "   'dataset:multilingual_librispeech',\n",
       "   'arxiv:2111.09296',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'xls_r',\n",
       "   'xls_r_pretrained',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Wav2Vec2ForPreTraining'],\n",
       "   'model_type': 'wav2vec2'},\n",
       "  'id': 'facebook/wav2vec2-xls-r-300m',\n",
       "  'downloads': 7341,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 22,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'datasets': ['common_voice', 'multilingual_librispeech'],\n",
       "   'tags': ['speech', 'xls_r', 'xls_r_pretrained'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'julien-c/dummy-diff-tokenizer': {'modelId': 'julien-c/dummy-diff-tokenizer',\n",
       "  'sha': '8b54c50bfd24739488683452f24d4471f5d75a21',\n",
       "  'lastModified': '2021-05-20T17:30:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'julien-c',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'julien-c/dummy-diff-tokenizer',\n",
       "  'downloads': 7259,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KoboldAI/GPT-J-6B-Skein': {'modelId': 'KoboldAI/GPT-J-6B-Skein',\n",
       "  'sha': '95a7ea75328cc8e117fdbf967b9fa12f49d1d24c',\n",
       "  'lastModified': '2022-03-14T22:44:49.000Z',\n",
       "  'tags': ['pytorch', 'gptj', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'KoboldAI',\n",
       "  'config': {'architectures': ['GPTJForCausalLM'],\n",
       "   'model_type': 'gptj',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'temperature': 1,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'KoboldAI/GPT-J-6B-Skein',\n",
       "  'downloads': 7252,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/vit-mae-base': {'modelId': 'facebook/vit-mae-base',\n",
       "  'sha': '87dd4faac12498cde93a176406329112584c0413',\n",
       "  'lastModified': '2022-03-29T16:18:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'vit_mae',\n",
       "   'pretraining',\n",
       "   'dataset:imagenet-1k',\n",
       "   'arxiv:2111.06377',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['ViTMAEForPreTraining'],\n",
       "   'model_type': 'vit_mae'},\n",
       "  'id': 'facebook/vit-mae-base',\n",
       "  'downloads': 7214,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['vision'],\n",
       "   'datasets': ['imagenet-1k']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn': {'modelId': 'jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn',\n",
       "  'sha': '5891ca7549f9dfe2ddae9a3c6cc915f17b214502',\n",
       "  'lastModified': '2022-06-22T17:00:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'zh-CN',\n",
       "   'dataset:common_voice',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'speech',\n",
       "   'xlsr-fine-tuning-week',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'jonatasgrosman',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn',\n",
       "  'downloads': 7209,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': [{'name': 'XLSR Wav2Vec2 Chinese (zh-CN) by Jonatas Grosman',\n",
       "    'results': [{'task': {'name': 'Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Common Voice zh-CN',\n",
       "       'type': 'common_voice',\n",
       "       'args': 'zh-CN'},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 82.37},\n",
       "       {'name': 'Test CER', 'type': 'cer', 'value': 19.03}]}]}],\n",
       "  'cardData': {'language': 'zh-CN',\n",
       "   'datasets': ['common_voice'],\n",
       "   'metrics': ['wer', 'cer'],\n",
       "   'tags': ['audio',\n",
       "    'automatic-speech-recognition',\n",
       "    'speech',\n",
       "    'xlsr-fine-tuning-week'],\n",
       "   'license': 'apache-2.0',\n",
       "   'model-index': [{'name': 'XLSR Wav2Vec2 Chinese (zh-CN) by Jonatas Grosman',\n",
       "     'results': [{'task': {'name': 'Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Common Voice zh-CN',\n",
       "        'type': 'common_voice',\n",
       "        'args': 'zh-CN'},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 82.37},\n",
       "        {'name': 'Test CER', 'type': 'cer', 'value': 19.03}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'IDEA-CCNL/Erlangshen-Roberta-330M-Similarity': {'modelId': 'IDEA-CCNL/Erlangshen-Roberta-330M-Similarity',\n",
       "  'sha': '8ed6c66504212201bd8f542a2467741baef8a133',\n",
       "  'lastModified': '2022-05-12T09:49:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'NLU',\n",
       "   'NLI',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'IDEA-CCNL',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'IDEA-CCNL/Erlangshen-Roberta-330M-Similarity',\n",
       "  'downloads': 7200,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '今天心情不好[SEP]今天很开心'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['bert', 'NLU', 'NLI'],\n",
       "   'inference': True,\n",
       "   'widget': [{'text': '今天心情不好[SEP]今天很开心'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-ur-en': {'modelId': 'Helsinki-NLP/opus-mt-ur-en',\n",
       "  'sha': 'c803d32b6f7a3a7a8cb1ba91d2947de0009f8cdc',\n",
       "  'lastModified': '2020-08-21T14:42:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'ur',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-ur-en',\n",
       "  'downloads': 7169,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ur', 'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'aubmindlab/bert-base-arabertv02-twitter': {'modelId': 'aubmindlab/bert-base-arabertv02-twitter',\n",
       "  'sha': '14bddd56ee5b02d1d92436ca14934687452a96ea',\n",
       "  'lastModified': '2021-10-16T22:10:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ar',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:OSIAN',\n",
       "   'dataset:1.5B Arabic Corpus',\n",
       "   'dataset:OSCAR Arabic Unshuffled',\n",
       "   'dataset:Twitter',\n",
       "   'arxiv:2003.00104',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'aubmindlab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'aubmindlab/bert-base-arabertv02-twitter',\n",
       "  'downloads': 7158,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': ' عاصمة لبنان هي [MASK] .'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ar',\n",
       "   'datasets': ['wikipedia',\n",
       "    'OSIAN',\n",
       "    '1.5B Arabic Corpus',\n",
       "    'OSCAR Arabic Unshuffled',\n",
       "    'Twitter'],\n",
       "   'widget': [{'text': ' عاصمة لبنان هي [MASK] .'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Geotrend/bert-base-th-cased': {'modelId': 'Geotrend/bert-base-th-cased',\n",
       "  'sha': '607c7b08e70c5ec7b2e3e013f394f0743dd39ca3',\n",
       "  'lastModified': '2021-05-18T20:11:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'th',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Geotrend',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'Geotrend/bert-base-th-cased',\n",
       "  'downloads': 7145,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'th',\n",
       "   'datasets': 'wikipedia',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'jonatasgrosman/wav2vec2-large-xlsr-53-german': {'modelId': 'jonatasgrosman/wav2vec2-large-xlsr-53-german',\n",
       "  'sha': '1e8033537f45651154ca94b1295817ffd11d5942',\n",
       "  'lastModified': '2022-06-22T17:05:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'de',\n",
       "   'dataset:common_voice',\n",
       "   'dataset:mozilla-foundation/common_voice_6_0',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'hf-asr-leaderboard',\n",
       "   'mozilla-foundation/common_voice_6_0',\n",
       "   'robust-speech-event',\n",
       "   'speech',\n",
       "   'xlsr-fine-tuning-week',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'jonatasgrosman',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'jonatasgrosman/wav2vec2-large-xlsr-53-german',\n",
       "  'downloads': 7139,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': [{'name': 'XLSR Wav2Vec2 German by Jonatas Grosman',\n",
       "    'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Common Voice de',\n",
       "       'type': 'common_voice',\n",
       "       'args': 'de'},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 12.06},\n",
       "       {'name': 'Test CER', 'type': 'cer', 'value': 2.92},\n",
       "       {'name': 'Test WER (+LM)', 'type': 'wer', 'value': 8.74},\n",
       "       {'name': 'Test CER (+LM)', 'type': 'cer', 'value': 2.28}]},\n",
       "     {'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Robust Speech Event - Dev Data',\n",
       "       'type': 'speech-recognition-community-v2/dev_data',\n",
       "       'args': 'de'},\n",
       "      'metrics': [{'name': 'Dev WER', 'type': 'wer', 'value': 32.75},\n",
       "       {'name': 'Dev CER', 'type': 'cer', 'value': 13.64},\n",
       "       {'name': 'Dev WER (+LM)', 'type': 'wer', 'value': 26.6},\n",
       "       {'name': 'Dev CER (+LM)', 'type': 'cer', 'value': 12.58}]}]}],\n",
       "  'cardData': {'language': 'de',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['common_voice', 'mozilla-foundation/common_voice_6_0'],\n",
       "   'metrics': ['wer', 'cer'],\n",
       "   'tags': ['audio',\n",
       "    'automatic-speech-recognition',\n",
       "    'de',\n",
       "    'hf-asr-leaderboard',\n",
       "    'mozilla-foundation/common_voice_6_0',\n",
       "    'robust-speech-event',\n",
       "    'speech',\n",
       "    'xlsr-fine-tuning-week'],\n",
       "   'model-index': [{'name': 'XLSR Wav2Vec2 German by Jonatas Grosman',\n",
       "     'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Common Voice de',\n",
       "        'type': 'common_voice',\n",
       "        'args': 'de'},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 12.06},\n",
       "        {'name': 'Test CER', 'type': 'cer', 'value': 2.92},\n",
       "        {'name': 'Test WER (+LM)', 'type': 'wer', 'value': 8.74},\n",
       "        {'name': 'Test CER (+LM)', 'type': 'cer', 'value': 2.28}]},\n",
       "      {'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Robust Speech Event - Dev Data',\n",
       "        'type': 'speech-recognition-community-v2/dev_data',\n",
       "        'args': 'de'},\n",
       "       'metrics': [{'name': 'Dev WER', 'type': 'wer', 'value': 32.75},\n",
       "        {'name': 'Dev CER', 'type': 'cer', 'value': 13.64},\n",
       "        {'name': 'Dev WER (+LM)', 'type': 'wer', 'value': 26.6},\n",
       "        {'name': 'Dev CER (+LM)', 'type': 'cer', 'value': 12.58}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'unicamp-dl/translation-en-pt-t5': {'modelId': 'unicamp-dl/translation-en-pt-t5',\n",
       "  'sha': '8418d7e9b1837687137af06624cb3596b45c9343',\n",
       "  'lastModified': '2021-10-11T03:47:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'pt',\n",
       "   'dataset:EMEA',\n",
       "   'dataset:ParaCrawl 99k',\n",
       "   'dataset:CAPES',\n",
       "   'dataset:Scielo',\n",
       "   'dataset:JRC-Acquis',\n",
       "   'dataset:Biomedical Domain Corpora',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'unicamp-dl',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'unicamp-dl/translation-en-pt-t5',\n",
       "  'downloads': 7132,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'pt'],\n",
       "   'datasets': ['EMEA',\n",
       "    'ParaCrawl 99k',\n",
       "    'CAPES',\n",
       "    'Scielo',\n",
       "    'JRC-Acquis',\n",
       "    'Biomedical Domain Corpora'],\n",
       "   'tags': ['translation'],\n",
       "   'metrics': ['bleu']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large': {'modelId': 'nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large',\n",
       "  'sha': 'd828558d1a570cbbb5e62a8dbf85c8f18bf7982a',\n",
       "  'lastModified': '2021-06-20T19:03:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'nreimers',\n",
       "  'config': {'architectures': ['XLMRobertaForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'nreimers/mMiniLMv2-L12-H384-distilled-from-XLMR-Large',\n",
       "  'downloads': 7119,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'charsiu/g2p_multilingual_byT5_small': {'modelId': 'charsiu/g2p_multilingual_byT5_small',\n",
       "  'sha': '834df67c125a811e1a60fbf9f0f39503115437ea',\n",
       "  'lastModified': '2022-05-19T05:02:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'charsiu',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'charsiu/g2p_multilingual_byT5_small',\n",
       "  'downloads': 7070,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allegro/herbert-large-cased': {'modelId': 'allegro/herbert-large-cased',\n",
       "  'sha': '8d0fa3bc0566c3a332bec0d471c8d8c37b5cbb90',\n",
       "  'lastModified': '2022-06-26T14:18:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'pl',\n",
       "   'transformers',\n",
       "   'herbert',\n",
       "   'license:cc-by-4.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'allegro',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'allegro/herbert-large-cased',\n",
       "  'downloads': 7042,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'pl', 'tags': ['herbert'], 'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-en-it': {'modelId': 'Helsinki-NLP/opus-mt-en-it',\n",
       "  'sha': '4c56f4ddc9fcfccec7799f5cef4d90f7c99dd658',\n",
       "  'lastModified': '2021-09-09T21:36:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'it',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-it',\n",
       "  'downloads': 7040,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/data2vec-text-base': {'modelId': 'facebook/data2vec-text-base',\n",
       "  'sha': 'bd0db19c3500ee7a0b626791db67fa6e9fda9a0b',\n",
       "  'lastModified': '2022-04-18T16:03:20.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'data2vec-text',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:2202.03555',\n",
       "   'arxiv:1806.02847',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Data2VecTextModel'],\n",
       "   'model_type': 'data2vec-text'},\n",
       "  'id': 'facebook/data2vec-text-base',\n",
       "  'downloads': 7037,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'lidiya/bart-large-xsum-samsum': {'modelId': 'lidiya/bart-large-xsum-samsum',\n",
       "  'sha': 'ea93258a6600f79d851b306864224595192edd51',\n",
       "  'lastModified': '2021-05-26T22:18:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:samsum',\n",
       "   'transformers',\n",
       "   'seq2seq',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'lidiya',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {}},\n",
       "  'id': 'lidiya/bart-large-xsum-samsum',\n",
       "  'downloads': 7029,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': \"Hannah: Hey, do you have Betty's number?\\nAmanda: Lemme check\\nAmanda: Sorry, can't find it.\\nAmanda: Ask Larry\\nAmanda: He called her last time we were at the park together\\nHannah: I don't know him well\\nAmanda: Don't be shy, he's very nice\\nHannah: If you say so..\\nHannah: I'd rather you texted him\\nAmanda: Just text him 🙂\\nHannah: Urgh.. Alright\\nHannah: Bye\\nAmanda: Bye bye\\n\"}],\n",
       "  'likes': 8,\n",
       "  'model-index': [{'name': 'bart-large-xsum-samsum',\n",
       "    'results': [{'task': {'name': 'Abstractive Text Summarization',\n",
       "       'type': 'abstractive-text-summarization'},\n",
       "      'dataset': {'name': 'SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization',\n",
       "       'type': 'samsum'},\n",
       "      'metrics': [{'name': 'Validation ROGUE-1',\n",
       "        'type': 'rogue-1',\n",
       "        'value': 54.3921},\n",
       "       {'name': 'Validation ROGUE-2', 'type': 'rogue-2', 'value': 29.8078},\n",
       "       {'name': 'Validation ROGUE-L', 'type': 'rogue-l', 'value': 45.1543},\n",
       "       {'name': 'Test ROGUE-1', 'type': 'rogue-1', 'value': 53.3059},\n",
       "       {'name': 'Test ROGUE-2', 'type': 'rogue-2', 'value': 28.355},\n",
       "       {'name': 'Test ROGUE-L', 'type': 'rogue-l', 'value': 44.0953}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['bart', 'seq2seq', 'summarization'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['samsum'],\n",
       "   'widget': [{'text': \"Hannah: Hey, do you have Betty's number?\\nAmanda: Lemme check\\nAmanda: Sorry, can't find it.\\nAmanda: Ask Larry\\nAmanda: He called her last time we were at the park together\\nHannah: I don't know him well\\nAmanda: Don't be shy, he's very nice\\nHannah: If you say so..\\nHannah: I'd rather you texted him\\nAmanda: Just text him 🙂\\nHannah: Urgh.. Alright\\nHannah: Bye\\nAmanda: Bye bye\\n\"}],\n",
       "   'model-index': [{'name': 'bart-large-xsum-samsum',\n",
       "     'results': [{'task': {'name': 'Abstractive Text Summarization',\n",
       "        'type': 'abstractive-text-summarization'},\n",
       "       'dataset': {'name': 'SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization',\n",
       "        'type': 'samsum'},\n",
       "       'metrics': [{'name': 'Validation ROGUE-1',\n",
       "         'type': 'rogue-1',\n",
       "         'value': 54.3921},\n",
       "        {'name': 'Validation ROGUE-2', 'type': 'rogue-2', 'value': 29.8078},\n",
       "        {'name': 'Validation ROGUE-L', 'type': 'rogue-l', 'value': 45.1543},\n",
       "        {'name': 'Test ROGUE-1', 'type': 'rogue-1', 'value': 53.3059},\n",
       "        {'name': 'Test ROGUE-2', 'type': 'rogue-2', 'value': 28.355},\n",
       "        {'name': 'Test ROGUE-L', 'type': 'rogue-l', 'value': 44.0953}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dkleczek/bert-base-polish-cased-v1': {'modelId': 'dkleczek/bert-base-polish-cased-v1',\n",
       "  'sha': 'fed744e81ebd16cf099b5c64c40688bc3e6ace67',\n",
       "  'lastModified': '2021-05-19T15:54:20.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'pretraining', 'pl', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dkleczek',\n",
       "  'config': {'architectures': ['BertForPreTraining'], 'model_type': 'bert'},\n",
       "  'id': 'dkleczek/bert-base-polish-cased-v1',\n",
       "  'downloads': 7021,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'pl',\n",
       "   'thumbnail': 'https://raw.githubusercontent.com/kldarek/polbert/master/img/polbert.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'klue/roberta-large': {'modelId': 'klue/roberta-large',\n",
       "  'sha': '5193b95701189160c45d02a1033a4ea55bdbe259',\n",
       "  'lastModified': '2021-10-20T16:13:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'ko',\n",
       "   'arxiv:2105.09680',\n",
       "   'transformers',\n",
       "   'korean',\n",
       "   'klue',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'klue',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'klue/roberta-large',\n",
       "  'downloads': 6965,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '대한민국의 수도는 [MASK] 입니다.'}],\n",
       "  'likes': 14,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ko',\n",
       "   'tags': ['korean', 'klue'],\n",
       "   'mask_token': '[MASK]',\n",
       "   'widget': [{'text': '대한민국의 수도는 [MASK] 입니다.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'rinna/japanese-gpt2-medium': {'modelId': 'rinna/japanese-gpt2-medium',\n",
       "  'sha': 'f464b76739c884d8b0479a0a7705b7fa71c3fd5a',\n",
       "  'lastModified': '2021-08-23T03:20:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'ja',\n",
       "   'dataset:cc100',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'japanese',\n",
       "   'lm',\n",
       "   'nlp',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'rinna',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'rinna/japanese-gpt2-medium',\n",
       "  'downloads': 6895,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '生命、宇宙、そして万物についての究極の疑問の答えは'}],\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'thumbnail': 'https://github.com/rinnakk/japanese-gpt2/blob/master/rinna.png',\n",
       "   'tags': ['ja', 'japanese', 'gpt2', 'text-generation', 'lm', 'nlp'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['cc100', 'wikipedia'],\n",
       "   'widget': [{'text': '生命、宇宙、そして万物についての究極の疑問の答えは'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allenai/biomed_roberta_base': {'modelId': 'allenai/biomed_roberta_base',\n",
       "  'sha': '6209646a5f79bbd383f8193d70e88ab00ae779f8',\n",
       "  'lastModified': '2021-05-20T13:00:31.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'roberta', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'model_type': 'roberta'},\n",
       "  'id': 'allenai/biomed_roberta_base',\n",
       "  'downloads': 6883,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/allenai.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'TheGoldenToaster/DialoGPT-medium-Bot': {'modelId': 'TheGoldenToaster/DialoGPT-medium-Bot',\n",
       "  'sha': 'b9e2e669356dfda8108ccdf76d4db16cef38f227',\n",
       "  'lastModified': '2022-04-04T21:58:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'TheGoldenToaster',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'TheGoldenToaster/DialoGPT-medium-Bot',\n",
       "  'downloads': 6873,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/beit-base-patch16-224-pt22k-ft22k': {'modelId': 'microsoft/beit-base-patch16-224-pt22k-ft22k',\n",
       "  'sha': 'e10357ec82ca7a0de830c20ffc715b6c33cde963',\n",
       "  'lastModified': '2022-01-28T10:17:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'beit',\n",
       "   'image-classification',\n",
       "   'dataset:imagenet',\n",
       "   'dataset:imagenet-21k',\n",
       "   'arxiv:2106.08254',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['BeitForImageClassification'],\n",
       "   'model_type': 'beit'},\n",
       "  'id': 'microsoft/beit-base-patch16-224-pt22k-ft22k',\n",
       "  'downloads': 6857,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['image-classification', 'vision'],\n",
       "   'datasets': ['imagenet', 'imagenet-21k']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageClassification',\n",
       "   'pipeline_tag': 'image-classification',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'sshleifer/distilbart-xsum-1-1': {'modelId': 'sshleifer/distilbart-xsum-1-1',\n",
       "  'sha': '891968fcbb0e421075cc2c3dfc8da8d4b24d54a4',\n",
       "  'lastModified': '2021-06-14T07:53:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:cnn_dailymail',\n",
       "   'dataset:xsum',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {}},\n",
       "  'id': 'sshleifer/distilbart-xsum-1-1',\n",
       "  'downloads': 6853,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['summarization'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['cnn_dailymail', 'xsum'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/distilbart_medium.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/data2vec-vision-base': {'modelId': 'facebook/data2vec-vision-base',\n",
       "  'sha': '72a7bdadab41d0e9a2c8d6887b9f8a50eebb8e0f',\n",
       "  'lastModified': '2022-05-03T15:52:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'data2vec-vision',\n",
       "   'feature-extraction',\n",
       "   'dataset:imagenet',\n",
       "   'dataset:imagenet-1k',\n",
       "   'arxiv:2202.03555',\n",
       "   'arxiv:2106.08254',\n",
       "   'transformers',\n",
       "   'image-classification',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Data2VecVisionModel'],\n",
       "   'model_type': 'data2vec-vision'},\n",
       "  'id': 'facebook/data2vec-vision-base',\n",
       "  'downloads': 6835,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['image-classification', 'vision'],\n",
       "   'datasets': ['imagenet', 'imagenet-1k']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'Helsinki-NLP/opus-mt-gmq-en': {'modelId': 'Helsinki-NLP/opus-mt-gmq-en',\n",
       "  'sha': '74efbe7477ba9acf0bcc143fcad9f5280db2fab4',\n",
       "  'lastModified': '2021-01-18T08:52:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'da',\n",
       "   'nb',\n",
       "   'sv',\n",
       "   'is',\n",
       "   'nn',\n",
       "   'fo',\n",
       "   'gmq',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-gmq-en',\n",
       "  'downloads': 6795,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['da', 'nb', 'sv', 'is', 'nn', 'fo', 'gmq', 'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'kykim/bert-kor-base': {'modelId': 'kykim/bert-kor-base',\n",
       "  'sha': '1779cc0982ada0216dd6de0dd4e86fb78201926d',\n",
       "  'lastModified': '2021-05-19T21:17:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ko',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'kykim',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'kykim/bert-kor-base',\n",
       "  'downloads': 6787,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ko'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nvidia/segformer-b0-finetuned-ade-512-512': {'modelId': 'nvidia/segformer-b0-finetuned-ade-512-512',\n",
       "  'sha': 'f5533fed06b606a60402e8164ff745c02c8bc943',\n",
       "  'lastModified': '2022-02-21T20:07:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'segformer',\n",
       "   'dataset:scene_parse_150',\n",
       "   'arxiv:2105.15203',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'image-segmentation',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-segmentation',\n",
       "  'private': False,\n",
       "  'author': 'nvidia',\n",
       "  'config': {'architectures': ['SegformerForSemanticSegmentation'],\n",
       "   'model_type': 'segformer'},\n",
       "  'id': 'nvidia/segformer-b0-finetuned-ade-512-512',\n",
       "  'downloads': 6730,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'src': 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg',\n",
       "    'example_title': 'House'},\n",
       "   {'src': 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000002.jpg',\n",
       "    'example_title': 'Castle'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['vision', 'image-segmentation'],\n",
       "   'datasets': ['scene_parse_150'],\n",
       "   'widget': [{'src': 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg',\n",
       "     'example_title': 'House'},\n",
       "    {'src': 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000002.jpg',\n",
       "     'example_title': 'Castle'}]},\n",
       "  'transformersInfo': {'auto_model': 'SegformerForSemanticSegmentation',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'Geotrend/bert-base-ru-cased': {'modelId': 'Geotrend/bert-base-ru-cased',\n",
       "  'sha': '2810f3d4fa13f6a045fcda6a6e91bfb085e60396',\n",
       "  'lastModified': '2021-05-18T20:09:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ru',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Geotrend',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'Geotrend/bert-base-ru-cased',\n",
       "  'downloads': 6692,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Меня зовут [MASK] и я инженер живущий в Нью-Йорке.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ru',\n",
       "   'datasets': 'wikipedia',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-bert-flax-only': {'modelId': 'hf-internal-testing/tiny-bert-flax-only',\n",
       "  'sha': 'c82d9a500998a5d90f393d8c72cadf1829eb0d51',\n",
       "  'lastModified': '2022-01-21T16:19:45.000Z',\n",
       "  'tags': ['jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'hf-internal-testing/tiny-bert-flax-only',\n",
       "  'downloads': 6640,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/hubert-base-ls960': {'modelId': 'facebook/hubert-base-ls960',\n",
       "  'sha': 'dba3bb02fda4248b6e082697eee756de8fe8aa8a',\n",
       "  'lastModified': '2021-11-05T12:43:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'hubert',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'dataset:librispeech_asr',\n",
       "   'arxiv:2106.07447',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['HubertModel'], 'model_type': 'hubert'},\n",
       "  'id': 'facebook/hubert-base-ls960',\n",
       "  'downloads': 6635,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['librispeech_asr'],\n",
       "   'tags': ['speech'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'eleldar/language-detection': {'modelId': 'eleldar/language-detection',\n",
       "  'sha': '735cc09f01b8b4d831228e0fa7640464e56a022e',\n",
       "  'lastModified': '2022-05-24T10:06:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'text-classification',\n",
       "   'arxiv:1911.02116',\n",
       "   'transformers',\n",
       "   'generated_from_trainer',\n",
       "   'license:mit',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'eleldar',\n",
       "  'config': {'architectures': ['XLMRobertaForSequenceClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'eleldar/language-detection',\n",
       "  'downloads': 6635,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 1,\n",
       "  'model-index': [{'name': 'xlm-roberta-base-language-detection',\n",
       "    'results': []}],\n",
       "  'cardData': {'license': 'mit',\n",
       "   'tags': ['generated_from_trainer'],\n",
       "   'metrics': ['accuracy', 'f1'],\n",
       "   'model-index': [{'name': 'xlm-roberta-base-language-detection',\n",
       "     'results': []}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flaubert/flaubert_small_cased': {'modelId': 'flaubert/flaubert_small_cased',\n",
       "  'sha': '21a2d6f46294ad07a0b692d96af443990c07f790',\n",
       "  'lastModified': '2021-05-19T16:56:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'flaubert',\n",
       "   'fill-mask',\n",
       "   'fr',\n",
       "   'dataset:flaubert',\n",
       "   'transformers',\n",
       "   'bert',\n",
       "   'language-model',\n",
       "   'flue',\n",
       "   'french',\n",
       "   'flaubert-small',\n",
       "   'cased',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'flaubert',\n",
       "  'config': {'architectures': ['FlaubertWithLMHeadModel'],\n",
       "   'model_type': 'flaubert'},\n",
       "  'id': 'flaubert/flaubert_small_cased',\n",
       "  'downloads': 6574,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<special1>',\n",
       "  'widgetData': [{'text': 'Paris est la <special1> de la France.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['flaubert'],\n",
       "   'metrics': ['flue'],\n",
       "   'tags': ['bert',\n",
       "    'language-model',\n",
       "    'flaubert',\n",
       "    'flue',\n",
       "    'french',\n",
       "    'flaubert-small',\n",
       "    'cased']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis': {'modelId': 'mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis',\n",
       "  'sha': '0392a911472f7fa3db4ebacee570be79b16187f2',\n",
       "  'lastModified': '2021-09-16T18:43:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'dataset:financial_phrasebank',\n",
       "   'transformers',\n",
       "   'generated_from_trainer',\n",
       "   'financial',\n",
       "   'stocks',\n",
       "   'sentiment',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis',\n",
       "  'downloads': 6572,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Operating profit totaled EUR 9.4 mn , down from EUR 11.7 mn in 2004 .'}],\n",
       "  'likes': 17,\n",
       "  'model-index': [{'name': 'distilRoberta-financial-sentiment',\n",
       "    'results': [{'task': {'name': 'Text Classification',\n",
       "       'type': 'text-classification'},\n",
       "      'dataset': {'name': 'financial_phrasebank',\n",
       "       'type': 'financial_phrasebank',\n",
       "       'args': 'sentences_allagree'},\n",
       "      'metrics': [{'name': 'Accuracy',\n",
       "        'type': 'accuracy',\n",
       "        'value': 0.9823008849557522}]}]}],\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['generated_from_trainer', 'financial', 'stocks', 'sentiment'],\n",
       "   'widget': [{'text': 'Operating profit totaled EUR 9.4 mn , down from EUR 11.7 mn in 2004 .'}],\n",
       "   'datasets': ['financial_phrasebank'],\n",
       "   'metrics': ['accuracy'],\n",
       "   'model-index': [{'name': 'distilRoberta-financial-sentiment',\n",
       "     'results': [{'task': {'name': 'Text Classification',\n",
       "        'type': 'text-classification'},\n",
       "       'dataset': {'name': 'financial_phrasebank',\n",
       "        'type': 'financial_phrasebank',\n",
       "        'args': 'sentences_allagree'},\n",
       "       'metrics': [{'name': 'Accuracy',\n",
       "         'type': 'accuracy',\n",
       "         'value': 0.9823008849557522}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/cocolm-large': {'modelId': 'microsoft/cocolm-large',\n",
       "  'sha': '6947b62b7ca98d5f883c291b7a32e9b2d54130ef',\n",
       "  'lastModified': '2022-02-07T22:49:54.000Z',\n",
       "  'tags': ['pytorch', 'arxiv:2102.08473', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {},\n",
       "  'id': 'microsoft/cocolm-large',\n",
       "  'downloads': 6557,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'vblagoje/bert-english-uncased-finetuned-pos': {'modelId': 'vblagoje/bert-english-uncased-finetuned-pos',\n",
       "  'sha': '46ec120264b121e8d92bef19b45c107d06d2cb99',\n",
       "  'lastModified': '2021-05-20T08:51:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'vblagoje',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'vblagoje/bert-english-uncased-finetuned-pos',\n",
       "  'downloads': 6542,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'AmbricJohnson5888/death': {'modelId': 'AmbricJohnson5888/death',\n",
       "  'sha': '68ca65a4f20454f7ea435dba297a0d959cd67183',\n",
       "  'lastModified': '2022-04-09T02:19:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'AmbricJohnson5888',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'AmbricJohnson5888/death',\n",
       "  'downloads': 6523,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cointegrated/rut5-base-paraphraser': {'modelId': 'cointegrated/rut5-base-paraphraser',\n",
       "  'sha': '89213d06450b722514e23ba55ae7c16a2203a3b8',\n",
       "  'lastModified': '2022-02-08T13:06:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'ru',\n",
       "   'dataset:cointegrated/ru-paraphrase-NMT-Leipzig',\n",
       "   'transformers',\n",
       "   'russian',\n",
       "   'paraphrasing',\n",
       "   'paraphraser',\n",
       "   'paraphrase',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'cointegrated',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'cointegrated/rut5-base-paraphraser',\n",
       "  'downloads': 6480,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Каждый охотник желает знать, где сидит фазан.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru'],\n",
       "   'tags': ['russian', 'paraphrasing', 'paraphraser', 'paraphrase'],\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': 'Каждый охотник желает знать, где сидит фазан.'}],\n",
       "   'datasets': ['cointegrated/ru-paraphrase-NMT-Leipzig']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dmis-lab/biobert-large-cased-v1.1': {'modelId': 'dmis-lab/biobert-large-cased-v1.1',\n",
       "  'sha': 'c6775648fdc33f369c4342679bcf0f2691e08b3c',\n",
       "  'lastModified': '2020-10-14T06:19:39.000Z',\n",
       "  'tags': ['pytorch', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dmis-lab',\n",
       "  'config': {},\n",
       "  'id': 'dmis-lab/biobert-large-cased-v1.1',\n",
       "  'downloads': 6434,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'facebook/s2t-small-mustc-en-fr-st': {'modelId': 'facebook/s2t-small-mustc-en-fr-st',\n",
       "  'sha': '90bd87c9a36fc51f1acb419760563551671e3b4e',\n",
       "  'lastModified': '2022-02-07T14:44:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'speech_to_text',\n",
       "   'automatic-speech-recognition',\n",
       "   'en',\n",
       "   'fr',\n",
       "   'dataset:mustc',\n",
       "   'arxiv:2010.05171',\n",
       "   'arxiv:1904.08779',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'speech-translation',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Speech2TextForConditionalGeneration'],\n",
       "   'model_type': 'speech_to_text'},\n",
       "  'id': 'facebook/s2t-small-mustc-en-fr-st',\n",
       "  'downloads': 6413,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'example_title': 'Librispeech sample 1',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/sample1.flac'},\n",
       "   {'example_title': 'Librispeech sample 2',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/sample2.flac'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'fr'],\n",
       "   'datasets': ['mustc'],\n",
       "   'tags': ['audio', 'speech-translation', 'automatic-speech-recognition'],\n",
       "   'license': 'mit',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'widget': [{'example_title': 'Librispeech sample 1',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/sample1.flac'},\n",
       "    {'example_title': 'Librispeech sample 2',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/sample2.flac'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSpeechSeq2Seq',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'google/pegasus-pubmed': {'modelId': 'google/pegasus-pubmed',\n",
       "  'sha': '27396b90fefc6b2f8365728fb1e23963d7feca2d',\n",
       "  'lastModified': '2020-10-22T16:33:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'pegasus',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:1912.08777',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['PegasusForConditionalGeneration'],\n",
       "   'model_type': 'pegasus'},\n",
       "  'id': 'google/pegasus-pubmed',\n",
       "  'downloads': 6400,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'tags': ['summarization']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-roa-en': {'modelId': 'Helsinki-NLP/opus-mt-roa-en',\n",
       "  'sha': '7d547ac627a5bee2cbeb5e614f9752927eb52aab',\n",
       "  'lastModified': '2020-08-21T14:42:49.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'it',\n",
       "   'ca',\n",
       "   'rm',\n",
       "   'es',\n",
       "   'ro',\n",
       "   'gl',\n",
       "   'co',\n",
       "   'wa',\n",
       "   'pt',\n",
       "   'oc',\n",
       "   'an',\n",
       "   'id',\n",
       "   'fr',\n",
       "   'ht',\n",
       "   'roa',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-roa-en',\n",
       "  'downloads': 6395,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Mi chiamo Wolfgang e vivo a Berlino'},\n",
       "   {'text': 'Mi chiamo Sarah e vivo a Londra'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['it',\n",
       "    'ca',\n",
       "    'rm',\n",
       "    'es',\n",
       "    'ro',\n",
       "    'gl',\n",
       "    'co',\n",
       "    'wa',\n",
       "    'pt',\n",
       "    'oc',\n",
       "    'an',\n",
       "    'id',\n",
       "    'fr',\n",
       "    'ht',\n",
       "    'roa',\n",
       "    'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-en-ro': {'modelId': 'Helsinki-NLP/opus-mt-en-ro',\n",
       "  'sha': '6d32771161c298fb3164c208e48fea050a85ab65',\n",
       "  'lastModified': '2021-09-09T21:38:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'ro',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-ro',\n",
       "  'downloads': 6371,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'HooshvareLab/distilbert-fa-zwnj-base-ner': {'modelId': 'HooshvareLab/distilbert-fa-zwnj-base-ner',\n",
       "  'sha': '36ccd9aa3dd64c3a83c76de0b8cc5b3f6fa3dc30',\n",
       "  'lastModified': '2021-03-21T14:32:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'token-classification',\n",
       "   'fa',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'HooshvareLab',\n",
       "  'config': {'architectures': ['DistilBertForTokenClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'HooshvareLab/distilbert-fa-zwnj-base-ner',\n",
       "  'downloads': 6371,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'این سریال به صورت رسمی در تاریخ دهم می ۲۰۱۱ توسط شبکه فاکس برای پخش رزرو شد.'},\n",
       "   {'text': 'دفتر مرکزی شرکت پارس\\u200cمینو در شهر اراک در استان مرکزی قرار دارد.'},\n",
       "   {'text': 'وی در سال ۲۰۱۳ درگذشت و مسئول خاکسپاری و اقوامش برای او مراسم یادبود گرفتند.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fa'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'shahrukhx01/question-vs-statement-classifier': {'modelId': 'shahrukhx01/question-vs-statement-classifier',\n",
       "  'sha': '38767ed252cb888c5c1507ea7933150537500893',\n",
       "  'lastModified': '2021-08-25T08:17:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'neural-search-query-classification',\n",
       "   'neural-search'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'shahrukhx01',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'shahrukhx01/question-vs-statement-classifier',\n",
       "  'downloads': 6362,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'what did you eat in lunch?'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['neural-search-query-classification', 'neural-search'],\n",
       "   'widget': [{'text': 'what did you eat in lunch?'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'avichr/heBERT': {'modelId': 'avichr/heBERT',\n",
       "  'sha': '01566c04aa226325662d5054331458e14ef3ede1',\n",
       "  'lastModified': '2022-04-15T09:36:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'avichr',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'avichr/heBERT',\n",
       "  'downloads': 6336,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flair/upos-multi-fast': {'modelId': 'flair/upos-multi-fast',\n",
       "  'sha': '4610445ba7d097da7e0ff23c3a782e9c474382c8',\n",
       "  'lastModified': '2021-03-02T22:22:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'de',\n",
       "   'fr',\n",
       "   'it',\n",
       "   'nl',\n",
       "   'pl',\n",
       "   'es',\n",
       "   'sv',\n",
       "   'da',\n",
       "   'no',\n",
       "   'fi',\n",
       "   'cs',\n",
       "   'dataset:ontonotes',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/upos-multi-fast',\n",
       "  'downloads': 6334,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'Ich liebe Berlin, as they say.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': ['en',\n",
       "    'de',\n",
       "    'fr',\n",
       "    'it',\n",
       "    'nl',\n",
       "    'pl',\n",
       "    'es',\n",
       "    'sv',\n",
       "    'da',\n",
       "    'no',\n",
       "    'fi',\n",
       "    'cs'],\n",
       "   'datasets': ['ontonotes'],\n",
       "   'widget': [{'text': 'Ich liebe Berlin, as they say.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sshleifer/tiny-ctrl': {'modelId': 'sshleifer/tiny-ctrl',\n",
       "  'sha': 'd76c849d54c665ccfd33b8aa501b17531289cae9',\n",
       "  'lastModified': '2020-05-13T23:21:48.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'ctrl', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['CTRLLMHeadModel'], 'model_type': 'ctrl'},\n",
       "  'id': 'sshleifer/tiny-ctrl',\n",
       "  'downloads': 6323,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'luhua/chinese_pretrain_mrc_roberta_wwm_ext_large': {'modelId': 'luhua/chinese_pretrain_mrc_roberta_wwm_ext_large',\n",
       "  'sha': '71a61139397cbb5fd773d8b8b72282a3387ff130',\n",
       "  'lastModified': '2021-06-12T02:53:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'luhua',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'luhua/chinese_pretrain_mrc_roberta_wwm_ext_large',\n",
       "  'downloads': 6317,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '我住在哪里？', 'context': '我叫沃尔夫冈，我住在柏林。'},\n",
       "   {'text': '我住在哪里？', 'context': '我叫萨拉，我住在伦敦。'},\n",
       "   {'text': '我的名字是什么？', 'context': '我叫克拉拉，我住在伯克利。'}],\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-speech-encoder-decoder': {'modelId': 'hf-internal-testing/tiny-random-speech-encoder-decoder',\n",
       "  'sha': '880a6041222f5297adfceb3debd1a955d1c48ba5',\n",
       "  'lastModified': '2021-12-24T15:13:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'speech-encoder-decoder',\n",
       "   'automatic-speech-recognition',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'architectures': ['SpeechEncoderDecoderModel'],\n",
       "   'model_type': 'speech-encoder-decoder'},\n",
       "  'id': 'hf-internal-testing/tiny-random-speech-encoder-decoder',\n",
       "  'downloads': 6286,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSpeechSeq2Seq',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flair/ner-spanish-large': {'modelId': 'flair/ner-spanish-large',\n",
       "  'sha': '9d4671d2f345c1258f37a29ce2321067f2ed296e',\n",
       "  'lastModified': '2021-05-08T15:36:59.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'es',\n",
       "   'dataset:conll2003',\n",
       "   'arxiv:2011.06993',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/ner-spanish-large',\n",
       "  'downloads': 6285,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'George Washington fue a Washington'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': 'es',\n",
       "   'datasets': ['conll2003'],\n",
       "   'widget': [{'text': 'George Washington fue a Washington'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sshleifer/tiny-distilbert-base-cased': {'modelId': 'sshleifer/tiny-distilbert-base-cased',\n",
       "  'sha': '657df2b83a6986d88e4f528740259c9b49f796b1',\n",
       "  'lastModified': '2021-05-20T07:12:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'sshleifer/tiny-distilbert-base-cased',\n",
       "  'downloads': 6270,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jonesy/DialoGPT-small_JT': {'modelId': 'Jonesy/DialoGPT-small_JT',\n",
       "  'sha': '15890d5e6e8b1966ef6b5ef1ff5c37c27acceda0',\n",
       "  'lastModified': '2021-10-15T18:56:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'Jonesy',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'Jonesy/DialoGPT-small_JT',\n",
       "  'downloads': 6269,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/test_dynamic_model': {'modelId': 'hf-internal-testing/test_dynamic_model',\n",
       "  'sha': '2efddce40dddaccd37bae208c3c7ca66dbedf68a',\n",
       "  'lastModified': '2022-01-25T22:03:13.000Z',\n",
       "  'tags': ['pytorch', 'new-model', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'architectures': ['NewModel'], 'model_type': 'new-model'},\n",
       "  'id': 'hf-internal-testing/test_dynamic_model',\n",
       "  'downloads': 6269,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'DeepPavlov/bert-base-cased-conversational': {'modelId': 'DeepPavlov/bert-base-cased-conversational',\n",
       "  'sha': '5415204d80daf12299c85dfddec5f5a7fc7b620a',\n",
       "  'lastModified': '2021-11-08T13:07:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'DeepPavlov',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'DeepPavlov/bert-base-cased-conversational',\n",
       "  'downloads': 6268,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dumitrescustefan/bert-base-romanian-uncased-v1': {'modelId': 'dumitrescustefan/bert-base-romanian-uncased-v1',\n",
       "  'sha': '9eb44d518953103bdc9f088217ec978c6ec9a9e4',\n",
       "  'lastModified': '2021-11-02T15:26:10.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'ro', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dumitrescustefan',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'dumitrescustefan/bert-base-romanian-uncased-v1',\n",
       "  'downloads': 6253,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ro'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'rinna/japanese-roberta-base': {'modelId': 'rinna/japanese-roberta-base',\n",
       "  'sha': '1559edfaaadefcb1661c016455990b0f6f68b20d',\n",
       "  'lastModified': '2021-09-13T00:46:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'ja',\n",
       "   'dataset:cc100',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'japanese',\n",
       "   'masked-lm',\n",
       "   'nlp',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'rinna',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'rinna/japanese-roberta-base',\n",
       "  'downloads': 6238,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '[CLS]4年に1度[MASK]は開かれる。'}],\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'thumbnail': 'https://github.com/rinnakk/japanese-gpt2/blob/master/rinna.png',\n",
       "   'tags': ['ja', 'japanese', 'roberta', 'masked-lm', 'nlp'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['cc100', 'wikipedia'],\n",
       "   'mask_token': '[MASK]',\n",
       "   'widget': [{'text': '[CLS]4年に1度[MASK]は開かれる。'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'digitalepidemiologylab/covid-twitter-bert-v2': {'modelId': 'digitalepidemiologylab/covid-twitter-bert-v2',\n",
       "  'sha': 'b113bc3c2590d7b32ed62603fe1ebe32e1e5beee',\n",
       "  'lastModified': '2021-09-22T08:20:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'Twitter',\n",
       "   'COVID-19',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'digitalepidemiologylab',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'digitalepidemiologylab/covid-twitter-bert-v2',\n",
       "  'downloads': 6234,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://raw.githubusercontent.com/digitalepidemiologylab/covid-twitter-bert/master/images/COVID-Twitter-BERT_small.png',\n",
       "   'tags': ['Twitter', 'COVID-19'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'facebook/fastspeech2-en-ljspeech': {'modelId': 'facebook/fastspeech2-en-ljspeech',\n",
       "  'sha': 'a3e3e5e2e62bb7ca7514b11aa469e9c5b01a20bf',\n",
       "  'lastModified': '2022-01-28T23:25:24.000Z',\n",
       "  'tags': ['en',\n",
       "   'dataset:ljspeech',\n",
       "   'arxiv:2006.04558',\n",
       "   'arxiv:2109.06912',\n",
       "   'fairseq',\n",
       "   'audio',\n",
       "   'text-to-speech'],\n",
       "  'pipeline_tag': 'text-to-speech',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': None,\n",
       "  'id': 'facebook/fastspeech2-en-ljspeech',\n",
       "  'downloads': 6229,\n",
       "  'library_name': 'fairseq',\n",
       "  'widgetData': [{'text': 'Hello, this is a test run.',\n",
       "    'example_title': 'Hello, this is a test run.'}],\n",
       "  'likes': 18,\n",
       "  'model-index': None,\n",
       "  'cardData': {'library_name': 'fairseq',\n",
       "   'task': 'text-to-speech',\n",
       "   'tags': ['fairseq', 'audio', 'text-to-speech'],\n",
       "   'language': 'en',\n",
       "   'datasets': ['ljspeech'],\n",
       "   'widget': [{'text': 'Hello, this is a test run.',\n",
       "     'example_title': 'Hello, this is a test run.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'pranavpsv/gpt2-genre-story-generator': {'modelId': 'pranavpsv/gpt2-genre-story-generator',\n",
       "  'sha': 'd617d856b2aae05be1c253a2e9daf8c99f1d9c6d',\n",
       "  'lastModified': '2021-05-23T11:02:06.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'gpt2', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'pranavpsv',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'pranavpsv/gpt2-genre-story-generator',\n",
       "  'downloads': 6225,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'haisongzhang/roberta-tiny-cased': {'modelId': 'haisongzhang/roberta-tiny-cased',\n",
       "  'sha': '2e8caf4404987b8cb20fa4b22955f56940b2ebc6',\n",
       "  'lastModified': '2021-05-19T17:53:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'haisongzhang',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'haisongzhang/roberta-tiny-cased',\n",
       "  'downloads': 6217,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Peltarion/xlm-roberta-longformer-base-4096': {'modelId': 'Peltarion/xlm-roberta-longformer-base-4096',\n",
       "  'sha': 'c2e164abd333ebd242de4178ea18c1260e00d330',\n",
       "  'lastModified': '2022-03-30T09:23:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'multilingual',\n",
       "   'dataset:wikitext',\n",
       "   'transformers',\n",
       "   'longformer',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Peltarion',\n",
       "  'config': {'architectures': ['LongModelForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'Peltarion/xlm-roberta-longformer-base-4096',\n",
       "  'downloads': 6197,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['longformer'],\n",
       "   'language': 'multilingual',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wikitext']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wav2vec2-large-robust-ft-swbd-300h': {'modelId': 'facebook/wav2vec2-large-robust-ft-swbd-300h',\n",
       "  'sha': '828a8386883f64170e04fae06dd866e0fe97de6b',\n",
       "  'lastModified': '2022-04-05T16:42:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'en',\n",
       "   'dataset:libri_light',\n",
       "   'dataset:common_voice',\n",
       "   'dataset:switchboard',\n",
       "   'dataset:fisher',\n",
       "   'arxiv:2104.01027',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'audio',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'facebook/wav2vec2-large-robust-ft-swbd-300h',\n",
       "  'downloads': 6190,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'example_title': 'Librispeech sample 1',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/sample1.flac'},\n",
       "   {'example_title': 'Librispeech sample 2',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/sample2.flac'}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['libri_light', 'common_voice', 'switchboard', 'fisher'],\n",
       "   'tags': ['speech', 'audio', 'automatic-speech-recognition'],\n",
       "   'widget': [{'example_title': 'Librispeech sample 1',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/sample1.flac'},\n",
       "    {'example_title': 'Librispeech sample 2',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/sample2.flac'}],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'Zixtrauce/JohnBot': {'modelId': 'Zixtrauce/JohnBot',\n",
       "  'sha': '3831946b0c973685e937c4db612ed7cce2733129',\n",
       "  'lastModified': '2022-01-02T06:43:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'Zixtrauce',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'Zixtrauce/JohnBot',\n",
       "  'downloads': 6141,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ivanlau/language-detection-fine-tuned-on-xlm-roberta-base': {'modelId': 'ivanlau/language-detection-fine-tuned-on-xlm-roberta-base',\n",
       "  'sha': '4207aaa00b26aea91d99cb1abc2d7f56814fbe05',\n",
       "  'lastModified': '2021-12-17T10:33:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'xlm-roberta',\n",
       "   'text-classification',\n",
       "   'dataset:common_language',\n",
       "   'transformers',\n",
       "   'generated_from_trainer',\n",
       "   'license:mit',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'ivanlau',\n",
       "  'config': {'architectures': ['XLMRobertaForSequenceClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'ivanlau/language-detection-fine-tuned-on-xlm-roberta-base',\n",
       "  'downloads': 6116,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 1,\n",
       "  'model-index': [{'name': 'language-detection-fine-tuned-on-xlm-roberta-base',\n",
       "    'results': [{'task': {'name': 'Text Classification',\n",
       "       'type': 'text-classification'},\n",
       "      'dataset': {'name': 'common_language',\n",
       "       'type': 'common_language',\n",
       "       'args': 'full'},\n",
       "      'metrics': [{'name': 'Accuracy',\n",
       "        'type': 'accuracy',\n",
       "        'value': 0.9738386718094919}]}]}],\n",
       "  'cardData': {'license': 'mit',\n",
       "   'tags': ['generated_from_trainer'],\n",
       "   'datasets': ['common_language'],\n",
       "   'metrics': ['accuracy'],\n",
       "   'model-index': [{'name': 'language-detection-fine-tuned-on-xlm-roberta-base',\n",
       "     'results': [{'task': {'name': 'Text Classification',\n",
       "        'type': 'text-classification'},\n",
       "       'dataset': {'name': 'common_language',\n",
       "        'type': 'common_language',\n",
       "        'args': 'full'},\n",
       "       'metrics': [{'name': 'Accuracy',\n",
       "         'type': 'accuracy',\n",
       "         'value': 0.9738386718094919}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/vit-base-patch32-224-in21k': {'modelId': 'google/vit-base-patch32-224-in21k',\n",
       "  'sha': 'ebb34016d84eb82beee2f88d5ae21a1f08a8ca88',\n",
       "  'lastModified': '2022-01-12T08:06:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'vit',\n",
       "   'feature-extraction',\n",
       "   'dataset:imagenet-21k',\n",
       "   'arxiv:2010.11929',\n",
       "   'arxiv:2006.03677',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['ViTModel'], 'model_type': 'vit'},\n",
       "  'id': 'google/vit-base-patch32-224-in21k',\n",
       "  'downloads': 6085,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['vision'],\n",
       "   'datasets': ['imagenet-21k'],\n",
       "   'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'Luyu/co-condenser-marco': {'modelId': 'Luyu/co-condenser-marco',\n",
       "  'sha': 'e0cef0ab2410aae0f0994366ddefb5649a266709',\n",
       "  'lastModified': '2021-08-13T13:54:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Luyu',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'Luyu/co-condenser-marco',\n",
       "  'downloads': 6084,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/gelectra-large-germanquad': {'modelId': 'deepset/gelectra-large-germanquad',\n",
       "  'sha': 'cfcb7ec9b76cc01eeac1c1f0cfa0afa5b3b9cbb3',\n",
       "  'lastModified': '2021-10-21T12:19:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'electra',\n",
       "   'question-answering',\n",
       "   'de',\n",
       "   'dataset:deepset/germanquad',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['ElectraForQuestionAnswering'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'deepset/gelectra-large-germanquad',\n",
       "  'downloads': 6066,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Wo wohne ich?',\n",
       "    'context': 'Mein Name ist Wolfgang und ich lebe in Berlin'},\n",
       "   {'text': 'Welcher Name wird auch verwendet, um den Amazonas-Regenwald auf Englisch zu beschreiben?',\n",
       "    'context': 'Der Amazonas-Regenwald, auf Englisch auch als Amazonien oder Amazonas-Dschungel bekannt, ist ein feuchter Laubwald, der den größten Teil des Amazonas-Beckens Südamerikas bedeckt. Dieses Becken umfasst 7.000.000 Quadratkilometer (2.700.000 Quadratmeilen), von denen 5.500.000 Quadratkilometer (2.100.000 Quadratmeilen) vom Regenwald bedeckt sind. Diese Region umfasst Gebiete von neun Nationen. Der größte Teil des Waldes befindet sich in Brasilien mit 60% des Regenwaldes, gefolgt von Peru mit 13%, Kolumbien mit 10% und geringen Mengen in Venezuela, Ecuador, Bolivien, Guyana, Suriname und Französisch-Guayana. Staaten oder Abteilungen in vier Nationen enthalten \"Amazonas\" in ihren Namen. Der Amazonas repräsentiert mehr als die Hälfte der verbleibenden Regenwälder des Planeten und umfasst den größten und artenreichsten tropischen Regenwald der Welt mit geschätzten 390 Milliarden Einzelbäumen, die in 16.000 Arten unterteilt sind.'}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de',\n",
       "   'datasets': ['deepset/germanquad'],\n",
       "   'license': 'mit',\n",
       "   'thumbnail': 'https://thumb.tildacdn.com/tild3433-3637-4830-a533-353833613061/-/resize/720x/-/format/webp/germanquad.jpg',\n",
       "   'tags': ['exbert']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/unixcoder-base': {'modelId': 'microsoft/unixcoder-base',\n",
       "  'sha': '02583b53b9290e674a43b6b74e89f98a71b2d22a',\n",
       "  'lastModified': '2022-03-23T06:05:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'microsoft/unixcoder-base',\n",
       "  'downloads': 6050,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/bert_uncased_L-8_H-512_A-8': {'modelId': 'google/bert_uncased_L-8_H-512_A-8',\n",
       "  'sha': '53b17ea0d90728ac770dfd13fcb3abdc75e4f4bf',\n",
       "  'lastModified': '2021-05-19T17:35:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'arxiv:1908.08962',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'google/bert_uncased_L-8_H-512_A-8',\n",
       "  'downloads': 6046,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'google/long-t5-local-base': {'modelId': 'google/long-t5-local-base',\n",
       "  'sha': 'e040d65029c54fb38eaefa4019bc3e2e31ba3c62',\n",
       "  'lastModified': '2022-06-22T09:04:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'longt5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:2112.07916',\n",
       "   'arxiv:1912.08777',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['LongT5ForConditionalGeneration'],\n",
       "   'model_type': 'longt5'},\n",
       "  'id': 'google/long-t5-local-base',\n",
       "  'downloads': 6034,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0', 'language': 'en'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'gogamza/kobart-base-v2': {'modelId': 'gogamza/kobart-base-v2',\n",
       "  'sha': 'd9a1f640896cef8dcfd693b1bc57510a2b09a18f',\n",
       "  'lastModified': '2021-11-11T07:43:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'feature-extraction',\n",
       "   'ko',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'gogamza',\n",
       "  'config': {'architectures': ['BartModel'], 'model_type': 'bart'},\n",
       "  'id': 'gogamza/kobart-base-v2',\n",
       "  'downloads': 6030,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ko', 'tags': ['bart'], 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pierreguillou/bert-base-cased-squad-v1.1-portuguese': {'modelId': 'pierreguillou/bert-base-cased-squad-v1.1-portuguese',\n",
       "  'sha': 'fda61a9dc93104d7944a4abf5d48d51eba229a13',\n",
       "  'lastModified': '2022-01-04T09:57:53.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'pt',\n",
       "   'dataset:brWaC',\n",
       "   'dataset:squad',\n",
       "   'dataset:squad_v1_pt',\n",
       "   'transformers',\n",
       "   'bert-base',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'pierreguillou',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'pierreguillou/bert-base-cased-squad-v1.1-portuguese',\n",
       "  'downloads': 6007,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Quando começou a pandemia de Covid-19 no mundo?',\n",
       "    'context': 'A pandemia de COVID-19, também conhecida como pandemia de coronavírus, é uma pandemia em curso de COVID-19, uma doença respiratória aguda causada pelo coronavírus da síndrome respiratória aguda grave 2 (SARS-CoV-2). A doença foi identificada pela primeira vez em Wuhan, na província de Hubei, República Popular da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano.'},\n",
       "   {'text': 'Onde foi descoberta a Covid-19?',\n",
       "    'context': 'A pandemia de COVID-19, também conhecida como pandemia de coronavírus, é uma pandemia em curso de COVID-19, uma doença respiratória aguda causada pelo coronavírus da síndrome respiratória aguda grave 2 (SARS-CoV-2). A doença foi identificada pela primeira vez em Wuhan, na província de Hubei, República Popular da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano.'}],\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'pt',\n",
       "   'license': 'mit',\n",
       "   'tags': ['question-answering', 'bert', 'bert-base', 'pytorch'],\n",
       "   'datasets': ['brWaC', 'squad', 'squad_v1_pt'],\n",
       "   'metrics': ['squad'],\n",
       "   'widget': [{'text': 'Quando começou a pandemia de Covid-19 no mundo?',\n",
       "     'context': 'A pandemia de COVID-19, também conhecida como pandemia de coronavírus, é uma pandemia em curso de COVID-19, uma doença respiratória aguda causada pelo coronavírus da síndrome respiratória aguda grave 2 (SARS-CoV-2). A doença foi identificada pela primeira vez em Wuhan, na província de Hubei, República Popular da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano.'},\n",
       "    {'text': 'Onde foi descoberta a Covid-19?',\n",
       "     'context': 'A pandemia de COVID-19, também conhecida como pandemia de coronavírus, é uma pandemia em curso de COVID-19, uma doença respiratória aguda causada pelo coronavírus da síndrome respiratória aguda grave 2 (SARS-CoV-2). A doença foi identificada pela primeira vez em Wuhan, na província de Hubei, República Popular da China, em 1 de dezembro de 2019, mas o primeiro caso foi reportado em 31 de dezembro do mesmo ano.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/deit-base-distilled-patch16-224': {'modelId': 'facebook/deit-base-distilled-patch16-224',\n",
       "  'sha': 'ea2cf46b3d9c8865fe1de652bf26f3de547298dd',\n",
       "  'lastModified': '2022-03-18T14:37:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'deit',\n",
       "   'image-classification',\n",
       "   'dataset:imagenet',\n",
       "   'arxiv:2012.12877',\n",
       "   'arxiv:2006.03677',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-classification',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['DeiTForImageClassificationWithTeacher'],\n",
       "   'model_type': 'deit'},\n",
       "  'id': 'facebook/deit-base-distilled-patch16-224',\n",
       "  'downloads': 5990,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['image-classification', 'vision'],\n",
       "   'datasets': ['imagenet']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageClassification',\n",
       "   'pipeline_tag': 'image-classification',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'benjamin/gerpt2': {'modelId': 'benjamin/gerpt2',\n",
       "  'sha': '76b77997c1a715c3cf61a8d086fb75baa3816ded',\n",
       "  'lastModified': '2022-05-11T09:17:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'benjamin',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 100}}},\n",
       "  'id': 'benjamin/gerpt2',\n",
       "  'downloads': 5986,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'In einer schockierenden Entdeckung fanden Wissenschaftler eine Herde Einhörner, die in einem abgelegenen, zuvor unerforschten Tal in den Anden lebten.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de',\n",
       "   'widget': [{'text': 'In einer schockierenden Entdeckung fanden Wissenschaftler eine Herde Einhörner, die in einem abgelegenen, zuvor unerforschten Tal in den Anden lebten.'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'funnel-transformer/small-base': {'modelId': 'funnel-transformer/small-base',\n",
       "  'sha': '373ecb760257d0059f1efdf58b3796d4d616ad0a',\n",
       "  'lastModified': '2020-12-11T21:40:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'funnel',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:gigaword',\n",
       "   'arxiv:2006.03236',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'funnel-transformer',\n",
       "  'config': {'architectures': ['FunnelBaseModel'], 'model_type': 'funnel'},\n",
       "  'id': 'funnel-transformer/small-base',\n",
       "  'downloads': 5985,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia', 'gigaword']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/electra-base-squad2': {'modelId': 'deepset/electra-base-squad2',\n",
       "  'sha': 'a620af2f3aab26e35a80422941a3ada5b8387519',\n",
       "  'lastModified': '2021-10-21T12:16:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'electra',\n",
       "   'question-answering',\n",
       "   'dataset:squad_v2',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['ElectraForQuestionAnswering'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'deepset/electra-base-squad2',\n",
       "  'downloads': 5968,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['squad_v2'], 'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'princeton-nlp/sup-simcse-bert-large-uncased': {'modelId': 'princeton-nlp/sup-simcse-bert-large-uncased',\n",
       "  'sha': '6711247726a5d5f78c17babf57d76fa99f7b1fdf',\n",
       "  'lastModified': '2021-05-20T02:56:23.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'princeton-nlp',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'princeton-nlp/sup-simcse-bert-large-uncased',\n",
       "  'downloads': 5956,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/s2t-small-librispeech-asr': {'modelId': 'facebook/s2t-small-librispeech-asr',\n",
       "  'sha': '89d22e9beca033913df096434cccc2c41199d8c1',\n",
       "  'lastModified': '2022-05-24T10:45:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'speech_to_text',\n",
       "   'automatic-speech-recognition',\n",
       "   'en',\n",
       "   'dataset:librispeech_asr',\n",
       "   'arxiv:2010.05171',\n",
       "   'arxiv:1904.08779',\n",
       "   'transformers',\n",
       "   'speech',\n",
       "   'audio',\n",
       "   'hf-asr-leaderboard',\n",
       "   'license:mit',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['Speech2TextForConditionalGeneration'],\n",
       "   'model_type': 'speech_to_text'},\n",
       "  'id': 'facebook/s2t-small-librispeech-asr',\n",
       "  'downloads': 5922,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'example_title': 'Librispeech sample 1',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/sample1.flac'},\n",
       "   {'example_title': 'Librispeech sample 2',\n",
       "    'src': 'https://cdn-media.huggingface.co/speech_samples/sample2.flac'}],\n",
       "  'likes': 9,\n",
       "  'model-index': [{'name': 's2t-small-librispeech-asr',\n",
       "    'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'LibriSpeech (clean)',\n",
       "       'type': 'librispeech_asr',\n",
       "       'config': 'clean',\n",
       "       'split': 'test',\n",
       "       'args': {'language': 'en'}},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 4.3}]},\n",
       "     {'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'LibriSpeech (other)',\n",
       "       'type': 'librispeech_asr',\n",
       "       'config': 'other',\n",
       "       'split': 'test',\n",
       "       'args': {'language': 'en'}},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 9}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['librispeech_asr'],\n",
       "   'tags': ['speech',\n",
       "    'audio',\n",
       "    'automatic-speech-recognition',\n",
       "    'hf-asr-leaderboard'],\n",
       "   'license': 'mit',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'widget': [{'example_title': 'Librispeech sample 1',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/sample1.flac'},\n",
       "    {'example_title': 'Librispeech sample 2',\n",
       "     'src': 'https://cdn-media.huggingface.co/speech_samples/sample2.flac'}],\n",
       "   'model-index': [{'name': 's2t-small-librispeech-asr',\n",
       "     'results': [{'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'LibriSpeech (clean)',\n",
       "        'type': 'librispeech_asr',\n",
       "        'config': 'clean',\n",
       "        'split': 'test',\n",
       "        'args': {'language': 'en'}},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 4.3}]},\n",
       "      {'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'LibriSpeech (other)',\n",
       "        'type': 'librispeech_asr',\n",
       "        'config': 'other',\n",
       "        'split': 'test',\n",
       "        'args': {'language': 'en'}},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 9}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSpeechSeq2Seq',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'mrm8488/t5-base-finetuned-emotion': {'modelId': 'mrm8488/t5-base-finetuned-emotion',\n",
       "  'sha': 'e44a316825f11230724b36412fbf1899c76e82de',\n",
       "  'lastModified': '2021-06-23T12:46:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:emotion',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'mrm8488/t5-base-finetuned-emotion',\n",
       "  'downloads': 5916,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'I wish you were here but it is impossible'}],\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['emotion'],\n",
       "   'widget': [{'text': 'I wish you were here but it is impossible'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sbcBI/sentiment_analysis_model': {'modelId': 'sbcBI/sentiment_analysis_model',\n",
       "  'sha': 'a994fec145b4e096961210b871c376a0ba8440ba',\n",
       "  'lastModified': '2022-05-16T18:37:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'distilbert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:Confidential',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'sbcBI',\n",
       "  'config': {'architectures': ['DistilBertForSequenceClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'sbcBI/sentiment_analysis_model',\n",
       "  'downloads': 5903,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['Confidential']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Salesforce/bart-large-xsum-samsum': {'modelId': 'Salesforce/bart-large-xsum-samsum',\n",
       "  'sha': 'bf8a8779c158901df223516a72b9efaa887ed1df',\n",
       "  'lastModified': '2021-06-09T19:36:02.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'Salesforce',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {}},\n",
       "  'id': 'Salesforce/bart-large-xsum-samsum',\n",
       "  'downloads': 5900,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/trocr-base-handwritten': {'modelId': 'microsoft/trocr-base-handwritten',\n",
       "  'sha': 'bad90c41e8b5a5cd03f658fbd568b44b2ee047c5',\n",
       "  'lastModified': '2022-07-01T07:35:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'vision-encoder-decoder',\n",
       "   'arxiv:2109.10282',\n",
       "   'transformers',\n",
       "   'trocr',\n",
       "   'image-to-text'],\n",
       "  'pipeline_tag': 'image-to-text',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['VisionEncoderDecoderModel'],\n",
       "   'model_type': 'vision-encoder-decoder'},\n",
       "  'id': 'microsoft/trocr-base-handwritten',\n",
       "  'downloads': 5889,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['trocr', 'image-to-text']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/rbt3': {'modelId': 'hfl/rbt3',\n",
       "  'sha': '0aa0527ff4170f29e1dfd3eb6ef60dc67e1bf75c',\n",
       "  'lastModified': '2021-05-19T19:19:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'arxiv:1906.08101',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'hfl/rbt3',\n",
       "  'downloads': 5868,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '巴黎是[MASK]国的首都。'}, {'text': '生活的真谛是[MASK]。'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'tags': ['bert'],\n",
       "   'license': 'apache-2.0',\n",
       "   'pipeline_tag': 'fill-mask'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'indobenchmark/indobert-base-p2': {'modelId': 'indobenchmark/indobert-base-p2',\n",
       "  'sha': '94b4e0a82081fa57f227fcc2024d1ea89b57ac1f',\n",
       "  'lastModified': '2021-05-19T20:24:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'id',\n",
       "   'dataset:Indo4B',\n",
       "   'arxiv:2009.05387',\n",
       "   'transformers',\n",
       "   'indobert',\n",
       "   'indobenchmark',\n",
       "   'indonlu',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'indobenchmark',\n",
       "  'config': {'architectures': ['BertModel'],\n",
       "   'model_type': 'bert',\n",
       "   'task_specific_params': None},\n",
       "  'id': 'indobenchmark/indobert-base-p2',\n",
       "  'downloads': 5856,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'id',\n",
       "   'tags': ['indobert', 'indobenchmark', 'indonlu'],\n",
       "   'license': 'mit',\n",
       "   'inference': False,\n",
       "   'datasets': ['Indo4B']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ktrapeznikov/albert-xlarge-v2-squad-v2': {'modelId': 'ktrapeznikov/albert-xlarge-v2-squad-v2',\n",
       "  'sha': 'fb1e05445e376bdb883e8d4f6696a0acaf62e0ae',\n",
       "  'lastModified': '2020-12-11T21:48:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'albert',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'ktrapeznikov',\n",
       "  'config': {'architectures': ['AlbertForQuestionAnswering'],\n",
       "   'model_type': 'albert'},\n",
       "  'id': 'ktrapeznikov/albert-xlarge-v2-squad-v2',\n",
       "  'downloads': 5854,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bert-large-cased-whole-word-masking': {'modelId': 'bert-large-cased-whole-word-masking',\n",
       "  'sha': '8b5d59881077680d99e2cac339412f20a180bd45',\n",
       "  'lastModified': '2021-05-18T16:30:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': None,\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'bert-large-cased-whole-word-masking',\n",
       "  'downloads': 5842,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sbcBI/sentiment_analysis': {'modelId': 'sbcBI/sentiment_analysis',\n",
       "  'sha': '2e9e3afe68478a6168a11adb6c6f1b741e00ae83',\n",
       "  'lastModified': '2022-04-22T06:42:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'distilbert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:Confidential',\n",
       "   'arxiv:1810.04805',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'sbcBI',\n",
       "  'config': {'architectures': ['DistilBertForSequenceClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'sbcBI/sentiment_analysis',\n",
       "  'downloads': 5828,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['Confidential']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'dbmdz/bert-base-italian-uncased': {'modelId': 'dbmdz/bert-base-italian-uncased',\n",
       "  'sha': 'd91243bae3a97a72691e9a6bfdf5d9f8fa4be9e4',\n",
       "  'lastModified': '2021-05-19T15:00:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'it',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'dbmdz',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'dbmdz/bert-base-italian-uncased',\n",
       "  'downloads': 5826,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': \"Roma è la [MASK] d'Italia.\"},\n",
       "   {'text': 'Lo scopo della vita è [MASK].'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'it', 'license': 'mit', 'datasets': ['wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KETI-AIR/ke-t5-small': {'modelId': 'KETI-AIR/ke-t5-small',\n",
       "  'sha': '3a2efa3a340d88de8aa93be0cad7884c34a64128',\n",
       "  'lastModified': '2021-06-23T03:13:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'KETI-AIR',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'KETI-AIR/ke-t5-small',\n",
       "  'downloads': 5815,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'castorini/unicoil-msmarco-passage': {'modelId': 'castorini/unicoil-msmarco-passage',\n",
       "  'sha': 'a9379ff729899cf1255960e604496c1a638346ce',\n",
       "  'lastModified': '2021-07-13T22:28:03.000Z',\n",
       "  'tags': ['pytorch', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'castorini',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'castorini/unicoil-msmarco-passage',\n",
       "  'downloads': 5796,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/electra-base-generator': {'modelId': 'google/electra-base-generator',\n",
       "  'sha': '1c65e3f5f4597679b87620707df5774c08c6606d',\n",
       "  'lastModified': '2021-04-30T07:42:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'rust',\n",
       "   'electra',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['ElectraForMaskedLM'], 'model_type': 'electra'},\n",
       "  'id': 'google/electra-base-generator',\n",
       "  'downloads': 5791,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Rostlab/prot_t5_xl_half_uniref50-enc': {'modelId': 'Rostlab/prot_t5_xl_half_uniref50-enc',\n",
       "  'sha': '2646ade9d44b7620ceac59797b2d9efd3341da37',\n",
       "  'lastModified': '2022-06-29T08:22:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'protein',\n",
       "   'dataset:UniRef50',\n",
       "   'transformers',\n",
       "   'protein language model'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'Rostlab',\n",
       "  'config': {'architectures': ['T5EncoderModel'], 'model_type': 't5'},\n",
       "  'id': 'Rostlab/prot_t5_xl_half_uniref50-enc',\n",
       "  'downloads': 5786,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'protein',\n",
       "   'tags': ['protein language model'],\n",
       "   'datasets': ['UniRef50']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/wmt19-en-ru': {'modelId': 'facebook/wmt19-en-ru',\n",
       "  'sha': 'a96448f0cef6a0b22c8f73f6f0e74b610efe806f',\n",
       "  'lastModified': '2020-12-11T21:39:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'fsmt',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'ru',\n",
       "   'dataset:wmt19',\n",
       "   'arxiv:1907.06616',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'wmt19',\n",
       "   'facebook',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['FSMTForConditionalGeneration'],\n",
       "   'model_type': 'fsmt'},\n",
       "  'id': 'facebook/wmt19-en-ru',\n",
       "  'downloads': 5785,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'ru'],\n",
       "   'tags': ['translation', 'wmt19', 'facebook'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wmt19'],\n",
       "   'metrics': ['bleu'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-lv-en': {'modelId': 'Helsinki-NLP/opus-mt-lv-en',\n",
       "  'sha': '3b019339e88ac4f79044be45cfa75ff5fedbceea',\n",
       "  'lastModified': '2021-09-10T13:57:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'lv',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-lv-en',\n",
       "  'downloads': 5779,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'moussaKam/frugalscore_tiny_bert-base_bert-score': {'modelId': 'moussaKam/frugalscore_tiny_bert-base_bert-score',\n",
       "  'sha': 'a487e5a875e63ef1f9cf6015a3a11be2d80aa550',\n",
       "  'lastModified': '2022-02-01T10:50:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'arxiv:2110.08559',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'moussaKam',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'moussaKam/frugalscore_tiny_bert-base_bert-score',\n",
       "  'downloads': 5773,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mrm8488/t5-base-finetuned-e2m-intent': {'modelId': 'mrm8488/t5-base-finetuned-e2m-intent',\n",
       "  'sha': '84f655dbb0f40e64e12ad1a61c125a1225fc2917',\n",
       "  'lastModified': '2020-12-11T21:55:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:event2Mind',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'mrm8488/t5-base-finetuned-e2m-intent',\n",
       "  'downloads': 5762,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'datasets': ['event2Mind']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KoboldAI/fairseq-dense-13B': {'modelId': 'KoboldAI/fairseq-dense-13B',\n",
       "  'sha': 'e936211b7bb8f406cb78efca22a5f7c43ba090b3',\n",
       "  'lastModified': '2022-02-01T22:51:59.000Z',\n",
       "  'tags': ['pytorch', 'xglm', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'KoboldAI',\n",
       "  'config': {'architectures': ['XGLMForCausalLM'], 'model_type': 'xglm'},\n",
       "  'id': 'KoboldAI/fairseq-dense-13B',\n",
       "  'downloads': 5735,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allenai/wmt19-de-en-6-6-base': {'modelId': 'allenai/wmt19-de-en-6-6-base',\n",
       "  'sha': 'a9ec1968c8c3962f0f85f9f38ee4b8093ce84f24',\n",
       "  'lastModified': '2020-12-11T21:33:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'fsmt',\n",
       "   'text2text-generation',\n",
       "   'de',\n",
       "   'en',\n",
       "   'dataset:wmt19',\n",
       "   'arxiv:2006.10369',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'wmt19',\n",
       "   'allenai',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'architectures': ['FSMTForConditionalGeneration'],\n",
       "   'model_type': 'fsmt'},\n",
       "  'id': 'allenai/wmt19-de-en-6-6-base',\n",
       "  'downloads': 5735,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['de', 'en'],\n",
       "   'thumbnail': None,\n",
       "   'tags': ['translation', 'wmt19', 'allenai'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wmt19'],\n",
       "   'metrics': ['bleu']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/pegasus-multi_news': {'modelId': 'google/pegasus-multi_news',\n",
       "  'sha': 'e68ac31f04c1daf2956f36d5ad1701f2d6f91932',\n",
       "  'lastModified': '2020-10-22T16:33:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'pegasus',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'arxiv:1912.08777',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['PegasusForConditionalGeneration'],\n",
       "   'model_type': 'pegasus'},\n",
       "  'id': 'google/pegasus-multi_news',\n",
       "  'downloads': 5725,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'tags': ['summarization']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'typeform/mobilebert-uncased-mnli': {'modelId': 'typeform/mobilebert-uncased-mnli',\n",
       "  'sha': 'b60d566014db63a45a440ee32b3e9e9a01d2a1fc',\n",
       "  'lastModified': '2021-02-14T09:11:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mobilebert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:multi_nli',\n",
       "   'transformers',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'typeform',\n",
       "  'config': {'architectures': ['MobileBertForSequenceClassification'],\n",
       "   'model_type': 'mobilebert'},\n",
       "  'id': 'typeform/mobilebert-uncased-mnli',\n",
       "  'downloads': 5707,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system’s largest world. Konstantin Batygin did not set out to solve one of the solar system’s most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system’s missing “Planet Nine,” spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn’t rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: “Oh! This is how Europa formed.” Europa is one of Jupiter’s four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the Côte d’Azur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system’s formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'tags': ['mobilebert'],\n",
       "   'datasets': ['multi_nli'],\n",
       "   'metrics': ['accuracy']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'textattack/bert-base-uncased-CoLA': {'modelId': 'textattack/bert-base-uncased-CoLA',\n",
       "  'sha': '5fed03dd6bc5f0b40e86cb04cd1a16eb404ba391',\n",
       "  'lastModified': '2021-05-20T07:31:05.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'textattack',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'textattack/bert-base-uncased-CoLA',\n",
       "  'downloads': 5694,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/deberta-v2-xxlarge': {'modelId': 'microsoft/deberta-v2-xxlarge',\n",
       "  'sha': 'da58c8f337be794a1bb98c218d0d8cc72c324884',\n",
       "  'lastModified': '2022-01-13T20:00:20.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'deberta-v2',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'deberta',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'deberta-v2'},\n",
       "  'id': 'microsoft/deberta-v2-xxlarge',\n",
       "  'downloads': 5686,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 10,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': 'deberta',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sachin/vit2distilgpt2': {'modelId': 'sachin/vit2distilgpt2',\n",
       "  'sha': '51be2b2bcadf5f37a4b2da6466f64e764c85add7',\n",
       "  'lastModified': '2022-01-27T12:15:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'vision-encoder-decoder',\n",
       "   'en',\n",
       "   'dataset:coco2017',\n",
       "   'transformers',\n",
       "   'image-to-text',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'image-to-text',\n",
       "  'private': False,\n",
       "  'author': 'sachin',\n",
       "  'config': {'architectures': ['VisionEncoderDecoderModel'],\n",
       "   'model_type': 'vision-encoder-decoder'},\n",
       "  'id': 'sachin/vit2distilgpt2',\n",
       "  'downloads': 5681,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['image-to-text'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['coco2017']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'EEE/DialoGPT-medium-brooke': {'modelId': 'EEE/DialoGPT-medium-brooke',\n",
       "  'sha': '6c5ccd6420a957b3116bbd02f21ed4e5ae1ac59d',\n",
       "  'lastModified': '2021-09-27T06:25:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'EEE',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'EEE/DialoGPT-medium-brooke',\n",
       "  'downloads': 5657,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'SAPOSS/password-model': {'modelId': 'SAPOSS/password-model',\n",
       "  'sha': '0ffd8651e74d7548268af1d0b2a611709149dcdf',\n",
       "  'lastModified': '2021-10-11T13:45:53.000Z',\n",
       "  'tags': ['tf', 'roberta', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'SAPOSS',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'SAPOSS/password-model',\n",
       "  'downloads': 5634,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'razent/SciFive-base-Pubmed': {'modelId': 'razent/SciFive-base-Pubmed',\n",
       "  'sha': '7ecd3e2966a97aa898461113a2dbb8da1acac625',\n",
       "  'lastModified': '2022-03-20T17:47:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:pubmed',\n",
       "   'arxiv:2106.03598',\n",
       "   'transformers',\n",
       "   'token-classification',\n",
       "   'text-classification',\n",
       "   'question-answering',\n",
       "   'text-generation',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'razent',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'razent/SciFive-base-Pubmed',\n",
       "  'downloads': 5626,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['token-classification',\n",
       "    'text-classification',\n",
       "    'question-answering',\n",
       "    'text2text-generation',\n",
       "    'text-generation'],\n",
       "   'datasets': ['pubmed']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment': {'modelId': 'IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment',\n",
       "  'sha': '21ff55d0bd2f7904d2a5380165ac2fd6d0d74b81',\n",
       "  'lastModified': '2022-05-27T07:59:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'NLU',\n",
       "   'Sentiment',\n",
       "   'Chinese',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'IDEA-CCNL',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment',\n",
       "  'downloads': 5606,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '今天心情不好'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['bert', 'NLU', 'Sentiment', 'Chinese'],\n",
       "   'inference': True,\n",
       "   'widget': [{'text': '今天心情不好'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'JorisCos/ConvTasNet_Libri1Mix_enhsingle_16k': {'modelId': 'JorisCos/ConvTasNet_Libri1Mix_enhsingle_16k',\n",
       "  'sha': 'bb8a876bc157b5cf3c405994accb798c49146016',\n",
       "  'lastModified': '2021-09-23T15:48:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'dataset:Libri1Mix',\n",
       "   'dataset:enh_single',\n",
       "   'asteroid',\n",
       "   'audio',\n",
       "   'ConvTasNet',\n",
       "   'audio-to-audio',\n",
       "   'license:cc-by-sa-4.0'],\n",
       "  'pipeline_tag': 'audio-to-audio',\n",
       "  'private': False,\n",
       "  'author': 'JorisCos',\n",
       "  'config': None,\n",
       "  'id': 'JorisCos/ConvTasNet_Libri1Mix_enhsingle_16k',\n",
       "  'downloads': 5600,\n",
       "  'library_name': 'asteroid',\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['asteroid', 'audio', 'ConvTasNet', 'audio-to-audio'],\n",
       "   'datasets': ['Libri1Mix', 'enh_single'],\n",
       "   'license': 'cc-by-sa-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'allenai/led-large-16384-arxiv': {'modelId': 'allenai/led-large-16384-arxiv',\n",
       "  'sha': '6d566f57e58195c1810dd8497ccf7f015409a1a9',\n",
       "  'lastModified': '2021-01-12T23:14:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'led',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:scientific_papers',\n",
       "   'arxiv:2004.05150',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'architectures': ['LEDForConditionalGeneration'],\n",
       "   'model_type': 'led'},\n",
       "  'id': 'allenai/led-large-16384-arxiv',\n",
       "  'downloads': 5598,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['scientific_papers'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flax-community/vqgan_f16_16384': {'modelId': 'flax-community/vqgan_f16_16384',\n",
       "  'sha': '38503412bfd13702a4a96b986366a86a62eb322d',\n",
       "  'lastModified': '2021-07-25T15:15:36.000Z',\n",
       "  'tags': ['jax', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'flax-community',\n",
       "  'config': {'architectures': ['del']},\n",
       "  'id': 'flax-community/vqgan_f16_16384',\n",
       "  'downloads': 5595,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'del'}},\n",
       " 'google/bert_uncased_L-12_H-768_A-12': {'modelId': 'google/bert_uncased_L-12_H-768_A-12',\n",
       "  'sha': 'cc478efa3482fefe275eb2733363db9713d499ef',\n",
       "  'lastModified': '2021-05-19T17:27:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'arxiv:1908.08962',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'google/bert_uncased_L-12_H-768_A-12',\n",
       "  'downloads': 5578,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'thumbnail': 'https://huggingface.co/front/thumbnails/google.png',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'KoboldAI/fairseq-dense-13B-Nerys-v2': {'modelId': 'KoboldAI/fairseq-dense-13B-Nerys-v2',\n",
       "  'sha': '45d7cf7f2a4f285bbe36a3421f2497c925f86ef4',\n",
       "  'lastModified': '2022-06-25T11:07:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xglm',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'KoboldAI',\n",
       "  'config': {'architectures': ['XGLMForCausalLM'], 'model_type': 'xglm'},\n",
       "  'id': 'KoboldAI/fairseq-dense-13B-Nerys-v2',\n",
       "  'downloads': 5575,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en', 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'kha-white/manga-ocr-base': {'modelId': 'kha-white/manga-ocr-base',\n",
       "  'sha': 'aa6573bd10b0d446cbf622e29c3e084914df9741',\n",
       "  'lastModified': '2022-06-22T15:34:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'vision-encoder-decoder',\n",
       "   'ja',\n",
       "   'dataset:manga109s',\n",
       "   'transformers',\n",
       "   'image-to-text',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-to-text',\n",
       "  'private': False,\n",
       "  'author': 'kha-white',\n",
       "  'config': {'architectures': ['VisionEncoderDecoderModel'],\n",
       "   'model_type': 'vision-encoder-decoder'},\n",
       "  'id': 'kha-white/manga-ocr-base',\n",
       "  'downloads': 5545,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'tags': ['image-to-text'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['manga109s']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/paraphrase-MiniLM-L12-v2': {'modelId': 'sentence-transformers/paraphrase-MiniLM-L12-v2',\n",
       "  'sha': '8f010f24d5c0e1ee9735f056d024fcda6557f70f',\n",
       "  'lastModified': '2022-06-15T20:18:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/paraphrase-MiniLM-L12-v2',\n",
       "  'downloads': 5534,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'philschmid/distilbart-cnn-12-6-samsum': {'modelId': 'philschmid/distilbart-cnn-12-6-samsum',\n",
       "  'sha': '196850410ee8bb4bab96e4d79b8ffa2cf3d1150c',\n",
       "  'lastModified': '2022-06-24T11:24:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:samsum',\n",
       "   'transformers',\n",
       "   'sagemaker',\n",
       "   'summarization',\n",
       "   'license:apache-2.0',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'philschmid',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4}}},\n",
       "  'id': 'philschmid/distilbart-cnn-12-6-samsum',\n",
       "  'downloads': 5489,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Jeff: Can I train a 🤗 Transformers model on Amazon SageMaker? \\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\nJeff: ok.\\nJeff: and how can I get started? \\nJeff: where can I find documentation? \\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face '}],\n",
       "  'likes': 4,\n",
       "  'model-index': [{'name': 'philschmid/distilbart-cnn-12-6-samsum',\n",
       "    'results': [{'task': {'type': 'summarization', 'name': 'Summarization'},\n",
       "      'dataset': {'name': 'samsum',\n",
       "       'type': 'samsum',\n",
       "       'config': 'samsum',\n",
       "       'split': 'test'},\n",
       "      'metrics': [{'name': 'ROUGE-1',\n",
       "        'type': 'rouge',\n",
       "        'value': 41.0895,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-2',\n",
       "        'type': 'rouge',\n",
       "        'value': 20.7459,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-L',\n",
       "        'type': 'rouge',\n",
       "        'value': 31.5952,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-LSUM',\n",
       "        'type': 'rouge',\n",
       "        'value': 38.3389,\n",
       "        'verified': True},\n",
       "       {'name': 'loss',\n",
       "        'type': 'loss',\n",
       "        'value': 1.4566329717636108,\n",
       "        'verified': True},\n",
       "       {'name': 'gen_len',\n",
       "        'type': 'gen_len',\n",
       "        'value': 59.6032,\n",
       "        'verified': True}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['sagemaker', 'bart', 'summarization'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['samsum'],\n",
       "   'widget': [{'text': 'Jeff: Can I train a 🤗 Transformers model on Amazon SageMaker? \\nPhilipp: Sure you can use the new Hugging Face Deep Learning Container. \\nJeff: ok.\\nJeff: and how can I get started? \\nJeff: where can I find documentation? \\nPhilipp: ok, ok you can find everything here. https://huggingface.co/blog/the-partnership-amazon-sagemaker-and-hugging-face '}],\n",
       "   'model-index': [{'name': 'philschmid/distilbart-cnn-12-6-samsum',\n",
       "     'results': [{'task': {'type': 'summarization', 'name': 'Summarization'},\n",
       "       'dataset': {'name': 'samsum',\n",
       "        'type': 'samsum',\n",
       "        'config': 'samsum',\n",
       "        'split': 'test'},\n",
       "       'metrics': [{'name': 'ROUGE-1',\n",
       "         'type': 'rouge',\n",
       "         'value': 41.0895,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-2',\n",
       "         'type': 'rouge',\n",
       "         'value': 20.7459,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-L',\n",
       "         'type': 'rouge',\n",
       "         'value': 31.5952,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-LSUM',\n",
       "         'type': 'rouge',\n",
       "         'value': 38.3389,\n",
       "         'verified': True},\n",
       "        {'name': 'loss',\n",
       "         'type': 'loss',\n",
       "         'value': 1.4566329717636108,\n",
       "         'verified': True},\n",
       "        {'name': 'gen_len',\n",
       "         'type': 'gen_len',\n",
       "         'value': 59.6032,\n",
       "         'verified': True}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'eugenesiow/drln-bam': {'modelId': 'eugenesiow/drln-bam',\n",
       "  'sha': 'c5d6efffdb6d7ddda5909f2d9105b33b69f4039f',\n",
       "  'lastModified': '2021-09-13T08:34:40.000Z',\n",
       "  'tags': ['DRLN',\n",
       "   'dataset:eugenesiow/Div2k',\n",
       "   'dataset:eugenesiow/Set5',\n",
       "   'dataset:eugenesiow/Set14',\n",
       "   'dataset:eugenesiow/BSD100',\n",
       "   'dataset:eugenesiow/Urban100',\n",
       "   'arxiv:1906.12021',\n",
       "   'arxiv:2104.07566',\n",
       "   'transformers',\n",
       "   'super-image',\n",
       "   'image-super-resolution',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'eugenesiow',\n",
       "  'config': {'model_type': 'DRLN'},\n",
       "  'id': 'eugenesiow/drln-bam',\n",
       "  'downloads': 5455,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['super-image', 'image-super-resolution'],\n",
       "   'datasets': ['eugenesiow/Div2k',\n",
       "    'eugenesiow/Set5',\n",
       "    'eugenesiow/Set14',\n",
       "    'eugenesiow/BSD100',\n",
       "    'eugenesiow/Urban100'],\n",
       "   'metrics': ['pnsr', 'ssim']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'AmbricJohnson5888/claura': {'modelId': 'AmbricJohnson5888/claura',\n",
       "  'sha': 'bb46422b9136a2fc1217cd6debdf362e49f26743',\n",
       "  'lastModified': '2022-04-09T04:18:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'AmbricJohnson5888',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'AmbricJohnson5888/claura',\n",
       "  'downloads': 5449,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'camembert/camembert-base': {'modelId': 'camembert/camembert-base',\n",
       "  'sha': 'e12767c19b74b1efc75b0af07bbde51ddd26b529',\n",
       "  'lastModified': '2022-06-17T23:06:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'camembert',\n",
       "   'fill-mask',\n",
       "   'fr',\n",
       "   'arxiv:1911.03894',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'camembert',\n",
       "  'config': {'architectures': ['CamembertForMaskedLM'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'camembert/camembert-base',\n",
       "  'downloads': 5428,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris est la <mask> de la France.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Langboat/mengzi-bert-base-fin': {'modelId': 'Langboat/mengzi-bert-base-fin',\n",
       "  'sha': 'b7b290d3b4dd5ec87f47d3cf5d55c9d00bd69e59',\n",
       "  'lastModified': '2021-10-18T05:53:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'arxiv:2110.06696',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Langboat',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'Langboat/mengzi-bert-base-fin',\n",
       "  'downloads': 5397,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '巴黎是[MASK]国的首都。'}, {'text': '生活的真谛是[MASK]。'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'robot-test/dummy-tokenizer-fast-with-model-config': {'modelId': 'robot-test/dummy-tokenizer-fast-with-model-config',\n",
       "  'sha': '8a27784052ebf721c837afe249050f6460c6a587',\n",
       "  'lastModified': '2021-05-31T15:40:58.000Z',\n",
       "  'tags': ['albert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'robot-test',\n",
       "  'config': {'model_type': 'albert'},\n",
       "  'id': 'robot-test/dummy-tokenizer-fast-with-model-config',\n",
       "  'downloads': 5374,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/msmarco-distilbert-cos-v5': {'modelId': 'sentence-transformers/msmarco-distilbert-cos-v5',\n",
       "  'sha': '97bf29337ec20da8a1fb1ff2bd5555de5b566baf',\n",
       "  'lastModified': '2022-06-15T21:48:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/msmarco-distilbert-cos-v5',\n",
       "  'downloads': 5360,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'richielleisart/Childe': {'modelId': 'richielleisart/Childe',\n",
       "  'sha': '5da9de8d7f9e9cfde2c126b6ac6531b3ddff606a',\n",
       "  'lastModified': '2022-01-19T18:52:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'richielleisart',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'richielleisart/Childe',\n",
       "  'downloads': 5335,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/layoutlm-base-cased': {'modelId': 'microsoft/layoutlm-base-cased',\n",
       "  'sha': '91acf0f93186fbcebb9f80c2e2259754f0ef922b',\n",
       "  'lastModified': '2021-09-27T05:55:31.000Z',\n",
       "  'tags': ['pytorch', 'layoutlm', 'arxiv:1912.13318', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'model_type': 'layoutlm'},\n",
       "  'id': 'microsoft/layoutlm-base-cased',\n",
       "  'downloads': 5328,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'facebook/rag-token-base': {'modelId': 'facebook/rag-token-base',\n",
       "  'sha': 'bdeb4ef1d547bcfe5445aba1704ece55af71dd58',\n",
       "  'lastModified': '2020-12-11T21:39:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rag',\n",
       "   'en',\n",
       "   'dataset:wiki_dpr',\n",
       "   'arxiv:2005.11401',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['RagTokenForGeneration'], 'model_type': 'rag'},\n",
       "  'id': 'facebook/rag-token-base',\n",
       "  'downloads': 5324,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wiki_dpr'],\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png'},\n",
       "  'transformersInfo': {'auto_model': 'RagTokenForGeneration',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Langboat/mengzi-t5-base': {'modelId': 'Langboat/mengzi-t5-base',\n",
       "  'sha': 'fbd9c58ba8e1a5393668fbdfe477ec70267c01e7',\n",
       "  'lastModified': '2021-10-21T12:33:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'zh',\n",
       "   'arxiv:2110.06696',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'Langboat',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'Langboat/mengzi-t5-base',\n",
       "  'downloads': 5299,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 13,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'esiebomajeremiah/autonlp-email-classification-657119381': {'modelId': 'esiebomajeremiah/autonlp-email-classification-657119381',\n",
       "  'sha': '484ba1babc3906d77331d95c1587aea7f3683637',\n",
       "  'lastModified': '2022-03-22T13:57:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:esiebomajeremiah/autonlp-data-email-classification',\n",
       "   'transformers',\n",
       "   'autonlp',\n",
       "   'co2_eq_emissions'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'esiebomajeremiah',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'esiebomajeremiah/autonlp-email-classification-657119381',\n",
       "  'downloads': 5281,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I love AutoNLP 🤗'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': 'autonlp',\n",
       "   'language': 'en',\n",
       "   'widget': [{'text': 'I love AutoNLP 🤗'}],\n",
       "   'datasets': ['esiebomajeremiah/autonlp-data-email-classification'],\n",
       "   'co2_eq_emissions': 3.516233232503715},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'BeIR/query-gen-msmarco-t5-base-v1': {'modelId': 'BeIR/query-gen-msmarco-t5-base-v1',\n",
       "  'sha': 'f86569026d333fbf584ce9f8d86b49dc521a7db0',\n",
       "  'lastModified': '2021-06-23T02:07:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'BeIR',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'BeIR/query-gen-msmarco-t5-base-v1',\n",
       "  'downloads': 5274,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'espnet/kamo-naoyuki-mini_an4_asr_train_raw_bpe_valid.acc.best': {'modelId': 'espnet/kamo-naoyuki-mini_an4_asr_train_raw_bpe_valid.acc.best',\n",
       "  'sha': '71320176d2a8de165fe0d2a8ef3687460e8af8d7',\n",
       "  'lastModified': '2021-07-02T12:57:18.000Z',\n",
       "  'tags': ['en',\n",
       "   'dataset:mini-an4',\n",
       "   'arxiv:1804.00015',\n",
       "   'espnet',\n",
       "   'audio',\n",
       "   'automatic-speech-recognition',\n",
       "   'license:cc-by-4.0'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'espnet',\n",
       "  'config': None,\n",
       "  'id': 'espnet/kamo-naoyuki-mini_an4_asr_train_raw_bpe_valid.acc.best',\n",
       "  'downloads': 5259,\n",
       "  'library_name': 'espnet',\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['espnet', 'audio', 'automatic-speech-recognition'],\n",
       "   'language': 'en',\n",
       "   'datasets': ['mini-an4'],\n",
       "   'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'valhalla/distilbart-mnli-12-9': {'modelId': 'valhalla/distilbart-mnli-12-9',\n",
       "  'sha': '66a037d826920a2f84a9d83edcbeb23a0951ed2e',\n",
       "  'lastModified': '2021-06-14T10:34:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bart',\n",
       "   'text-classification',\n",
       "   'dataset:mnli',\n",
       "   'transformers',\n",
       "   'distilbart',\n",
       "   'distilbart-mnli',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'valhalla',\n",
       "  'config': {'architectures': ['BartForSequenceClassification'],\n",
       "   'model_type': 'bart'},\n",
       "  'id': 'valhalla/distilbart-mnli-12-9',\n",
       "  'downloads': 5254,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system’s largest world. Konstantin Batygin did not set out to solve one of the solar system’s most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system’s missing “Planet Nine,” spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn’t rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: “Oh! This is how Europa formed.” Europa is one of Jupiter’s four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the Côte d’Azur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system’s formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['mnli'],\n",
       "   'tags': ['distilbart', 'distilbart-mnli'],\n",
       "   'pipeline_tag': 'zero-shot-classification'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sahri/indonesiasentiment': {'modelId': 'sahri/indonesiasentiment',\n",
       "  'sha': '99f38e6c1b34109bbf4a6d7c6556c56f5d2eef6a',\n",
       "  'lastModified': '2022-01-17T04:50:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'id',\n",
       "   'dataset:indonlu',\n",
       "   'arxiv:1907.11692',\n",
       "   'transformers',\n",
       "   'indonesian-roberta-base-sentiment-classifier',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'sahri',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'sahri/indonesiasentiment',\n",
       "  'downloads': 5236,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'tidak jelek tapi keren'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'id',\n",
       "   'tags': ['indonesian-roberta-base-sentiment-classifier'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['indonlu'],\n",
       "   'widget': [{'text': 'tidak jelek tapi keren'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cl-tohoku/bert-base-japanese-char': {'modelId': 'cl-tohoku/bert-base-japanese-char',\n",
       "  'sha': '6aa4c7bc39337858fee3e70f258edeada2e308ea',\n",
       "  'lastModified': '2021-09-23T13:45:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ja',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'cl-tohoku',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'cl-tohoku/bert-base-japanese-char',\n",
       "  'downloads': 5211,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '仙台は「[MASK]の都」と呼ばれている。'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'datasets': ['wikipedia'],\n",
       "   'widget': [{'text': '仙台は「[MASK]の都」と呼ばれている。'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ThomasSimonini/t5-end2end-question-generation': {'modelId': 'ThomasSimonini/t5-end2end-question-generation',\n",
       "  'sha': '1dda3f93db6cfa1e7fc84e1208d0a49febb5fb5c',\n",
       "  'lastModified': '2021-10-10T08:30:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'dataset:squad',\n",
       "   'transformers',\n",
       "   'generated_from_trainer',\n",
       "   'license:apache-2.0',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'ThomasSimonini',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'ThomasSimonini/t5-end2end-question-generation',\n",
       "  'downloads': 5197,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': {'error': 'Schema validation error. \"[0].results[0].metrics\" is required'},\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['generated_from_trainer'],\n",
       "   'datasets': ['squad'],\n",
       "   'model-index': [{'name': 't5-end2end-question-generation',\n",
       "     'results': [{'task': {'name': 'Sequence-to-sequence Language Modeling',\n",
       "        'type': 'text2text-generation'},\n",
       "       'dataset': {'name': 'squad',\n",
       "        'type': 'squad',\n",
       "        'args': 'plain_text'}}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/stsb-mpnet-base-v2': {'modelId': 'sentence-transformers/stsb-mpnet-base-v2',\n",
       "  'sha': '9f9d3d9da582d245066b519ab1e99c3f54a0594e',\n",
       "  'lastModified': '2021-08-05T08:31:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'mpnet',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['MPNetModel'], 'model_type': 'mpnet'},\n",
       "  'id': 'sentence-transformers/stsb-mpnet-base-v2',\n",
       "  'downloads': 5179,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-hu-en': {'modelId': 'Helsinki-NLP/opus-mt-hu-en',\n",
       "  'sha': '62599d9d6eca96022d26974486779623f09ebc9c',\n",
       "  'lastModified': '2021-09-09T22:10:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'hu',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-hu-en',\n",
       "  'downloads': 5178,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/muppet-roberta-large': {'modelId': 'facebook/muppet-roberta-large',\n",
       "  'sha': '87df24857474bf92dc6789bf1e5a8d73bc7510cb',\n",
       "  'lastModified': '2021-06-28T21:44:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:2101.11038',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'facebook/muppet-roberta-large',\n",
       "  'downloads': 5172,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'clue/albert_chinese_tiny': {'modelId': 'clue/albert_chinese_tiny',\n",
       "  'sha': '654acaf73c361ad56e4f4b1e2bb0023cbb1872b2',\n",
       "  'lastModified': '2020-12-11T21:35:55.000Z',\n",
       "  'tags': ['pytorch', 'albert', 'zh', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'clue',\n",
       "  'config': {'model_type': 'albert'},\n",
       "  'id': 'clue/albert_chinese_tiny',\n",
       "  'downloads': 5169,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'zh'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'sentence-transformers/all-roberta-large-v1': {'modelId': 'sentence-transformers/all-roberta-large-v1',\n",
       "  'sha': '42d37b9d8c9929c64dce4a2b25f6eaa0f59eaf99',\n",
       "  'lastModified': '2021-08-31T09:33:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'arxiv:1904.06472',\n",
       "   'arxiv:2102.07033',\n",
       "   'arxiv:2104.08727',\n",
       "   'arxiv:1704.05179',\n",
       "   'arxiv:1810.09305',\n",
       "   'sentence-transformers',\n",
       "   'feature-extraction',\n",
       "   'sentence-similarity',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/all-roberta-large-v1',\n",
       "  'downloads': 5152,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity'],\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'laxya007/gpt2_bd2': {'modelId': 'laxya007/gpt2_bd2',\n",
       "  'sha': '92cf1b3a812d597077a02d3b734294bff30d0840',\n",
       "  'lastModified': '2022-03-19T13:11:52.000Z',\n",
       "  'tags': ['pytorch', 'gpt2', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'laxya007',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'laxya007/gpt2_bd2',\n",
       "  'downloads': 5147,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'DB13067/Peterbot': {'modelId': 'DB13067/Peterbot',\n",
       "  'sha': '8562a504120603feadd5d9c676ea3e3f8c5ff72b',\n",
       "  'lastModified': '2022-03-14T13:51:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'DB13067',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'DB13067/Peterbot',\n",
       "  'downloads': 5130,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'fnlp/bart-large-chinese': {'modelId': 'fnlp/bart-large-chinese',\n",
       "  'sha': 'b47be247db39e74f5383784524c68bfddf0aa496',\n",
       "  'lastModified': '2021-10-29T05:19:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'feature-extraction',\n",
       "   'zh',\n",
       "   'arxiv:2109.05729',\n",
       "   'transformers',\n",
       "   'text2text-generation',\n",
       "   'Chinese',\n",
       "   'seq2seq'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'fnlp',\n",
       "  'config': {'architectures': ['BartModel'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'length_penalty': 1,\n",
       "     'max_length': 128,\n",
       "     'min_length': 12,\n",
       "     'num_beams': 4},\n",
       "    'summarization_cnn': {'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'num_beams': 4},\n",
       "    'summarization_xsum': {'length_penalty': 1,\n",
       "     'max_length': 62,\n",
       "     'min_length': 11,\n",
       "     'num_beams': 6}}},\n",
       "  'id': 'fnlp/bart-large-chinese',\n",
       "  'downloads': 5129,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['text2text-generation', 'Chinese', 'seq2seq'],\n",
       "   'language': 'zh'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/test-two-configs': {'modelId': 'hf-internal-testing/test-two-configs',\n",
       "  'sha': 'f7afc8d394d28773381f2b9797624982c25c045d',\n",
       "  'lastModified': '2022-01-24T23:27:07.000Z',\n",
       "  'tags': ['bert', 'fill-mask', 'transformers', 'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'hf-internal-testing/test-two-configs',\n",
       "  'downloads': 5126,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'stanfordnlp/stanza-en': {'modelId': 'stanfordnlp/stanza-en',\n",
       "  'sha': 'ca17c5a027df47832c797cadf6907b5d219811a1',\n",
       "  'lastModified': '2022-05-28T07:40:43.000Z',\n",
       "  'tags': ['en', 'stanza', 'token-classification', 'license:apache-2.0'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'stanfordnlp',\n",
       "  'config': None,\n",
       "  'id': 'stanfordnlp/stanza-en',\n",
       "  'downloads': 5102,\n",
       "  'library_name': 'stanza',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['stanza', 'token-classification'],\n",
       "   'library_name': 'stanza',\n",
       "   'language': 'en',\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'uw-hai/polyjuice': {'modelId': 'uw-hai/polyjuice',\n",
       "  'sha': 'f5bef2f7053c2ce6c3fd19875c3cff77754479ef',\n",
       "  'lastModified': '2021-05-24T01:21:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'counterfactual generation'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'uw-hai',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'uw-hai/polyjuice',\n",
       "  'downloads': 5078,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'It is great for kids. <|perturb|> [negation] It [BLANK] great for kids. [SEP]'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['counterfactual generation'],\n",
       "   'widget': [{'text': 'It is great for kids. <|perturb|> [negation] It [BLANK] great for kids. [SEP]'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bigscience/bigscience-small-testing': {'modelId': 'bigscience/bigscience-small-testing',\n",
       "  'sha': '9db81994d104ea82084a364542406cc52d7863d7',\n",
       "  'lastModified': '2022-06-27T18:30:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bloom',\n",
       "   'feature-extraction',\n",
       "   'eng',\n",
       "   'transformers',\n",
       "   'integration',\n",
       "   'text-generation'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'bigscience',\n",
       "  'config': {'architectures': ['BloomModel'], 'model_type': 'bloom'},\n",
       "  'id': 'bigscience/bigscience-small-testing',\n",
       "  'downloads': 5077,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['eng'],\n",
       "   'tags': ['integration'],\n",
       "   'pipeline_tag': 'text-generation'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/msmarco-distilbert-base-v3': {'modelId': 'sentence-transformers/msmarco-distilbert-base-v3',\n",
       "  'sha': 'cabff1a69c4aecef223c78fdb32c9f3fc4bda7dc',\n",
       "  'lastModified': '2022-06-15T21:45:04.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sentence-transformers/msmarco-distilbert-base-v3',\n",
       "  'downloads': 5075,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'PlanTL-GOB-ES/roberta-base-biomedical-clinical-es': {'modelId': 'PlanTL-GOB-ES/roberta-base-biomedical-clinical-es',\n",
       "  'sha': '617bf244e3106b6d50abfc600d62b858d798867d',\n",
       "  'lastModified': '2022-04-08T14:10:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'es',\n",
       "   'arxiv:2109.03570',\n",
       "   'arxiv:2109.07765',\n",
       "   'transformers',\n",
       "   'biomedical',\n",
       "   'clinical',\n",
       "   'spanish',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'PlanTL-GOB-ES',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'PlanTL-GOB-ES/roberta-base-biomedical-clinical-es',\n",
       "  'downloads': 5072,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'El único antecedente personal a reseñar era la <mask> arterial.'},\n",
       "   {'text': 'Las radiologías óseas de cuerpo entero no detectan alteraciones <mask>, ni alteraciones vertebrales.'},\n",
       "   {'text': 'En el <mask> toraco-abdómino-pélvico no se encontraron hallazgos patológicos de interés.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['es'],\n",
       "   'tags': ['biomedical', 'clinical', 'spanish'],\n",
       "   'license': 'apache-2.0',\n",
       "   'metrics': ['ppl'],\n",
       "   'widget': [{'text': 'El único antecedente personal a reseñar era la <mask> arterial.'},\n",
       "    {'text': 'Las radiologías óseas de cuerpo entero no detectan alteraciones <mask>, ni alteraciones vertebrales.'},\n",
       "    {'text': 'En el <mask> toraco-abdómino-pélvico no se encontraron hallazgos patológicos de interés.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'openai/clip-vit-base-patch16': {'modelId': 'openai/clip-vit-base-patch16',\n",
       "  'sha': '6cef4adda11be098f7c823c95de721298611f514',\n",
       "  'lastModified': '2022-03-14T18:00:36.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'clip',\n",
       "   'feature-extraction',\n",
       "   'arxiv:2103.00020',\n",
       "   'arxiv:1908.04913',\n",
       "   'transformers',\n",
       "   'vision'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'openai',\n",
       "  'config': {'architectures': ['CLIPModel'], 'model_type': 'clip'},\n",
       "  'id': 'openai/clip-vit-base-patch16',\n",
       "  'downloads': 5064,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['vision']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'human-centered-summarization/financial-summarization-pegasus': {'modelId': 'human-centered-summarization/financial-summarization-pegasus',\n",
       "  'sha': 'a720f829427cb196a5618a0416473b8597cd106e',\n",
       "  'lastModified': '2022-06-29T06:25:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'pegasus',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:xsum',\n",
       "   'arxiv:1912.08777',\n",
       "   'transformers',\n",
       "   'summarization',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'summarization',\n",
       "  'private': False,\n",
       "  'author': 'human-centered-summarization',\n",
       "  'config': {'architectures': ['PegasusForConditionalGeneration'],\n",
       "   'model_type': 'pegasus'},\n",
       "  'id': 'human-centered-summarization/financial-summarization-pegasus',\n",
       "  'downloads': 5058,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'National Commercial Bank (NCB), Saudi Arabia’s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Samba’s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf region’s third-largest lender. The entity’s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East’s biggest lender with about $268 billion of assets.'}],\n",
       "  'likes': 20,\n",
       "  'model-index': [{'name': 'human-centered-summarization/financial-summarization-pegasus',\n",
       "    'results': [{'task': {'type': 'summarization', 'name': 'Summarization'},\n",
       "      'dataset': {'name': 'xsum',\n",
       "       'type': 'xsum',\n",
       "       'config': 'default',\n",
       "       'split': 'test'},\n",
       "      'metrics': [{'name': 'ROUGE-1',\n",
       "        'type': 'rouge',\n",
       "        'value': 35.2055,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-2',\n",
       "        'type': 'rouge',\n",
       "        'value': 16.5689,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-L',\n",
       "        'type': 'rouge',\n",
       "        'value': 30.1285,\n",
       "        'verified': True},\n",
       "       {'name': 'ROUGE-LSUM',\n",
       "        'type': 'rouge',\n",
       "        'value': 30.1706,\n",
       "        'verified': True},\n",
       "       {'name': 'loss',\n",
       "        'type': 'loss',\n",
       "        'value': 2.7092134952545166,\n",
       "        'verified': True},\n",
       "       {'name': 'gen_len',\n",
       "        'type': 'gen_len',\n",
       "        'value': 15.1414,\n",
       "        'verified': True}]}]}],\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': 'summarization',\n",
       "   'datasets': ['xsum'],\n",
       "   'metrics': ['rouge'],\n",
       "   'widget': [{'text': 'National Commercial Bank (NCB), Saudi Arabia’s largest lender by assets, agreed to buy rival Samba Financial Group for $15 billion in the biggest banking takeover this year.NCB will pay 28.45 riyals ($7.58) for each Samba share, according to a statement on Sunday, valuing it at about 55.7 billion riyals. NCB will offer 0.739 new shares for each Samba share, at the lower end of the 0.736-0.787 ratio the banks set when they signed an initial framework agreement in June.The offer is a 3.5% premium to Samba’s Oct. 8 closing price of 27.50 riyals and about 24% higher than the level the shares traded at before the talks were made public. Bloomberg News first reported the merger discussions.The new bank will have total assets of more than $220 billion, creating the Gulf region’s third-largest lender. The entity’s $46 billion market capitalization nearly matches that of Qatar National Bank QPSC, which is still the Middle East’s biggest lender with about $268 billion of assets.'}],\n",
       "   'model-index': [{'name': 'human-centered-summarization/financial-summarization-pegasus',\n",
       "     'results': [{'task': {'type': 'summarization', 'name': 'Summarization'},\n",
       "       'dataset': {'name': 'xsum',\n",
       "        'type': 'xsum',\n",
       "        'config': 'default',\n",
       "        'split': 'test'},\n",
       "       'metrics': [{'name': 'ROUGE-1',\n",
       "         'type': 'rouge',\n",
       "         'value': 35.2055,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-2',\n",
       "         'type': 'rouge',\n",
       "         'value': 16.5689,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-L',\n",
       "         'type': 'rouge',\n",
       "         'value': 30.1285,\n",
       "         'verified': True},\n",
       "        {'name': 'ROUGE-LSUM',\n",
       "         'type': 'rouge',\n",
       "         'value': 30.1706,\n",
       "         'verified': True},\n",
       "        {'name': 'loss',\n",
       "         'type': 'loss',\n",
       "         'value': 2.7092134952545166,\n",
       "         'verified': True},\n",
       "        {'name': 'gen_len',\n",
       "         'type': 'gen_len',\n",
       "         'value': 15.1414,\n",
       "         'verified': True}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco': {'modelId': 'sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco',\n",
       "  'sha': 'a03882cb371476b74ca3557366452cd868ac4f42',\n",
       "  'lastModified': '2021-04-15T08:54:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'distilbert',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'dataset:ms_marco',\n",
       "   'arxiv:2104.06967',\n",
       "   'transformers',\n",
       "   'dpr',\n",
       "   'dense-passage-retrieval',\n",
       "   'knowledge-distillation'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'sebastian-hofstaetter',\n",
       "  'config': {'architectures': ['DistilBertModel'], 'model_type': 'distilbert'},\n",
       "  'id': 'sebastian-hofstaetter/distilbert-dot-tas_b-b256-msmarco',\n",
       "  'downloads': 5056,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 9,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['dpr', 'dense-passage-retrieval', 'knowledge-distillation'],\n",
       "   'datasets': ['ms_marco']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nreimers/BERT-Tiny_L-2_H-128_A-2': {'modelId': 'nreimers/BERT-Tiny_L-2_H-128_A-2',\n",
       "  'sha': '9cb03776b08d300ae73aa6ba4860a760c606f62d',\n",
       "  'lastModified': '2021-05-28T11:05:21.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'nreimers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'nreimers/BERT-Tiny_L-2_H-128_A-2',\n",
       "  'downloads': 5003,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'uer/chinese_roberta_L-2_H-128': {'modelId': 'uer/chinese_roberta_L-2_H-128',\n",
       "  'sha': '3c66eff1fba8d47108e92380f9b7be15356b1774',\n",
       "  'lastModified': '2022-02-19T08:59:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'Chinese',\n",
       "   'dataset:CLUECorpusSmall',\n",
       "   'arxiv:1909.05658',\n",
       "   'arxiv:1908.08962',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'uer',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'uer/chinese_roberta_L-2_H-128',\n",
       "  'downloads': 4996,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '北京是[MASK]国的首都。'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'Chinese',\n",
       "   'datasets': 'CLUECorpusSmall',\n",
       "   'widget': [{'text': '北京是[MASK]国的首都。'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'voidful/albert_chinese_tiny': {'modelId': 'voidful/albert_chinese_tiny',\n",
       "  'sha': 'd40f566a40f057e5d8a6f7b2cd5171a4f104126f',\n",
       "  'lastModified': '2021-08-03T05:07:02.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'voidful',\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'voidful/albert_chinese_tiny',\n",
       "  'downloads': 4996,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '今天[MASK]情很好'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'zh',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'widget': [{'text': '今天[MASK]情很好'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-uk-en': {'modelId': 'Helsinki-NLP/opus-mt-uk-en',\n",
       "  'sha': 'd6c1e62ab5c03e34a3d118382be7a27b704241f0',\n",
       "  'lastModified': '2021-09-11T10:51:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'uk',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-uk-en',\n",
       "  'downloads': 4980,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Мене звати Вольфґанґ і я живу в Берліні.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sshleifer/tiny-xlnet-base-cased': {'modelId': 'sshleifer/tiny-xlnet-base-cased',\n",
       "  'sha': '275d2c323ddd18dad60cd585934383c29027878b',\n",
       "  'lastModified': '2020-05-08T15:35:32.000Z',\n",
       "  'tags': ['pytorch', 'xlnet', 'text-generation', 'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'sshleifer',\n",
       "  'config': {'architectures': ['XLNetLMHeadModel'],\n",
       "   'model_type': 'xlnet',\n",
       "   'task_specific_params': {'text_generation': {'max_length': 210}}},\n",
       "  'id': 'sshleifer/tiny-xlnet-base-cased',\n",
       "  'downloads': 4924,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/detr-resnet-50-panoptic': {'modelId': 'facebook/detr-resnet-50-panoptic',\n",
       "  'sha': 'fc15262cfd4c13cbdad6d1d55ff0cd31a2251a27',\n",
       "  'lastModified': '2022-06-27T08:30:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'detr',\n",
       "   'image-segmentation',\n",
       "   'dataset:coco',\n",
       "   'arxiv:2005.12872',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-segmentation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['DetrForSegmentation'], 'model_type': 'detr'},\n",
       "  'id': 'facebook/detr-resnet-50-panoptic',\n",
       "  'downloads': 4903,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg',\n",
       "    'example_title': 'Football Match'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/dog-cat.jpg',\n",
       "    'example_title': 'Dog & Cat'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/construction-site.jpg',\n",
       "    'example_title': 'Construction Site'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/apple-orange.jpg',\n",
       "    'example_title': 'Apple & Orange'}],\n",
       "  'likes': 25,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['image-segmentation', 'vision'],\n",
       "   'datasets': ['coco'],\n",
       "   'widget': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/football-match.jpg',\n",
       "     'example_title': 'Football Match'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/dog-cat.jpg',\n",
       "     'example_title': 'Dog & Cat'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/construction-site.jpg',\n",
       "     'example_title': 'Construction Site'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/apple-orange.jpg',\n",
       "     'example_title': 'Apple & Orange'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageSegmentation',\n",
       "   'pipeline_tag': 'image-segmentation',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'Helsinki-NLP/opus-mt-en-fi': {'modelId': 'Helsinki-NLP/opus-mt-en-fi',\n",
       "  'sha': '627fe90df5c335be61521cd89c68f62e2bdce050',\n",
       "  'lastModified': '2021-09-09T21:35:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'fi',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-en-fi',\n",
       "  'downloads': 4877,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'stanleychu2/t5-transition': {'modelId': 'stanleychu2/t5-transition',\n",
       "  'sha': '3188c7352c52d6ee172c050a3346f6a10bfd6635',\n",
       "  'lastModified': '2022-03-07T09:06:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'stanleychu2',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'stanleychu2/t5-transition',\n",
       "  'downloads': 4876,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'CAMeL-Lab/bert-base-arabic-camelbert-mix-ner': {'modelId': 'CAMeL-Lab/bert-base-arabic-camelbert-mix-ner',\n",
       "  'sha': '40b8059a6f3bfbb49d64038e131f49b93cc37417',\n",
       "  'lastModified': '2021-10-17T11:13:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'ar',\n",
       "   'arxiv:2103.06678',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'CAMeL-Lab',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'CAMeL-Lab/bert-base-arabic-camelbert-mix-ner',\n",
       "  'downloads': 4798,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'إمارة أبوظبي هي إحدى إمارات دولة الإمارات العربية المتحدة السبع'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ar'],\n",
       "   'license': 'apache-2.0',\n",
       "   'widget': [{'text': 'إمارة أبوظبي هي إحدى إمارات دولة الإمارات العربية المتحدة السبع'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/rag-sequence-base': {'modelId': 'facebook/rag-sequence-base',\n",
       "  'sha': '7c7ae51878178639f47b6d416bef67a35a5a41f9',\n",
       "  'lastModified': '2020-12-11T21:39:37.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rag',\n",
       "   'arxiv:2005.11401',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['RagSequenceForGeneration'],\n",
       "   'model_type': 'rag'},\n",
       "  'id': 'facebook/rag-sequence-base',\n",
       "  'downloads': 4780,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/facebook.png'},\n",
       "  'transformersInfo': {'auto_model': 'RagSequenceForGeneration',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'patrickvonplaten/longformer-random-tiny': {'modelId': 'patrickvonplaten/longformer-random-tiny',\n",
       "  'sha': '8f15d46e686753d8c1ffb6e876fa90740a1c32c3',\n",
       "  'lastModified': '2020-08-05T09:22:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'longformer',\n",
       "   'feature-extraction',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'patrickvonplaten',\n",
       "  'config': {'architectures': ['LongformerModel'], 'model_type': 'longformer'},\n",
       "  'id': 'patrickvonplaten/longformer-random-tiny',\n",
       "  'downloads': 4773,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'textattack/bert-base-uncased-MRPC': {'modelId': 'textattack/bert-base-uncased-MRPC',\n",
       "  'sha': 'd421614df8fbeb22d6826a24d6397809fdc1e3ff',\n",
       "  'lastModified': '2021-05-20T07:32:52.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'textattack',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'textattack/bert-base-uncased-MRPC',\n",
       "  'downloads': 4772,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'tprincessazula/Dialog-GPT-small-KATARA-AVATAR': {'modelId': 'tprincessazula/Dialog-GPT-small-KATARA-AVATAR',\n",
       "  'sha': '9e7c17b7f5ef120e895120c49721f3a000e5a240',\n",
       "  'lastModified': '2022-01-05T13:46:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'tprincessazula',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'tprincessazula/Dialog-GPT-small-KATARA-AVATAR',\n",
       "  'downloads': 4767,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'MoritzLaurer/mDeBERTa-v3-base-mnli-xnli': {'modelId': 'MoritzLaurer/mDeBERTa-v3-base-mnli-xnli',\n",
       "  'sha': 'dada2f51640768682a07331e7c244c0d25e9dc85',\n",
       "  'lastModified': '2022-06-18T09:27:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'deberta-v2',\n",
       "   'text-classification',\n",
       "   'multilingual',\n",
       "   'en',\n",
       "   'ar',\n",
       "   'bg',\n",
       "   'de',\n",
       "   'el',\n",
       "   'es',\n",
       "   'fr',\n",
       "   'hi',\n",
       "   'ru',\n",
       "   'sw',\n",
       "   'th',\n",
       "   'tr',\n",
       "   'ur',\n",
       "   'vu',\n",
       "   'zh',\n",
       "   'dataset:multi_nli',\n",
       "   'dataset:xnli',\n",
       "   'arxiv:2111.09543',\n",
       "   'arxiv:1809.05053',\n",
       "   'arxiv:1911.02116',\n",
       "   'transformers',\n",
       "   'zero-shot-classification',\n",
       "   'nli'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'MoritzLaurer',\n",
       "  'config': {'architectures': ['DebertaV2ForSequenceClassification'],\n",
       "   'model_type': 'deberta-v2'},\n",
       "  'id': 'MoritzLaurer/mDeBERTa-v3-base-mnli-xnli',\n",
       "  'downloads': 4735,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU',\n",
       "    'candidate_labels': 'politics, economy, entertainment, environment'}],\n",
       "  'likes': 26,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'en',\n",
       "    'ar',\n",
       "    'bg',\n",
       "    'de',\n",
       "    'el',\n",
       "    'es',\n",
       "    'fr',\n",
       "    'hi',\n",
       "    'ru',\n",
       "    'sw',\n",
       "    'th',\n",
       "    'tr',\n",
       "    'ur',\n",
       "    'vu',\n",
       "    'zh'],\n",
       "   'tags': ['zero-shot-classification',\n",
       "    'text-classification',\n",
       "    'nli',\n",
       "    'pytorch'],\n",
       "   'metrics': ['accuracy'],\n",
       "   'datasets': ['multi_nli', 'xnli'],\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'widget': [{'text': 'Angela Merkel ist eine Politikerin in Deutschland und Vorsitzende der CDU',\n",
       "     'candidate_labels': 'politics, economy, entertainment, environment'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Salesforce/grappa_large_jnt': {'modelId': 'Salesforce/grappa_large_jnt',\n",
       "  'sha': '0d2500a00f3a4b71addaf09a0c1abe30788362dd',\n",
       "  'lastModified': '2021-05-20T12:23:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Salesforce',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'Salesforce/grappa_large_jnt',\n",
       "  'downloads': 4727,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ImAPizza/DialoGPT-medium-alberttwo': {'modelId': 'ImAPizza/DialoGPT-medium-alberttwo',\n",
       "  'sha': 'bedcf2148b3c45ebc5c0c8632d41fe4f4cde1d9f',\n",
       "  'lastModified': '2021-08-29T13:39:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'ImAPizza',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'ImAPizza/DialoGPT-medium-alberttwo',\n",
       "  'downloads': 4723,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/msmarco-MiniLM-L-6-v3': {'modelId': 'sentence-transformers/msmarco-MiniLM-L-6-v3',\n",
       "  'sha': '195276c0c8647b99dfe128bd8bc4ecd1a66d41f8',\n",
       "  'lastModified': '2022-06-15T21:52:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sentence-transformers/msmarco-MiniLM-L-6-v3',\n",
       "  'downloads': 4710,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Parth/result': {'modelId': 'Parth/result',\n",
       "  'sha': 'b06ca90f3eda5e5ee4e1dc3a55714e7cb5ffcc00',\n",
       "  'lastModified': '2021-06-23T03:47:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'Parth',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'Parth/result',\n",
       "  'downloads': 4704,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nghuyong/ernie-gram-zh': {'modelId': 'nghuyong/ernie-gram-zh',\n",
       "  'sha': '257fee0915f1cba8dbea92c976493dcdd0491174',\n",
       "  'lastModified': '2022-04-04T06:00:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'zh',\n",
       "   'arxiv:2010.12148',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'nghuyong',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'nghuyong/ernie-gram-zh',\n",
       "  'downloads': 4698,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'zh'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/dpr-ctx_encoder-multiset-base': {'modelId': 'facebook/dpr-ctx_encoder-multiset-base',\n",
       "  'sha': '6c01adf9e9e7c812c0fa998fed97eec3262c2cf4',\n",
       "  'lastModified': '2020-11-25T16:58:57.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'dpr', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['DPRContextEncoder'], 'model_type': 'dpr'},\n",
       "  'id': 'facebook/dpr-ctx_encoder-multiset-base',\n",
       "  'downloads': 4688,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'DPRContextEncoder',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/DialogRPT-updown': {'modelId': 'microsoft/DialogRPT-updown',\n",
       "  'sha': 'afe1247fd7e1b3abea28a52ea72db4ce1c8d2186',\n",
       "  'lastModified': '2021-05-23T09:19:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-classification',\n",
       "   'arxiv:2009.06978',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['GPT2ForSequenceClassification'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'microsoft/DialogRPT-updown',\n",
       "  'downloads': 4686,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hakurei/lit-6B': {'modelId': 'hakurei/lit-6B',\n",
       "  'sha': 'cc2e78adb62590cb2889d338d46f8cf1ef396453',\n",
       "  'lastModified': '2021-11-08T23:02:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gptj',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'causal-lm',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'hakurei',\n",
       "  'config': {'architectures': ['GPTJForCausalLM'],\n",
       "   'model_type': 'gptj',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'temperature': 1,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'hakurei/lit-6B',\n",
       "  'downloads': 4663,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['pytorch', 'causal-lm'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-sla-en': {'modelId': 'Helsinki-NLP/opus-mt-sla-en',\n",
       "  'sha': 'ad888a10d15c0cbe1e45b94c18e260fdc19035f7',\n",
       "  'lastModified': '2020-08-21T14:42:49.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'be',\n",
       "   'hr',\n",
       "   'mk',\n",
       "   'cs',\n",
       "   'ru',\n",
       "   'pl',\n",
       "   'bg',\n",
       "   'uk',\n",
       "   'sl',\n",
       "   'sla',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-sla-en',\n",
       "  'downloads': 4658,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 0,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['be',\n",
       "    'hr',\n",
       "    'mk',\n",
       "    'cs',\n",
       "    'ru',\n",
       "    'pl',\n",
       "    'bg',\n",
       "    'uk',\n",
       "    'sl',\n",
       "    'sla',\n",
       "    'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'mrm8488/bert-italian-finedtuned-squadv1-it-alfa': {'modelId': 'mrm8488/bert-italian-finedtuned-squadv1-it-alfa',\n",
       "  'sha': '829047a5ce1a8ef73bde97666eff10b8e128b42e',\n",
       "  'lastModified': '2021-05-20T00:24:19.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'it',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'mrm8488',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'mrm8488/bert-italian-finedtuned-squadv1-it-alfa',\n",
       "  'downloads': 4610,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Dove vivo?',\n",
       "    'context': 'Mi chiamo Wolfgang e vivo a Berlino'},\n",
       "   {'text': 'Dove vivo?', 'context': 'Mi chiamo Sarah e vivo a Londra'},\n",
       "   {'text': 'Come mio chiamo?',\n",
       "    'context': 'Mi chiamo Clara e vivo a Berkeley.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'it', 'thumbnail': None},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sonoisa/sentence-bert-base-ja-mean-tokens-v2': {'modelId': 'sonoisa/sentence-bert-base-ja-mean-tokens-v2',\n",
       "  'sha': 'a230680fdb31ed495808f08e2d700361dc982542',\n",
       "  'lastModified': '2021-12-26T08:33:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'ja',\n",
       "   'sentence-transformers',\n",
       "   'sentence-bert',\n",
       "   'sentence-similarity',\n",
       "   'license:cc-by-sa-4.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'sonoisa',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'sonoisa/sentence-bert-base-ja-mean-tokens-v2',\n",
       "  'downloads': 4589,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'sentence-bert',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'blanchefort/rubert-base-cased-sentiment': {'modelId': 'blanchefort/rubert-base-cased-sentiment',\n",
       "  'sha': '1dfb5bcf1904a12eb157a0dfaf06029e606ce7c7',\n",
       "  'lastModified': '2021-05-19T13:05:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'ru',\n",
       "   'transformers',\n",
       "   'sentiment'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'blanchefort',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'blanchefort/rubert-base-cased-sentiment',\n",
       "  'downloads': 4583,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Ты мне нравишься. Я тебя люблю'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ru'],\n",
       "   'tags': ['sentiment', 'text-classification']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deep-learning-analytics/GrammarCorrector': {'modelId': 'deep-learning-analytics/GrammarCorrector',\n",
       "  'sha': '6ca90bd771c373a0542d4257a5c34d26cd0d3c59',\n",
       "  'lastModified': '2021-12-23T02:51:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'deep-learning-analytics',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'deep-learning-analytics/GrammarCorrector',\n",
       "  'downloads': 4571,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/chinese-pert-base': {'modelId': 'hfl/chinese-pert-base',\n",
       "  'sha': '54f84f9b553c9184d92e1d476010299aac42cf86',\n",
       "  'lastModified': '2022-02-24T02:57:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'license:cc-by-nc-sa-4.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'hfl/chinese-pert-base',\n",
       "  'downloads': 4565,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'license': 'cc-by-nc-sa-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-th-en': {'modelId': 'Helsinki-NLP/opus-mt-th-en',\n",
       "  'sha': '90080f69e69c567e2b145fc8723c1e53f4f760e6',\n",
       "  'lastModified': '2020-08-21T14:42:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'th',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-th-en',\n",
       "  'downloads': 4548,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['th', 'en'],\n",
       "   'tags': ['translation'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/muppet-roberta-base': {'modelId': 'facebook/muppet-roberta-base',\n",
       "  'sha': 'caf238c63db946bdfbd00575713462838e823997',\n",
       "  'lastModified': '2021-06-28T21:44:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:2101.11038',\n",
       "   'transformers',\n",
       "   'exbert',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'facebook/muppet-roberta-base',\n",
       "  'downloads': 4543,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['exbert'],\n",
       "   'license': 'mit',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allegro/herbert-klej-cased-tokenizer-v1': {'modelId': 'allegro/herbert-klej-cased-tokenizer-v1',\n",
       "  'sha': '22f03e76fc129ea8f15a92e7213d5beeba83f4c8',\n",
       "  'lastModified': '2021-05-28T16:19:05.000Z',\n",
       "  'tags': ['xlm', 'pl', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'allegro',\n",
       "  'config': {'model_type': 'xlm'},\n",
       "  'id': 'allegro/herbert-klej-cased-tokenizer-v1',\n",
       "  'downloads': 4505,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<special1>',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'pl'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'asi/gpt-fr-cased-small': {'modelId': 'asi/gpt-fr-cased-small',\n",
       "  'sha': 'd4ddd1d506690415df78683829f5aba3878888a3',\n",
       "  'lastModified': '2021-06-30T13:47:26.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'fr',\n",
       "   'transformers',\n",
       "   'text-generation',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'asi',\n",
       "  'config': {'model_type': 'gpt2'},\n",
       "  'id': 'asi/gpt-fr-cased-small',\n",
       "  'downloads': 4500,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': \"Mon nom est Julien et j'aime\"},\n",
       "   {'text': 'Mon nom est Thomas et mon principal'},\n",
       "   {'text': 'Il était une fois'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['fr'],\n",
       "   'tags': ['tf', 'pytorch', 'gpt2', 'text-generation'],\n",
       "   'license': 'apache-2.0',\n",
       "   'thumbnail': 'https://raw.githubusercontent.com/AntoineSimoulin/gpt-fr/main/imgs/logo.png'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'zenham/wail_m_e4_16h_2k': {'modelId': 'zenham/wail_m_e4_16h_2k',\n",
       "  'sha': '5ac007e21763708150470814fc800ab1e72c9582',\n",
       "  'lastModified': '2022-03-10T03:06:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'zenham',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'zenham/wail_m_e4_16h_2k',\n",
       "  'downloads': 4493,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'akreal/tiny-random-bert': {'modelId': 'akreal/tiny-random-bert',\n",
       "  'sha': '843b6aea20ebae1c96598b6187b1bc26c105652a',\n",
       "  'lastModified': '2021-08-18T14:42:20.000Z',\n",
       "  'tags': ['pytorch', 'tf', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'akreal',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'akreal/tiny-random-bert',\n",
       "  'downloads': 4477,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'abjbpi/Dwight_Schrute': {'modelId': 'abjbpi/Dwight_Schrute',\n",
       "  'sha': '451aab582fe08f5210a58859f9ec1c79278e341b',\n",
       "  'lastModified': '2021-06-04T11:43:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'abjbpi',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'abjbpi/Dwight_Schrute',\n",
       "  'downloads': 4473,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ydshieh/wav2vec2-large-xlsr-53-chinese-zh-cn-gpt': {'modelId': 'ydshieh/wav2vec2-large-xlsr-53-chinese-zh-cn-gpt',\n",
       "  'sha': 'cf511f7e00de089e67e80fbedd5fbfb8e76ea067',\n",
       "  'lastModified': '2021-04-01T14:09:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'zh',\n",
       "   'dataset:common_voice',\n",
       "   'transformers',\n",
       "   'audio',\n",
       "   'speech',\n",
       "   'xlsr-fine-tuning-week',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'ydshieh',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'ydshieh/wav2vec2-large-xlsr-53-chinese-zh-cn-gpt',\n",
       "  'downloads': 4473,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 9,\n",
       "  'model-index': [{'name': 'XLSR Wav2Vec2 Large 53 - Chinese (zh-CN), by Yih-Dar SHIEH',\n",
       "    'results': [{'task': {'name': 'Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Common Voice zh-CN',\n",
       "       'type': 'common_voice',\n",
       "       'args': 'zh-CN'},\n",
       "      'metrics': [{'name': 'Test CER', 'type': 'cer', 'value': 20.9}]}]}],\n",
       "  'cardData': {'language': 'zh',\n",
       "   'datasets': ['common_voice'],\n",
       "   'metrics': ['cer'],\n",
       "   'tags': ['audio',\n",
       "    'automatic-speech-recognition',\n",
       "    'speech',\n",
       "    'xlsr-fine-tuning-week'],\n",
       "   'license': 'apache-2.0',\n",
       "   'model-index': [{'name': 'XLSR Wav2Vec2 Large 53 - Chinese (zh-CN), by Yih-Dar SHIEH',\n",
       "     'results': [{'task': {'name': 'Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Common Voice zh-CN',\n",
       "        'type': 'common_voice',\n",
       "        'args': 'zh-CN'},\n",
       "       'metrics': [{'name': 'Test CER', 'type': 'cer', 'value': 20.9}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'benjamin/gerpt2-large': {'modelId': 'benjamin/gerpt2-large',\n",
       "  'sha': 'd0ec9b299d7a96e24d03303de120ffb81769f366',\n",
       "  'lastModified': '2022-05-11T09:16:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'de',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'benjamin',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'benjamin/gerpt2-large',\n",
       "  'downloads': 4459,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'In einer schockierenden Entdeckung fanden Wissenschaftler eine Herde Einhörner, die in einem abgelegenen, zuvor unerforschten Tal in den Anden lebten.'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'de',\n",
       "   'widget': [{'text': 'In einer schockierenden Entdeckung fanden Wissenschaftler eine Herde Einhörner, die in einem abgelegenen, zuvor unerforschten Tal in den Anden lebten.'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cl-tohoku/bert-base-japanese-char-v2': {'modelId': 'cl-tohoku/bert-base-japanese-char-v2',\n",
       "  'sha': 'e17e40a15857ad47d63f6eb4cc9fb62c136d2301',\n",
       "  'lastModified': '2021-09-23T13:45:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'ja',\n",
       "   'dataset:wikipedia',\n",
       "   'transformers',\n",
       "   'license:cc-by-sa-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'cl-tohoku',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'cl-tohoku/bert-base-japanese-char-v2',\n",
       "  'downloads': 4453,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '東北大学で[MASK]の研究をしています。'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ja',\n",
       "   'license': 'cc-by-sa-4.0',\n",
       "   'datasets': ['wikipedia'],\n",
       "   'widget': [{'text': '東北大学で[MASK]の研究をしています。'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/flava-full': {'modelId': 'facebook/flava-full',\n",
       "  'sha': '57949b6a84a80fd01c3dd62a09450d8670f1c418',\n",
       "  'lastModified': '2022-05-25T07:53:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'flava',\n",
       "   'pretraining',\n",
       "   'arxiv:2112.04482',\n",
       "   'arxiv:2108.10904',\n",
       "   'transformers',\n",
       "   'license:bsd-3-clause'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['FlavaForPreTraining'], 'model_type': 'flava'},\n",
       "  'id': 'facebook/flava-full',\n",
       "  'downloads': 4451,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'bsd-3-clause'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'hfl/chinese-xlnet-base': {'modelId': 'hfl/chinese-xlnet-base',\n",
       "  'sha': '34b827684078f956411389834966eb55588f5254',\n",
       "  'lastModified': '2021-03-03T01:44:59.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlnet',\n",
       "   'text-generation',\n",
       "   'zh',\n",
       "   'arxiv:2004.13922',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['XLNetLMHeadModel'], 'model_type': 'xlnet'},\n",
       "  'id': 'hfl/chinese-xlnet-base',\n",
       "  'downloads': 4450,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': '我叫朱利安，我喜欢'},\n",
       "   {'text': '我叫托马斯，我的主要'},\n",
       "   {'text': '我叫玛丽亚，我最喜欢的'},\n",
       "   {'text': '我叫克拉拉，我是'},\n",
       "   {'text': '从前，'}],\n",
       "  'likes': 11,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Zixtrauce/BaekBot': {'modelId': 'Zixtrauce/BaekBot',\n",
       "  'sha': '8dd7cbc7e2f1f6d711330aabb4a165d87a8dc7f5',\n",
       "  'lastModified': '2021-12-31T08:30:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'Zixtrauce',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'Zixtrauce/BaekBot',\n",
       "  'downloads': 4439,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/xtremedistil-l6-h384-uncased': {'modelId': 'microsoft/xtremedistil-l6-h384-uncased',\n",
       "  'sha': '359df7d52613d4edc15647e6d65e0d87200eb747',\n",
       "  'lastModified': '2021-08-05T17:48:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'arxiv:2106.04563',\n",
       "   'transformers',\n",
       "   'text-classification',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'microsoft/xtremedistil-l6-h384-uncased',\n",
       "  'downloads': 4434,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 15,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'tags': ['text-classification'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'aware-ai/roberta-large-squadv2': {'modelId': 'aware-ai/roberta-large-squadv2',\n",
       "  'sha': '59a93e1104aa42295190ecec42bf829fbc83b0bb',\n",
       "  'lastModified': '2021-05-20T12:37:36.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'aware-ai',\n",
       "  'config': {'architectures': ['RobertaForQuestionAnswering'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'aware-ai/roberta-large-squadv2',\n",
       "  'downloads': 4421,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'stas/t5-very-small-random': {'modelId': 'stas/t5-very-small-random',\n",
       "  'sha': '988f491d1f2b837d47895885d96b1d4992a25d0e',\n",
       "  'lastModified': '2021-04-21T02:34:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'stas',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 200,\n",
       "     'min_length': 30,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'summarize: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to German: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to French: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'max_length': 300,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'translate English to Romanian: '}}},\n",
       "  'id': 'stas/t5-very-small-random',\n",
       "  'downloads': 4417,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/t5-large-lm-adapt': {'modelId': 'google/t5-large-lm-adapt',\n",
       "  'sha': '96ce18564557a62d6ff1cb3771af167433827961',\n",
       "  'lastModified': '2021-11-01T14:00:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:c4',\n",
       "   'arxiv:2002.05202',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   't5-lm-adapt',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'google/t5-large-lm-adapt',\n",
       "  'downloads': 4415,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'datasets': ['c4'],\n",
       "   'tags': ['t5-lm-adapt'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli': {'modelId': 'symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli',\n",
       "  'sha': 'e6331eecd7f50dd13c8ebcdc57a9f0a22f2ff56e',\n",
       "  'lastModified': '2021-09-30T11:27:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'feature-extraction',\n",
       "   'ar',\n",
       "   'bg',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'es',\n",
       "   'fr',\n",
       "   'ru',\n",
       "   'th',\n",
       "   'tr',\n",
       "   'ur',\n",
       "   'vn',\n",
       "   'zh',\n",
       "   'dataset:SNLI',\n",
       "   'dataset:MNLI',\n",
       "   'dataset:ANLI',\n",
       "   'dataset:XNLI',\n",
       "   'sentence-transformers',\n",
       "   'zero-shot-classification',\n",
       "   'sentence-similarity',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'symanto',\n",
       "  'config': {'architectures': ['XLMRobertaModel'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'symanto/sn-xlm-roberta-base-snli-mnli-anli-xnli',\n",
       "  'downloads': 4415,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'هذا شخص سعيد',\n",
       "    'sentences': ['هذا كلب سعيد', 'هذا شخص سعيد جدا', 'اليوم هو يوم مشمس']}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ar',\n",
       "    'bg',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'es',\n",
       "    'fr',\n",
       "    'ru',\n",
       "    'th',\n",
       "    'tr',\n",
       "    'ur',\n",
       "    'vn',\n",
       "    'zh'],\n",
       "   'datasets': ['SNLI', 'MNLI', 'ANLI', 'XNLI'],\n",
       "   'pipeline_tag': 'sentence-similarity',\n",
       "   'tags': ['zero-shot-classification',\n",
       "    'sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Huffon/sentence-klue-roberta-base': {'modelId': 'Huffon/sentence-klue-roberta-base',\n",
       "  'sha': 'a5aca746f7931205aa44992e81fdeb7faf7c443c',\n",
       "  'lastModified': '2021-06-20T17:32:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'ko',\n",
       "   'dataset:klue',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'Huffon',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'Huffon/sentence-klue-roberta-base',\n",
       "  'downloads': 4410,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ko',\n",
       "   'tags': ['roberta', 'sentence-transformers'],\n",
       "   'datasets': ['klue']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Helsinki-NLP/opus-mt-ca-en': {'modelId': 'Helsinki-NLP/opus-mt-ca-en',\n",
       "  'sha': '22113f5e0e8e89677d6e0142e55c85402eecb455',\n",
       "  'lastModified': '2021-09-09T21:28:18.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'marian',\n",
       "   'text2text-generation',\n",
       "   'ca',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'translation',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'translation',\n",
       "  'private': False,\n",
       "  'author': 'Helsinki-NLP',\n",
       "  'config': {'architectures': ['MarianMTModel'], 'model_type': 'marian'},\n",
       "  'id': 'Helsinki-NLP/opus-mt-ca-en',\n",
       "  'downloads': 4380,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['translation'], 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'OFA-Sys/OFA-base': {'modelId': 'OFA-Sys/OFA-base',\n",
       "  'sha': '965ce12f22b00caeb1a45b9f69559c4818ab3fd1',\n",
       "  'lastModified': '2022-05-03T06:03:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'ofa',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'OFA-Sys',\n",
       "  'config': {'architectures': ['OFAForConditionalGeneration'],\n",
       "   'model_type': 'ofa'},\n",
       "  'id': 'OFA-Sys/OFA-base',\n",
       "  'downloads': 4351,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 0,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation'}},\n",
       " 'nlpconnect/roberta-base-squad2-nq': {'modelId': 'nlpconnect/roberta-base-squad2-nq',\n",
       "  'sha': 'b925b1c7e676c8b46fb103307fc9f065bd46f392',\n",
       "  'lastModified': '2021-08-31T03:56:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'question-answering',\n",
       "   'dataset:squad_v2',\n",
       "   'dataset:natural_questions',\n",
       "   'transformers',\n",
       "   'qa',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'nlpconnect',\n",
       "  'config': {'architectures': ['RobertaForQuestionAnswering'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'nlpconnect/roberta-base-squad2-nq',\n",
       "  'downloads': 4348,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['qa'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['squad_v2', 'natural_questions']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'csebuetnlp/banglabert': {'modelId': 'csebuetnlp/banglabert',\n",
       "  'sha': '7bed1e381af5564564faadc9718f25c6116491e0',\n",
       "  'lastModified': '2022-05-10T05:17:06.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'electra',\n",
       "   'pretraining',\n",
       "   'bn',\n",
       "   'arxiv:2101.00204',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'csebuetnlp',\n",
       "  'config': {'architectures': ['ElectraForPreTraining'],\n",
       "   'model_type': 'electra'},\n",
       "  'id': 'csebuetnlp/banglabert',\n",
       "  'downloads': 4319,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['bn'], 'licenses': ['cc-by-nc-sa-4.0']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForPreTraining',\n",
       "   'pipeline_tag': 'pretraining',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'savasy/bert-base-turkish-sentiment-cased': {'modelId': 'savasy/bert-base-turkish-sentiment-cased',\n",
       "  'sha': '330ec37b18140dcd5c5dd6357d59463ae9deb2e0',\n",
       "  'lastModified': '2021-05-20T04:55:01.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'tr',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'savasy',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'savasy/bert-base-turkish-sentiment-cased',\n",
       "  'downloads': 4285,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'tr'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hfl/cino-large': {'modelId': 'hfl/cino-large',\n",
       "  'sha': '03c0611c2dd4b1e82eece4a6ff964510615f2eab',\n",
       "  'lastModified': '2022-01-24T09:28:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'xlm-roberta',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'bo',\n",
       "   'kk',\n",
       "   'ko',\n",
       "   'mn',\n",
       "   'ug',\n",
       "   'yue',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'hfl',\n",
       "  'config': {'architectures': ['XLMRobertaForMaskedLM'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'hfl/cino-large',\n",
       "  'downloads': 4279,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': '巴黎是<mask>国的首都。'}, {'text': '生活的真谛是<mask>。'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh', 'bo', 'kk', 'ko', 'mn', 'ug', 'yue'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'uer/chinese_roberta_L-4_H-512': {'modelId': 'uer/chinese_roberta_L-4_H-512',\n",
       "  'sha': '45bff68199891d98fdeee7b222f2d4d35912a46a',\n",
       "  'lastModified': '2022-02-19T09:11:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'Chinese',\n",
       "   'dataset:CLUECorpusSmall',\n",
       "   'arxiv:1909.05658',\n",
       "   'arxiv:1908.08962',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'uer',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'uer/chinese_roberta_L-4_H-512',\n",
       "  'downloads': 4258,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '北京是[MASK]国的首都。'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'Chinese',\n",
       "   'datasets': 'CLUECorpusSmall',\n",
       "   'widget': [{'text': '北京是[MASK]国的首都。'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allegro/herbert-klej-cased-v1': {'modelId': 'allegro/herbert-klej-cased-v1',\n",
       "  'sha': '6953ff83476f8e7a4afb4131cb629c0cffde6c9e',\n",
       "  'lastModified': '2021-05-28T16:18:22.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'pl',\n",
       "   'arxiv:2005.00630',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'allegro',\n",
       "  'config': {'model_type': 'roberta'},\n",
       "  'id': 'allegro/herbert-klej-cased-v1',\n",
       "  'downloads': 4237,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'pl'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'deutsche-telekom/bert-multi-english-german-squad2': {'modelId': 'deutsche-telekom/bert-multi-english-german-squad2',\n",
       "  'sha': '7d4c38391cca9950a1bcee19ecdbe287d8ceffb4',\n",
       "  'lastModified': '2021-07-14T13:17:23.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'question-answering',\n",
       "   'de',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'english',\n",
       "   'german',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'deutsche-telekom',\n",
       "  'config': {'architectures': ['BertForQuestionAnswering'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'deutsche-telekom/bert-multi-english-german-squad2',\n",
       "  'downloads': 4225,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Wo wohne ich?',\n",
       "    'context': 'Mein Name ist Wolfgang und ich lebe in Berlin'},\n",
       "   {'text': 'Welcher Name wird auch verwendet, um den Amazonas-Regenwald auf Englisch zu beschreiben?',\n",
       "    'context': 'Der Amazonas-Regenwald, auf Englisch auch als Amazonien oder Amazonas-Dschungel bekannt, ist ein feuchter Laubwald, der den größten Teil des Amazonas-Beckens Südamerikas bedeckt. Dieses Becken umfasst 7.000.000 Quadratkilometer (2.700.000 Quadratmeilen), von denen 5.500.000 Quadratkilometer (2.100.000 Quadratmeilen) vom Regenwald bedeckt sind. Diese Region umfasst Gebiete von neun Nationen. Der größte Teil des Waldes befindet sich in Brasilien mit 60% des Regenwaldes, gefolgt von Peru mit 13%, Kolumbien mit 10% und geringen Mengen in Venezuela, Ecuador, Bolivien, Guyana, Suriname und Französisch-Guayana. Staaten oder Abteilungen in vier Nationen enthalten \"Amazonas\" in ihren Namen. Der Amazonas repräsentiert mehr als die Hälfte der verbleibenden Regenwälder des Planeten und umfasst den größten und artenreichsten tropischen Regenwald der Welt mit geschätzten 390 Milliarden Einzelbäumen, die in 16.000 Arten unterteilt sind.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['de', 'en'],\n",
       "   'license': 'mit',\n",
       "   'tags': ['english', 'german']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hivemind/gpt-j-6B-8bit': {'modelId': 'hivemind/gpt-j-6B-8bit',\n",
       "  'sha': '636b67ff80cb47e083bdfd8074c45857f72cac65',\n",
       "  'lastModified': '2022-02-10T23:15:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gptj',\n",
       "   'text-generation',\n",
       "   'arxiv:2106.09685',\n",
       "   'arxiv:2110.02861',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'hivemind',\n",
       "  'config': {'architectures': ['GPTJForCausalLM'],\n",
       "   'model_type': 'gptj',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50,\n",
       "     'temperature': 1}}},\n",
       "  'id': 'hivemind/gpt-j-6B-8bit',\n",
       "  'downloads': 4215,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 58,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'flaubert/flaubert_base_uncased': {'modelId': 'flaubert/flaubert_base_uncased',\n",
       "  'sha': '56ea0bf6e54b59c192f99f2397e932a9915cae4c',\n",
       "  'lastModified': '2021-10-18T08:14:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'flaubert',\n",
       "   'fill-mask',\n",
       "   'fr',\n",
       "   'dataset:flaubert',\n",
       "   'transformers',\n",
       "   'bert',\n",
       "   'language-model',\n",
       "   'flue',\n",
       "   'french',\n",
       "   'flaubert-base',\n",
       "   'uncased',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'flaubert',\n",
       "  'config': {'architectures': ['FlaubertWithLMHeadModel'],\n",
       "   'model_type': 'flaubert'},\n",
       "  'id': 'flaubert/flaubert_base_uncased',\n",
       "  'downloads': 4204,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<special1>',\n",
       "  'widgetData': [{'text': 'Paris est la <special1> de la France.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fr',\n",
       "   'license': 'mit',\n",
       "   'datasets': ['flaubert'],\n",
       "   'metrics': ['flue'],\n",
       "   'tags': ['bert',\n",
       "    'language-model',\n",
       "    'flaubert',\n",
       "    'flue',\n",
       "    'french',\n",
       "    'flaubert-base',\n",
       "    'uncased']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'google/bigbird-base-trivia-itc': {'modelId': 'google/bigbird-base-trivia-itc',\n",
       "  'sha': '29c5c29e0297ad7eb9b90ef69fecba71508f5ca4',\n",
       "  'lastModified': '2021-06-02T14:53:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'big_bird',\n",
       "   'question-answering',\n",
       "   'en',\n",
       "   'dataset:trivia_qa',\n",
       "   'arxiv:2007.14062',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['BigBirdForQuestionAnswering'],\n",
       "   'model_type': 'big_bird'},\n",
       "  'id': 'google/bigbird-base-trivia-itc',\n",
       "  'downloads': 4203,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['trivia_qa']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'allenai/tk-instruct-base-def-pos': {'modelId': 'allenai/tk-instruct-base-def-pos',\n",
       "  'sha': '196e8998944bded8e53c6fe3a757a905a3d5382f',\n",
       "  'lastModified': '2022-05-27T06:30:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'dataset:natural instructions v2.0',\n",
       "   'arxiv:1910.10683',\n",
       "   'arxiv:2204.07705',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'allenai',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'allenai/tk-instruct-base-def-pos',\n",
       "  'downloads': 4202,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['natural instructions v2.0']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12': {'modelId': 'bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12',\n",
       "  'sha': 'a0ce2a0e0feb8f4caf0346b139266f5320b90322',\n",
       "  'lastModified': '2021-09-24T07:45:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'dataset:pubmed',\n",
       "   'transformers',\n",
       "   'bluebert',\n",
       "   'license:cc0-1.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'bionlp',\n",
       "  'config': {},\n",
       "  'id': 'bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12',\n",
       "  'downloads': 4179,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'],\n",
       "   'tags': ['bluebert'],\n",
       "   'license': 'cc0-1.0',\n",
       "   'datasets': ['pubmed']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'hf-internal-testing/tiny-albert': {'modelId': 'hf-internal-testing/tiny-albert',\n",
       "  'sha': 'de839e2e6aa81798e2ac85a8ac414da41862e23a',\n",
       "  'lastModified': '2021-07-16T01:27:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'albert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'architectures': ['AlbertForMaskedLM'], 'model_type': 'albert'},\n",
       "  'id': 'hf-internal-testing/tiny-albert',\n",
       "  'downloads': 4173,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'EMBEDDIA/crosloengual-bert': {'modelId': 'EMBEDDIA/crosloengual-bert',\n",
       "  'sha': '750255b6915cf42623143690d8ea79ceab8ee2e8',\n",
       "  'lastModified': '2021-05-18T18:21:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'hr',\n",
       "   'sl',\n",
       "   'en',\n",
       "   'multilingual',\n",
       "   'arxiv:2006.07890',\n",
       "   'transformers',\n",
       "   'license:cc-by-4.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'EMBEDDIA',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'EMBEDDIA/crosloengual-bert',\n",
       "  'downloads': 4169,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['hr', 'sl', 'en', 'multilingual'],\n",
       "   'license': 'cc-by-4.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'DeepChem/ChemBERTa-77M-MLM': {'modelId': 'DeepChem/ChemBERTa-77M-MLM',\n",
       "  'sha': 'ed8a5374f2024ec8da53760af91a33fb8f6a15ff',\n",
       "  'lastModified': '2022-01-20T18:02:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'DeepChem',\n",
       "  'config': {'architectures': ['RobertaForMaskedLM'], 'model_type': 'roberta'},\n",
       "  'id': 'DeepChem/ChemBERTa-77M-MLM',\n",
       "  'downloads': 4166,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/convnext-tiny-224': {'modelId': 'facebook/convnext-tiny-224',\n",
       "  'sha': '0d1c8dedaa107d4ae537c5b10e5cd0a8c865e84e',\n",
       "  'lastModified': '2022-02-26T12:15:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'convnext',\n",
       "   'image-classification',\n",
       "   'dataset:imagenet-1k',\n",
       "   'arxiv:2201.03545',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-classification',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['ConvNextForImageClassification'],\n",
       "   'model_type': 'convnext'},\n",
       "  'id': 'facebook/convnext-tiny-224',\n",
       "  'downloads': 4163,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg',\n",
       "    'example_title': 'Tiger'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg',\n",
       "    'example_title': 'Teapot'},\n",
       "   {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg',\n",
       "    'example_title': 'Palace'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['vision', 'image-classification'],\n",
       "   'datasets': ['imagenet-1k'],\n",
       "   'widget': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg',\n",
       "     'example_title': 'Tiger'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg',\n",
       "     'example_title': 'Teapot'},\n",
       "    {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg',\n",
       "     'example_title': 'Palace'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageClassification',\n",
       "   'pipeline_tag': 'image-classification',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'flair/ner-english-ontonotes': {'modelId': 'flair/ner-english-ontonotes',\n",
       "  'sha': '4e50d09d85d60fd36e2c78175d4e405b1e3caa8c',\n",
       "  'lastModified': '2021-03-02T22:07:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'en',\n",
       "   'dataset:ontonotes',\n",
       "   'flair',\n",
       "   'token-classification',\n",
       "   'sequence-tagger-model'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'flair',\n",
       "  'config': None,\n",
       "  'id': 'flair/ner-english-ontonotes',\n",
       "  'downloads': 4149,\n",
       "  'library_name': 'flair',\n",
       "  'widgetData': [{'text': 'On September 1st George Washington won 1 dollar.'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['flair',\n",
       "    'token-classification',\n",
       "    'sequence-tagger-model'],\n",
       "   'language': 'en',\n",
       "   'datasets': ['ontonotes'],\n",
       "   'widget': [{'text': 'On September 1st George Washington won 1 dollar.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'uer/roberta-base-finetuned-dianping-chinese': {'modelId': 'uer/roberta-base-finetuned-dianping-chinese',\n",
       "  'sha': '9498566e5da5b6cdc52f8eea002be9c24aae959a',\n",
       "  'lastModified': '2022-02-20T07:57:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'zh',\n",
       "   'arxiv:1909.05658',\n",
       "   'arxiv:1708.02657',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'uer',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'uer/roberta-base-finetuned-dianping-chinese',\n",
       "  'downloads': 4132,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '这本书真的很不错'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'zh', 'widget': [{'text': '这本书真的很不错'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'studio-ousia/luke-large-finetuned-tacred': {'modelId': 'studio-ousia/luke-large-finetuned-tacred',\n",
       "  'sha': 'ba3d02d7791d738d6bd480592ed814525124fbbc',\n",
       "  'lastModified': '2022-03-23T12:31:16.000Z',\n",
       "  'tags': ['pytorch', 'luke', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'studio-ousia',\n",
       "  'config': {'architectures': ['LukeForEntityPairClassification'],\n",
       "   'model_type': 'luke'},\n",
       "  'id': 'studio-ousia/luke-large-finetuned-tacred',\n",
       "  'downloads': 4112,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'LukeForEntityPairClassification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'jeniya/BERTOverflow': {'modelId': 'jeniya/BERTOverflow',\n",
       "  'sha': '0361ca9842fd3f88b2e4eea1626d56c2e1265bce',\n",
       "  'lastModified': '2021-05-19T20:47:17.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'jeniya',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'jeniya/BERTOverflow',\n",
       "  'downloads': 4107,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'valhalla/t5-small-e2e-qg': {'modelId': 'valhalla/t5-small-e2e-qg',\n",
       "  'sha': 'feec82746b18ab037724c14f11277f320bd73920',\n",
       "  'lastModified': '2021-07-30T13:10:33.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'dataset:squad',\n",
       "   'arxiv:1910.10683',\n",
       "   'transformers',\n",
       "   'question-generation',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'valhalla',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 1.5,\n",
       "     'max_length': 256,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'generate questions: '},\n",
       "    'translation_en_to_de': {'early_stopping': True,\n",
       "     'length_penalty': 1.5,\n",
       "     'max_length': 256,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'generate questions: '},\n",
       "    'translation_en_to_fr': {'early_stopping': True,\n",
       "     'length_penalty': 1.5,\n",
       "     'max_length': 256,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'generate questions: '},\n",
       "    'translation_en_to_ro': {'early_stopping': True,\n",
       "     'length_penalty': 1.5,\n",
       "     'max_length': 256,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4,\n",
       "     'prefix': 'generate questions: '}}},\n",
       "  'id': 'valhalla/t5-small-e2e-qg',\n",
       "  'downloads': 4104,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Python is developed by Guido Van Rossum and released in 1991. </s>'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'datasets': ['squad'],\n",
       "   'tags': ['question-generation'],\n",
       "   'widget': [{'text': 'Python is developed by Guido Van Rossum and released in 1991. </s>'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ckiplab/gpt2-base-chinese': {'modelId': 'ckiplab/gpt2-base-chinese',\n",
       "  'sha': '5b35975016d00703e7d812b9197ea81f295b65c3',\n",
       "  'lastModified': '2022-05-10T03:28:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'lm-head',\n",
       "   'license:gpl-3.0'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'ckiplab',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'ckiplab/gpt2-base-chinese',\n",
       "  'downloads': 4091,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '我叫朱利安，我喜欢'},\n",
       "   {'text': '我叫托马斯，我的主要'},\n",
       "   {'text': '我叫玛丽亚，我最喜欢的'},\n",
       "   {'text': '我叫克拉拉，我是'},\n",
       "   {'text': '从前，'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'thumbnail': 'https://ckip.iis.sinica.edu.tw/files/ckip_logo.png',\n",
       "   'tags': ['pytorch', 'lm-head', 'gpt2', 'zh'],\n",
       "   'license': 'gpl-3.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'shibing624/text2vec-base-chinese': {'modelId': 'shibing624/text2vec-base-chinese',\n",
       "  'sha': 'b455bb011898ad5d8b16cea238d070cd34db4b05',\n",
       "  'lastModified': '2022-03-14T06:43:16.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'transformers',\n",
       "   'text2vec',\n",
       "   'sentence-similarity',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'shibing624',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'shibing624/text2vec-base-chinese',\n",
       "  'downloads': 4076,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['text2vec',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Norod78/hebrew-bad_wiki-gpt_neo-tiny': {'modelId': 'Norod78/hebrew-bad_wiki-gpt_neo-tiny',\n",
       "  'sha': '3429083d9673b269efa98b256d3c84e040e97a8f',\n",
       "  'lastModified': '2022-07-04T07:25:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt_neo',\n",
       "   'text-generation',\n",
       "   'he',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'Norod78',\n",
       "  'config': {'architectures': ['GPTNeoForCausalLM'], 'model_type': 'gpt_neo'},\n",
       "  'id': 'Norod78/hebrew-bad_wiki-gpt_neo-tiny',\n",
       "  'downloads': 4044,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'מתמטיקה:'},\n",
       "   {'text': 'עליית המכונות'},\n",
       "   {'text': 'ויקיפדיה העברית'},\n",
       "   {'text': 'האירוויזיון הוא'},\n",
       "   {'text': 'דוד בן-גוריון היה'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'he',\n",
       "   'thumbnail': 'https://avatars1.githubusercontent.com/u/3617152?norod.jpg',\n",
       "   'widget': [{'text': 'מתמטיקה:'},\n",
       "    {'text': 'עליית המכונות'},\n",
       "    {'text': 'ויקיפדיה העברית'},\n",
       "    {'text': 'האירוויזיון הוא'},\n",
       "    {'text': 'דוד בן-גוריון היה'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Zixtrauce/BDBot4Epoch': {'modelId': 'Zixtrauce/BDBot4Epoch',\n",
       "  'sha': '77357067c689ccb8c19220a32137eb8646bf87e5',\n",
       "  'lastModified': '2022-01-01T23:46:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'Zixtrauce',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'Zixtrauce/BDBot4Epoch',\n",
       "  'downloads': 4040,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/distilroberta-base-paraphrase-v1': {'modelId': 'sentence-transformers/distilroberta-base-paraphrase-v1',\n",
       "  'sha': '0191e446424b49506ba016264788b49bb7b11eb9',\n",
       "  'lastModified': '2022-06-15T21:53:03.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/distilroberta-base-paraphrase-v1',\n",
       "  'downloads': 4029,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/opt-13b': {'modelId': 'facebook/opt-13b',\n",
       "  'sha': '45d913414643f29e9273a362ef881109c36b72a5',\n",
       "  'lastModified': '2022-06-24T05:21:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'opt',\n",
       "   'text-generation',\n",
       "   'en',\n",
       "   'arxiv:2205.01068',\n",
       "   'arxiv:2005.14165',\n",
       "   'transformers',\n",
       "   'license:other'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['OPTForCausalLM'], 'model_type': 'opt'},\n",
       "  'id': 'facebook/opt-13b',\n",
       "  'downloads': 4013,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'inference': False,\n",
       "   'tags': ['opt', 'text-generation'],\n",
       "   'license': 'other',\n",
       "   'commercial': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'textattack/distilbert-base-cased-CoLA': {'modelId': 'textattack/distilbert-base-cased-CoLA',\n",
       "  'sha': '73fd8dc841293aab1caea98581bb57481c87ff55',\n",
       "  'lastModified': '2020-06-09T16:45:43.000Z',\n",
       "  'tags': ['pytorch', 'distilbert', 'text-classification', 'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'textattack',\n",
       "  'config': {'architectures': ['DistilBertForSequenceClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'textattack/distilbert-base-cased-CoLA',\n",
       "  'downloads': 4009,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'setu4993/LaBSE': {'modelId': 'setu4993/LaBSE',\n",
       "  'sha': '082cccca1eea3e0fab80749de8e8aded21bec253',\n",
       "  'lastModified': '2021-12-05T06:10:07.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'as',\n",
       "   'az',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bn',\n",
       "   'bo',\n",
       "   'bs',\n",
       "   'ca',\n",
       "   'ceb',\n",
       "   'co',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'eo',\n",
       "   'es',\n",
       "   'et',\n",
       "   'eu',\n",
       "   'fa',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'haw',\n",
       "   'he',\n",
       "   'hi',\n",
       "   'hmn',\n",
       "   'hr',\n",
       "   'ht',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'id',\n",
       "   'ig',\n",
       "   'is',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'ku',\n",
       "   'ky',\n",
       "   'la',\n",
       "   'lb',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mi',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'mt',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'ny',\n",
       "   'or',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'pt',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'rw',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'sm',\n",
       "   'sn',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'st',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'tg',\n",
       "   'th',\n",
       "   'tk',\n",
       "   'tl',\n",
       "   'tr',\n",
       "   'tt',\n",
       "   'ug',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'wo',\n",
       "   'xh',\n",
       "   'yi',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'zu',\n",
       "   'dataset:CommonCrawl',\n",
       "   'dataset:Wikipedia',\n",
       "   'arxiv:2007.01852',\n",
       "   'transformers',\n",
       "   'sentence_embedding',\n",
       "   'multilingual',\n",
       "   'google',\n",
       "   'sentence-similarity',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'setu4993',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'setu4993/LaBSE',\n",
       "  'downloads': 4000,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 17,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'as',\n",
       "    'az',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bn',\n",
       "    'bo',\n",
       "    'bs',\n",
       "    'ca',\n",
       "    'ceb',\n",
       "    'co',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'eo',\n",
       "    'es',\n",
       "    'et',\n",
       "    'eu',\n",
       "    'fa',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'haw',\n",
       "    'he',\n",
       "    'hi',\n",
       "    'hmn',\n",
       "    'hr',\n",
       "    'ht',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'id',\n",
       "    'ig',\n",
       "    'is',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'ku',\n",
       "    'ky',\n",
       "    'la',\n",
       "    'lb',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mi',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'mt',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'ny',\n",
       "    'or',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'pt',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'rw',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'sm',\n",
       "    'sn',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'st',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'tg',\n",
       "    'th',\n",
       "    'tk',\n",
       "    'tl',\n",
       "    'tr',\n",
       "    'tt',\n",
       "    'ug',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'wo',\n",
       "    'xh',\n",
       "    'yi',\n",
       "    'yo',\n",
       "    'zh',\n",
       "    'zu'],\n",
       "   'tags': ['bert',\n",
       "    'sentence_embedding',\n",
       "    'multilingual',\n",
       "    'google',\n",
       "    'sentence-similarity'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['CommonCrawl', 'Wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'philschmid/BERT-Banking77': {'modelId': 'philschmid/BERT-Banking77',\n",
       "  'sha': 'e08d5e191921b9e0713327dc7e29293ecb286043',\n",
       "  'lastModified': '2022-06-24T14:31:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:banking77',\n",
       "   'transformers',\n",
       "   'autotrain',\n",
       "   'model-index',\n",
       "   'co2_eq_emissions'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'philschmid',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'philschmid/BERT-Banking77',\n",
       "  'downloads': 3994,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I am still waiting on my card?'}],\n",
       "  'model-index': [{'name': 'BERT-Banking77',\n",
       "    'results': [{'task': {'name': 'Text Classification',\n",
       "       'type': 'text-classification'},\n",
       "      'dataset': {'name': 'BANKING77', 'type': 'banking77'},\n",
       "      'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 92.64},\n",
       "       {'name': 'Macro F1', 'type': 'macro-f1', 'value': 92.64},\n",
       "       {'name': 'Weighted F1', 'type': 'weighted-f1', 'value': 92.6}]},\n",
       "     {'task': {'type': 'text-classification', 'name': 'Text Classification'},\n",
       "      'dataset': {'name': 'banking77',\n",
       "       'type': 'banking77',\n",
       "       'config': 'default',\n",
       "       'split': 'test'},\n",
       "      'metrics': [{'name': 'Accuracy',\n",
       "        'type': 'accuracy',\n",
       "        'value': 0.9275974025974026,\n",
       "        'verified': True},\n",
       "       {'name': 'Precision Macro',\n",
       "        'type': 'precision',\n",
       "        'value': 0.9305185253845069,\n",
       "        'verified': True},\n",
       "       {'name': 'Precision Micro',\n",
       "        'type': 'precision',\n",
       "        'value': 0.9275974025974026,\n",
       "        'verified': True},\n",
       "       {'name': 'Precision Weighted',\n",
       "        'type': 'precision',\n",
       "        'value': 0.9305185253845071,\n",
       "        'verified': True},\n",
       "       {'name': 'Recall Macro',\n",
       "        'type': 'recall',\n",
       "        'value': 0.9275974025974028,\n",
       "        'verified': True},\n",
       "       {'name': 'Recall Micro',\n",
       "        'type': 'recall',\n",
       "        'value': 0.9275974025974026,\n",
       "        'verified': True},\n",
       "       {'name': 'Recall Weighted',\n",
       "        'type': 'recall',\n",
       "        'value': 0.9275974025974026,\n",
       "        'verified': True},\n",
       "       {'name': 'F1 Macro',\n",
       "        'type': 'f1',\n",
       "        'value': 0.927623314966026,\n",
       "        'verified': True},\n",
       "       {'name': 'F1 Micro',\n",
       "        'type': 'f1',\n",
       "        'value': 0.9275974025974026,\n",
       "        'verified': True},\n",
       "       {'name': 'F1 Weighted',\n",
       "        'type': 'f1',\n",
       "        'value': 0.927623314966026,\n",
       "        'verified': True},\n",
       "       {'name': 'loss',\n",
       "        'type': 'loss',\n",
       "        'value': 0.3199225962162018,\n",
       "        'verified': True}]}]}],\n",
       "  'cardData': {'tags': 'autotrain',\n",
       "   'language': 'en',\n",
       "   'widget': [{'text': 'I am still waiting on my card?'}],\n",
       "   'datasets': ['banking77'],\n",
       "   'model-index': [{'name': 'BERT-Banking77',\n",
       "     'results': [{'task': {'name': 'Text Classification',\n",
       "        'type': 'text-classification'},\n",
       "       'dataset': {'name': 'BANKING77', 'type': 'banking77'},\n",
       "       'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 92.64},\n",
       "        {'name': 'Macro F1', 'type': 'macro-f1', 'value': 92.64},\n",
       "        {'name': 'Weighted F1', 'type': 'weighted-f1', 'value': 92.6}]},\n",
       "      {'task': {'type': 'text-classification', 'name': 'Text Classification'},\n",
       "       'dataset': {'name': 'banking77',\n",
       "        'type': 'banking77',\n",
       "        'config': 'default',\n",
       "        'split': 'test'},\n",
       "       'metrics': [{'name': 'Accuracy',\n",
       "         'type': 'accuracy',\n",
       "         'value': 0.9275974025974026,\n",
       "         'verified': True},\n",
       "        {'name': 'Precision Macro',\n",
       "         'type': 'precision',\n",
       "         'value': 0.9305185253845069,\n",
       "         'verified': True},\n",
       "        {'name': 'Precision Micro',\n",
       "         'type': 'precision',\n",
       "         'value': 0.9275974025974026,\n",
       "         'verified': True},\n",
       "        {'name': 'Precision Weighted',\n",
       "         'type': 'precision',\n",
       "         'value': 0.9305185253845071,\n",
       "         'verified': True},\n",
       "        {'name': 'Recall Macro',\n",
       "         'type': 'recall',\n",
       "         'value': 0.9275974025974028,\n",
       "         'verified': True},\n",
       "        {'name': 'Recall Micro',\n",
       "         'type': 'recall',\n",
       "         'value': 0.9275974025974026,\n",
       "         'verified': True},\n",
       "        {'name': 'Recall Weighted',\n",
       "         'type': 'recall',\n",
       "         'value': 0.9275974025974026,\n",
       "         'verified': True},\n",
       "        {'name': 'F1 Macro',\n",
       "         'type': 'f1',\n",
       "         'value': 0.927623314966026,\n",
       "         'verified': True},\n",
       "        {'name': 'F1 Micro',\n",
       "         'type': 'f1',\n",
       "         'value': 0.9275974025974026,\n",
       "         'verified': True},\n",
       "        {'name': 'F1 Weighted',\n",
       "         'type': 'f1',\n",
       "         'value': 0.927623314966026,\n",
       "         'verified': True},\n",
       "        {'name': 'loss',\n",
       "         'type': 'loss',\n",
       "         'value': 0.3199225962162018,\n",
       "         'verified': True}]}]}],\n",
       "   'co2_eq_emissions': 0.03330651014155927},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'DeepESP/gpt2-spanish': {'modelId': 'DeepESP/gpt2-spanish',\n",
       "  'sha': '1b935e39cf9893108bd2f4fb5317f48ae1c3ab5e',\n",
       "  'lastModified': '2021-10-19T08:52:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'es',\n",
       "   'dataset:ebooks',\n",
       "   'transformers',\n",
       "   'GPT-2',\n",
       "   'Spanish',\n",
       "   'ebooks',\n",
       "   'nlg',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'DeepESP',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 50}}},\n",
       "  'id': 'DeepESP/gpt2-spanish',\n",
       "  'downloads': 3987,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Quisiera saber que va a suceder'}],\n",
       "  'likes': 7,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'es',\n",
       "   'tags': ['GPT-2', 'Spanish', 'ebooks', 'nlg'],\n",
       "   'datasets': ['ebooks'],\n",
       "   'widget': [{'text': 'Quisiera saber que va a suceder'}],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nreimers/TinyBERT_L-4_H-312_v2': {'modelId': 'nreimers/TinyBERT_L-4_H-312_v2',\n",
       "  'sha': 'd782507ee95c6565fe5924fcd6090999055e8db6',\n",
       "  'lastModified': '2021-05-28T11:02:32.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'nreimers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'nreimers/TinyBERT_L-4_H-312_v2',\n",
       "  'downloads': 3984,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'hf-internal-testing/tiny-random-unispeech-sat': {'modelId': 'hf-internal-testing/tiny-random-unispeech-sat',\n",
       "  'sha': '4a779774d9473a62b9436b87cf9ac885b97e6f16',\n",
       "  'lastModified': '2022-01-26T13:47:21.000Z',\n",
       "  'tags': ['pytorch', 'unispeech-sat', 'audio-classification', 'transformers'],\n",
       "  'pipeline_tag': 'audio-classification',\n",
       "  'private': False,\n",
       "  'author': 'hf-internal-testing',\n",
       "  'config': {'architectures': ['UniSpeechSatForSequenceClassification'],\n",
       "   'model_type': 'unispeech-sat'},\n",
       "  'id': 'hf-internal-testing/tiny-random-unispeech-sat',\n",
       "  'downloads': 3952,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForAudioClassification',\n",
       "   'pipeline_tag': 'audio-classification',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'enelpol/poleval2021-task3': {'modelId': 'enelpol/poleval2021-task3',\n",
       "  'sha': 'b6e13ae11eca4e958f21dc6ce4b4f8f161c2da30',\n",
       "  'lastModified': '2022-04-25T12:29:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'enelpol',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'enelpol/poleval2021-task3',\n",
       "  'downloads': 3950,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cardiffnlp/twitter-roberta-base-offensive': {'modelId': 'cardiffnlp/twitter-roberta-base-offensive',\n",
       "  'sha': 'afebc0c0c4c58177a8e6ab683c25beffeb351135',\n",
       "  'lastModified': '2021-05-20T15:05:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'arxiv:2010.12421',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cardiffnlp',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cardiffnlp/twitter-roberta-base-offensive',\n",
       "  'downloads': 3937,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KM4STfulltext/SSCI-SciBERT-e2': {'modelId': 'KM4STfulltext/SSCI-SciBERT-e2',\n",
       "  'sha': '94c8bf53e16f6a586c5fa7d105b628898bb2aeab',\n",
       "  'lastModified': '2022-06-01T09:25:14.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'KM4STfulltext',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'KM4STfulltext/SSCI-SciBERT-e2',\n",
       "  'downloads': 3933,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Paris is the [MASK] of France.'},\n",
       "   {'text': 'The goal of life is [MASK].'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Langboat/mengzi-bert-base': {'modelId': 'Langboat/mengzi-bert-base',\n",
       "  'sha': 'a685cb1101fb1ea116e8432b2e14042194e4738b',\n",
       "  'lastModified': '2021-10-14T09:01:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'zh',\n",
       "   'arxiv:2110.06696',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'Langboat',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'Langboat/mengzi-bert-base',\n",
       "  'downloads': 3917,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '生活的真谛是[MASK]。'}],\n",
       "  'likes': 15,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'license': 'apache-2.0',\n",
       "   'widget': [{'text': '生活的真谛是[MASK]。'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'uer/gpt2-chinese-lyric': {'modelId': 'uer/gpt2-chinese-lyric',\n",
       "  'sha': '5cd62aaad5739037a840c90f4dd3244bd8cdce10',\n",
       "  'lastModified': '2022-02-20T05:02:43.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'Chinese',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'uer',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'text-generation': {'do_sample': True,\n",
       "     'max_length': 128}}},\n",
       "  'id': 'uer/gpt2-chinese-lyric',\n",
       "  'downloads': 3915,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '最美的不是下雨天，是曾与你躲过雨的屋檐'}],\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'Chinese',\n",
       "   'widget': [{'text': '最美的不是下雨天，是曾与你躲过雨的屋檐'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'funnel-transformer/large': {'modelId': 'funnel-transformer/large',\n",
       "  'sha': '3f56dd1209e9470b922bd24715c78db03af0fe51',\n",
       "  'lastModified': '2022-06-07T15:26:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'funnel',\n",
       "   'feature-extraction',\n",
       "   'en',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'dataset:gigaword',\n",
       "   'arxiv:2006.03236',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'funnel-transformer',\n",
       "  'config': {'architectures': ['FunnelModel'], 'model_type': 'funnel'},\n",
       "  'id': 'funnel-transformer/large',\n",
       "  'downloads': 3910,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia', 'gigaword']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/ms-marco-MiniLM-L-4-v2': {'modelId': 'cross-encoder/ms-marco-MiniLM-L-4-v2',\n",
       "  'sha': '1f1ab0943a42a52afd702e7e8337bec985c189ea',\n",
       "  'lastModified': '2021-08-05T08:39:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'cross-encoder/ms-marco-MiniLM-L-4-v2',\n",
       "  'downloads': 3882,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'KETI-AIR/ke-t5-base': {'modelId': 'KETI-AIR/ke-t5-base',\n",
       "  'sha': '3fdf631a2986e928ad8b22863f9da16da74d9906',\n",
       "  'lastModified': '2021-06-23T02:50:50.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   't5',\n",
       "   'text2text-generation',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'KETI-AIR',\n",
       "  'config': {'architectures': ['T5ForConditionalGeneration'],\n",
       "   'model_type': 't5'},\n",
       "  'id': 'KETI-AIR/ke-t5-base',\n",
       "  'downloads': 3880,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nreimers/MiniLM-L6-H384-uncased': {'modelId': 'nreimers/MiniLM-L6-H384-uncased',\n",
       "  'sha': '3276f0fac9d818781d7a1327b3ff818fc4e643c0',\n",
       "  'lastModified': '2021-08-30T20:05:29.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'feature-extraction',\n",
       "   'transformers',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'nreimers',\n",
       "  'config': {'architectures': ['BertModel'], 'model_type': 'bert'},\n",
       "  'id': 'nreimers/MiniLM-L6-H384-uncased',\n",
       "  'downloads': 3879,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 8,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sampathkethineedi/industry-classification': {'modelId': 'sampathkethineedi/industry-classification',\n",
       "  'sha': '9914232048445adee24e4d4683c4d1873a9bafb4',\n",
       "  'lastModified': '2020-07-16T15:27:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'transformers',\n",
       "   'tensorflow',\n",
       "   'industry',\n",
       "   'buisiness',\n",
       "   'description',\n",
       "   'multi-class',\n",
       "   'classification'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'sampathkethineedi',\n",
       "  'config': {'architectures': ['DistilBertForSequenceClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'sampathkethineedi/industry-classification',\n",
       "  'downloads': 3879,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://huggingface.co/sampathkethineedi',\n",
       "   'tags': ['distilbert',\n",
       "    'pytorch',\n",
       "    'tensorflow',\n",
       "    'text-classification',\n",
       "    'industry',\n",
       "    'buisiness',\n",
       "    'description',\n",
       "    'multi-class',\n",
       "    'classification'],\n",
       "   'liscence': 'mit',\n",
       "   'inference': False},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'jplu/tf-camembert-base': {'modelId': 'jplu/tf-camembert-base',\n",
       "  'sha': '2a40769e6a43da48783193e96e28cb2ddd5fbdb0',\n",
       "  'lastModified': '2020-12-11T21:47:52.000Z',\n",
       "  'tags': ['tf',\n",
       "   'camembert',\n",
       "   'fill-mask',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'jplu',\n",
       "  'config': {'architectures': ['CamembertForMaskedLM'],\n",
       "   'model_type': 'camembert'},\n",
       "  'id': 'jplu/tf-camembert-base',\n",
       "  'downloads': 3868,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'indigo-ai/BERTino': {'modelId': 'indigo-ai/BERTino',\n",
       "  'sha': '4539d128176da9bbadf98ab813c9acaf68e8d8f1',\n",
       "  'lastModified': '2021-09-22T08:51:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'distilbert',\n",
       "   'fill-mask',\n",
       "   'it',\n",
       "   'transformers',\n",
       "   'DISTILbert',\n",
       "   'Italian',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'indigo-ai',\n",
       "  'config': {'architectures': ['DistilBertForMaskedLM'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'indigo-ai/BERTino',\n",
       "  'downloads': 3854,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Vado al [MASK] a fare la spesa'},\n",
       "   {'text': 'Vado al parco a guardare le [MASK]'},\n",
       "   {'text': 'Il cielo è [MASK] di stelle.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'it',\n",
       "   'tags': ['DISTILbert', 'Italian'],\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': 'Vado al [MASK] a fare la spesa'},\n",
       "    {'text': 'Vado al parco a guardare le [MASK]'},\n",
       "    {'text': 'Il cielo è [MASK] di stelle.'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'sentence-transformers/nli-distilroberta-base-v2': {'modelId': 'sentence-transformers/nli-distilroberta-base-v2',\n",
       "  'sha': 'ee9754ad61d9164d693c8e4c458238433037023f',\n",
       "  'lastModified': '2022-06-15T21:56:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/nli-distilroberta-base-v2',\n",
       "  'downloads': 3854,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'optimum/distilbert-base-uncased-mnli': {'modelId': 'optimum/distilbert-base-uncased-mnli',\n",
       "  'sha': 'f2a41159d38dd6a54b91839de259af674c938e69',\n",
       "  'lastModified': '2022-03-24T16:19:12.000Z',\n",
       "  'tags': ['onnx',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:multi_nli',\n",
       "   'transformers',\n",
       "   'distilbert',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'optimum',\n",
       "  'config': {'architectures': ['DistilBertForSequenceClassification']},\n",
       "  'id': 'optimum/distilbert-base-uncased-mnli',\n",
       "  'downloads': 3847,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system’s largest world. Konstantin Batygin did not set out to solve one of the solar system’s most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system’s missing “Planet Nine,” spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn’t rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: “Oh! This is how Europa formed.” Europa is one of Jupiter’s four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the Côte d’Azur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system’s formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'tags': ['distilbert'],\n",
       "   'datasets': ['multi_nli'],\n",
       "   'metrics': ['accuracy']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification'}},\n",
       " 'facebook/mgenre-wiki': {'modelId': 'facebook/mgenre-wiki',\n",
       "  'sha': 'dbb6f7bc18c4f477073231b125254182f1290155',\n",
       "  'lastModified': '2022-06-14T14:23:17.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'mbart',\n",
       "   'text2text-generation',\n",
       "   'multilingual',\n",
       "   'af',\n",
       "   'am',\n",
       "   'ar',\n",
       "   'as',\n",
       "   'az',\n",
       "   'be',\n",
       "   'bg',\n",
       "   'bm',\n",
       "   'bn',\n",
       "   'br',\n",
       "   'bs',\n",
       "   'ca',\n",
       "   'cs',\n",
       "   'cy',\n",
       "   'da',\n",
       "   'de',\n",
       "   'el',\n",
       "   'en',\n",
       "   'eo',\n",
       "   'es',\n",
       "   'et',\n",
       "   'eu',\n",
       "   'fa',\n",
       "   'ff',\n",
       "   'fi',\n",
       "   'fr',\n",
       "   'fy',\n",
       "   'ga',\n",
       "   'gd',\n",
       "   'gl',\n",
       "   'gn',\n",
       "   'gu',\n",
       "   'ha',\n",
       "   'he',\n",
       "   'hi',\n",
       "   'hr',\n",
       "   'ht',\n",
       "   'hu',\n",
       "   'hy',\n",
       "   'id',\n",
       "   'ig',\n",
       "   'is',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'jv',\n",
       "   'ka',\n",
       "   'kg',\n",
       "   'kk',\n",
       "   'km',\n",
       "   'kn',\n",
       "   'ko',\n",
       "   'ku',\n",
       "   'ky',\n",
       "   'la',\n",
       "   'lg',\n",
       "   'ln',\n",
       "   'lo',\n",
       "   'lt',\n",
       "   'lv',\n",
       "   'mg',\n",
       "   'mk',\n",
       "   'ml',\n",
       "   'mn',\n",
       "   'mr',\n",
       "   'ms',\n",
       "   'my',\n",
       "   'ne',\n",
       "   'nl',\n",
       "   'no',\n",
       "   'om',\n",
       "   'or',\n",
       "   'pa',\n",
       "   'pl',\n",
       "   'ps',\n",
       "   'pt',\n",
       "   'qu',\n",
       "   'ro',\n",
       "   'ru',\n",
       "   'sa',\n",
       "   'sd',\n",
       "   'si',\n",
       "   'sk',\n",
       "   'sl',\n",
       "   'so',\n",
       "   'sq',\n",
       "   'sr',\n",
       "   'ss',\n",
       "   'su',\n",
       "   'sv',\n",
       "   'sw',\n",
       "   'ta',\n",
       "   'te',\n",
       "   'th',\n",
       "   'ti',\n",
       "   'tl',\n",
       "   'tn',\n",
       "   'tr',\n",
       "   'uk',\n",
       "   'ur',\n",
       "   'uz',\n",
       "   'vi',\n",
       "   'wo',\n",
       "   'xh',\n",
       "   'yo',\n",
       "   'zh',\n",
       "   'arxiv:2103.12528',\n",
       "   'arxiv:2001.08210',\n",
       "   'transformers',\n",
       "   'retrieval',\n",
       "   'entity-retrieval',\n",
       "   'named-entity-disambiguation',\n",
       "   'entity-disambiguation',\n",
       "   'named-entity-linking',\n",
       "   'entity-linking',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['MBartForConditionalGeneration'],\n",
       "   'model_type': 'mbart'},\n",
       "  'id': 'facebook/mgenre-wiki',\n",
       "  'downloads': 3830,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['multilingual',\n",
       "    'af',\n",
       "    'am',\n",
       "    'ar',\n",
       "    'as',\n",
       "    'az',\n",
       "    'be',\n",
       "    'bg',\n",
       "    'bm',\n",
       "    'bn',\n",
       "    'br',\n",
       "    'bs',\n",
       "    'ca',\n",
       "    'cs',\n",
       "    'cy',\n",
       "    'da',\n",
       "    'de',\n",
       "    'el',\n",
       "    'en',\n",
       "    'eo',\n",
       "    'es',\n",
       "    'et',\n",
       "    'eu',\n",
       "    'fa',\n",
       "    'ff',\n",
       "    'fi',\n",
       "    'fr',\n",
       "    'fy',\n",
       "    'ga',\n",
       "    'gd',\n",
       "    'gl',\n",
       "    'gn',\n",
       "    'gu',\n",
       "    'ha',\n",
       "    'he',\n",
       "    'hi',\n",
       "    'hr',\n",
       "    'ht',\n",
       "    'hu',\n",
       "    'hy',\n",
       "    'id',\n",
       "    'ig',\n",
       "    'is',\n",
       "    'it',\n",
       "    'ja',\n",
       "    'jv',\n",
       "    'ka',\n",
       "    'kg',\n",
       "    'kk',\n",
       "    'km',\n",
       "    'kn',\n",
       "    'ko',\n",
       "    'ku',\n",
       "    'ky',\n",
       "    'la',\n",
       "    'lg',\n",
       "    'ln',\n",
       "    'lo',\n",
       "    'lt',\n",
       "    'lv',\n",
       "    'mg',\n",
       "    'mk',\n",
       "    'ml',\n",
       "    'mn',\n",
       "    'mr',\n",
       "    'ms',\n",
       "    'my',\n",
       "    'ne',\n",
       "    'nl',\n",
       "    'no',\n",
       "    'om',\n",
       "    'or',\n",
       "    'pa',\n",
       "    'pl',\n",
       "    'ps',\n",
       "    'pt',\n",
       "    'qu',\n",
       "    'ro',\n",
       "    'ru',\n",
       "    'sa',\n",
       "    'sd',\n",
       "    'si',\n",
       "    'sk',\n",
       "    'sl',\n",
       "    'so',\n",
       "    'sq',\n",
       "    'sr',\n",
       "    'ss',\n",
       "    'su',\n",
       "    'sv',\n",
       "    'sw',\n",
       "    'ta',\n",
       "    'te',\n",
       "    'th',\n",
       "    'ti',\n",
       "    'tl',\n",
       "    'tn',\n",
       "    'tr',\n",
       "    'uk',\n",
       "    'ur',\n",
       "    'uz',\n",
       "    'vi',\n",
       "    'wo',\n",
       "    'xh',\n",
       "    'yo',\n",
       "    'zh'],\n",
       "   'tags': ['retrieval',\n",
       "    'entity-retrieval',\n",
       "    'named-entity-disambiguation',\n",
       "    'entity-disambiguation',\n",
       "    'named-entity-linking',\n",
       "    'entity-linking',\n",
       "    'text2text-generation']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Prime2911/DialoGPT-medium-handsomejack': {'modelId': 'Prime2911/DialoGPT-medium-handsomejack',\n",
       "  'sha': 'c9d3196d32519f073bec0ce80aeb01abe78d8075',\n",
       "  'lastModified': '2022-03-11T07:54:11.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'Prime2911',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'Prime2911/DialoGPT-medium-handsomejack',\n",
       "  'downloads': 3826,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'bakrianoo/sinai-voice-ar-stt': {'modelId': 'bakrianoo/sinai-voice-ar-stt',\n",
       "  'sha': '2d226249edf809b01a0e11159d1201ae1704b63c',\n",
       "  'lastModified': '2022-03-23T18:25:21.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'wav2vec2',\n",
       "   'automatic-speech-recognition',\n",
       "   'ar',\n",
       "   'dataset:mozilla-foundation/common_voice_8_0',\n",
       "   'transformers',\n",
       "   'hf-asr-leaderboard',\n",
       "   'robust-speech-event',\n",
       "   'license:apache-2.0',\n",
       "   'model-index'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'bakrianoo',\n",
       "  'config': {'architectures': ['Wav2Vec2ForCTC'], 'model_type': 'wav2vec2'},\n",
       "  'id': 'bakrianoo/sinai-voice-ar-stt',\n",
       "  'downloads': 3822,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'example_title': 'Example 1',\n",
       "    'src': 'https://huggingface.co/bakrianoo/sinai-voice-ar-stt/raw/main/examples/common_voice_ar_19077324.mp3'},\n",
       "   {'example_title': 'Example 2',\n",
       "    'src': 'https://huggingface.co/bakrianoo/sinai-voice-ar-stt/raw/main/examples/common_voice_ar_19205138.mp3'},\n",
       "   {'example_title': 'Example 3',\n",
       "    'src': 'https://huggingface.co/bakrianoo/sinai-voice-ar-stt/raw/main/examples/common_voice_ar_19331711.mp3'}],\n",
       "  'likes': 6,\n",
       "  'model-index': [{'name': 'Sinai Voice Arabic Speech Recognition Model',\n",
       "    'results': [{'task': {'type': 'automatic-speech-recognition',\n",
       "       'name': 'Speech Recognition'},\n",
       "      'dataset': {'type': 'mozilla-foundation/common_voice_8_0',\n",
       "       'name': 'Common Voice ar',\n",
       "       'args': 'ar'},\n",
       "      'metrics': [{'type': 'wer', 'value': 0.181, 'name': 'Test WER'},\n",
       "       {'type': 'cer', 'value': 0.049, 'name': 'Test CER'}]},\n",
       "     {'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Robust Speech Event - Dev Data',\n",
       "       'type': 'speech-recognition-community-v2/dev_data',\n",
       "       'args': 'ar'},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 93.03}]},\n",
       "     {'task': {'name': 'Automatic Speech Recognition',\n",
       "       'type': 'automatic-speech-recognition'},\n",
       "      'dataset': {'name': 'Robust Speech Event - Test Data',\n",
       "       'type': 'speech-recognition-community-v2/eval_data',\n",
       "       'args': 'ar'},\n",
       "      'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 90.79}]}]}],\n",
       "  'cardData': {'language': ['ar'],\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['automatic-speech-recognition',\n",
       "    'hf-asr-leaderboard',\n",
       "    'robust-speech-event'],\n",
       "   'datasets': ['mozilla-foundation/common_voice_8_0'],\n",
       "   'metrics': ['wer', 'cer'],\n",
       "   'model-index': [{'name': 'Sinai Voice Arabic Speech Recognition Model',\n",
       "     'results': [{'task': {'type': 'automatic-speech-recognition',\n",
       "        'name': 'Speech Recognition'},\n",
       "       'dataset': {'type': 'mozilla-foundation/common_voice_8_0',\n",
       "        'name': 'Common Voice ar',\n",
       "        'args': 'ar'},\n",
       "       'metrics': [{'type': 'wer', 'value': 0.181, 'name': 'Test WER'},\n",
       "        {'type': 'cer', 'value': 0.049, 'name': 'Test CER'}]},\n",
       "      {'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Robust Speech Event - Dev Data',\n",
       "        'type': 'speech-recognition-community-v2/dev_data',\n",
       "        'args': 'ar'},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 93.03}]},\n",
       "      {'task': {'name': 'Automatic Speech Recognition',\n",
       "        'type': 'automatic-speech-recognition'},\n",
       "       'dataset': {'name': 'Robust Speech Event - Test Data',\n",
       "        'type': 'speech-recognition-community-v2/eval_data',\n",
       "        'args': 'ar'},\n",
       "       'metrics': [{'name': 'Test WER', 'type': 'wer', 'value': 90.79}]}]}],\n",
       "   'widget': [{'example_title': 'Example 1',\n",
       "     'src': 'https://huggingface.co/bakrianoo/sinai-voice-ar-stt/raw/main/examples/common_voice_ar_19077324.mp3'},\n",
       "    {'example_title': 'Example 2',\n",
       "     'src': 'https://huggingface.co/bakrianoo/sinai-voice-ar-stt/raw/main/examples/common_voice_ar_19205138.mp3'},\n",
       "    {'example_title': 'Example 3',\n",
       "     'src': 'https://huggingface.co/bakrianoo/sinai-voice-ar-stt/raw/main/examples/common_voice_ar_19331711.mp3'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCTC',\n",
       "   'pipeline_tag': 'automatic-speech-recognition',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'google/canine-c': {'modelId': 'google/canine-c',\n",
       "  'sha': '1e8c8b3a4e860cb2a23a14c3fbba61ef3aed51f6',\n",
       "  'lastModified': '2021-08-13T08:24:13.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'canine',\n",
       "   'feature-extraction',\n",
       "   'multilingual',\n",
       "   'dataset:bookcorpus',\n",
       "   'dataset:wikipedia',\n",
       "   'arxiv:2103.06874',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'google',\n",
       "  'config': {'architectures': ['CanineModel'], 'model_type': 'canine'},\n",
       "  'id': 'google/canine-c',\n",
       "  'downloads': 3806,\n",
       "  'library_name': 'transformers',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['bookcorpus', 'wikipedia']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'AK270802/DialoGPT-small-harrypotter': {'modelId': 'AK270802/DialoGPT-small-harrypotter',\n",
       "  'sha': '5e5434fd66c852ebf69cc07279d85f55a645768e',\n",
       "  'lastModified': '2022-01-16T11:19:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'AK270802',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'AK270802/DialoGPT-small-harrypotter',\n",
       "  'downloads': 3761,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'gogamza/kobart-summarization': {'modelId': 'gogamza/kobart-summarization',\n",
       "  'sha': '8a63d6913edc0e16a902e3fa8b688a134f0dd776',\n",
       "  'lastModified': '2021-11-22T10:59:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'ko',\n",
       "   'transformers',\n",
       "   'license:mit',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'gogamza',\n",
       "  'config': {'architectures': ['BartForConditionalGeneration'],\n",
       "   'model_type': 'bart'},\n",
       "  'id': 'gogamza/kobart-summarization',\n",
       "  'downloads': 3760,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'ko', 'tags': ['bart'], 'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'banden/DialoGPT-medium-RickBot': {'modelId': 'banden/DialoGPT-medium-RickBot',\n",
       "  'sha': '010c54cb71eeb60b5a15b270842d132bde6254aa',\n",
       "  'lastModified': '2021-09-21T14:58:30.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'gpt2',\n",
       "   'text-generation',\n",
       "   'transformers',\n",
       "   'conversational'],\n",
       "  'pipeline_tag': 'conversational',\n",
       "  'private': False,\n",
       "  'author': 'banden',\n",
       "  'config': {'architectures': ['GPT2LMHeadModel'],\n",
       "   'model_type': 'gpt2',\n",
       "   'task_specific_params': {'conversational': {'max_length': 1000}}},\n",
       "  'id': 'banden/DialoGPT-medium-RickBot',\n",
       "  'downloads': 3751,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hey my name is Thomas! How are you?'},\n",
       "   {'text': 'Hey my name is Mariama! How are you?'},\n",
       "   {'text': 'Hey my name is Clara! How are you?'},\n",
       "   {'text': 'Hey my name is Julien! How are you?'},\n",
       "   {'text': 'Hi.'}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['conversational']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'inokufu/flaubert-base-uncased-xnli-sts': {'modelId': 'inokufu/flaubert-base-uncased-xnli-sts',\n",
       "  'sha': 'e4fcf0bc37d8d55fbbe1cc8096eaeaa4057d2560',\n",
       "  'lastModified': '2022-02-18T16:50:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'flaubert',\n",
       "   'feature-extraction',\n",
       "   'fr',\n",
       "   'dataset:xnli',\n",
       "   'dataset:stsb_multi_mt',\n",
       "   'arxiv:1809.05053',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'xnli',\n",
       "   'stsb_multi_mt'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'inokufu',\n",
       "  'config': {'architectures': ['FlaubertModel'], 'model_type': 'flaubert'},\n",
       "  'id': 'inokufu/flaubert-base-uncased-xnli-sts',\n",
       "  'downloads': 3746,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<special1>',\n",
       "  'widgetData': [{'source_sentence': \"C'est une personne heureuse\",\n",
       "    'sentences': [\"C'est un chien heureux\",\n",
       "     \"C'est une personne très heureuse\",\n",
       "     \"Aujourd'hui est une journée ensoleillée\"]}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'language': 'fr',\n",
       "   'tags': ['sentence-similarity',\n",
       "    'transformers',\n",
       "    'fr',\n",
       "    'flaubert',\n",
       "    'sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'xnli',\n",
       "    'stsb_multi_mt'],\n",
       "   'datasets': ['xnli', 'stsb_multi_mt']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nvidia/mit-b0': {'modelId': 'nvidia/mit-b0',\n",
       "  'sha': 'f13c39228a46f6f160f23f27cbef82160b8c8773',\n",
       "  'lastModified': '2022-03-02T14:59:41.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'segformer',\n",
       "   'image-classification',\n",
       "   'dataset:imagenet_1k',\n",
       "   'arxiv:2105.15203',\n",
       "   'transformers',\n",
       "   'vision',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'image-classification',\n",
       "  'private': False,\n",
       "  'author': 'nvidia',\n",
       "  'config': {'architectures': ['SegformerForImageClassification'],\n",
       "   'model_type': 'segformer'},\n",
       "  'id': 'nvidia/mit-b0',\n",
       "  'downloads': 3745,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'src': 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg',\n",
       "    'example_title': 'House'},\n",
       "   {'src': 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000002.jpg',\n",
       "    'example_title': 'Castle'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0',\n",
       "   'tags': ['vision'],\n",
       "   'datasets': ['imagenet_1k'],\n",
       "   'widget': [{'src': 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000001.jpg',\n",
       "     'example_title': 'House'},\n",
       "    {'src': 'https://huggingface.co/datasets/hf-internal-testing/fixtures_ade20k/resolve/main/ADE_val_00000002.jpg',\n",
       "     'example_title': 'Castle'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForImageClassification',\n",
       "   'pipeline_tag': 'image-classification',\n",
       "   'processor': 'AutoFeatureExtractor'}},\n",
       " 'sentence-transformers/roberta-base-nli-mean-tokens': {'modelId': 'sentence-transformers/roberta-base-nli-mean-tokens',\n",
       "  'sha': '993765530351e8b2a4da74bed694d80de826cbb3',\n",
       "  'lastModified': '2022-06-15T21:54:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'roberta',\n",
       "   'feature-extraction',\n",
       "   'arxiv:1908.10084',\n",
       "   'sentence-transformers',\n",
       "   'sentence-similarity',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'private': False,\n",
       "  'author': 'sentence-transformers',\n",
       "  'config': {'architectures': ['RobertaModel'], 'model_type': 'roberta'},\n",
       "  'id': 'sentence-transformers/roberta-base-nli-mean-tokens',\n",
       "  'downloads': 3745,\n",
       "  'library_name': 'sentence-transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'source_sentence': 'That is a happy person',\n",
       "    'sentences': ['That is a happy dog',\n",
       "     'That is a very happy person',\n",
       "     'Today is a sunny day']}],\n",
       "  'model-index': None,\n",
       "  'cardData': {'pipeline_tag': 'sentence-similarity',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['sentence-transformers',\n",
       "    'feature-extraction',\n",
       "    'sentence-similarity',\n",
       "    'transformers']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ahotrod/albert_xxlargev1_squad2_512': {'modelId': 'ahotrod/albert_xxlargev1_squad2_512',\n",
       "  'sha': '291f0fa26d2c80d8a473b6116164a083d252b4fe',\n",
       "  'lastModified': '2020-12-11T21:31:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'albert',\n",
       "   'question-answering',\n",
       "   'transformers',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'question-answering',\n",
       "  'private': False,\n",
       "  'author': 'ahotrod',\n",
       "  'config': {'architectures': ['AlbertForQuestionAnswering'],\n",
       "   'model_type': 'albert'},\n",
       "  'id': 'ahotrod/albert_xxlargev1_squad2_512',\n",
       "  'downloads': 3740,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'Where do I live?',\n",
       "    'context': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'Where do I live?',\n",
       "    'context': 'My name is Sarah and I live in London'},\n",
       "   {'text': \"What's my name?\",\n",
       "    'context': 'My name is Clara and I live in Berkeley.'},\n",
       "   {'text': 'Which name is also used to describe the Amazon rainforest in English?',\n",
       "    'context': 'The Amazon rainforest (Portuguese: Floresta Amazônica or Amazônia; Spanish: Selva Amazónica, Amazonía or usually Amazonia; French: Forêt amazonienne; Dutch: Amazoneregenwoud), also known in English as Amazonia or the Amazon Jungle, is a moist broadleaf forest that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 square kilometres (2,700,000 sq mi), of which 5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest. This region includes territory belonging to nine nations. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, Colombia with 10%, and with minor amounts in Venezuela, Ecuador, Bolivia, Guyana, Suriname and French Guiana. States or departments in four nations contain \"Amazonas\" in their names. The Amazon represents over half of the planet\\'s remaining rainforests, and comprises the largest and most biodiverse tract of tropical rainforest in the world, with an estimated 390 billion individual trees divided into 16,000 species.'}],\n",
       "  'likes': 2,\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForQuestionAnswering',\n",
       "   'pipeline_tag': 'question-answering',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'nboost/pt-tinybert-msmarco': {'modelId': 'nboost/pt-tinybert-msmarco',\n",
       "  'sha': 'b0de5c59e4779c149295b9bd0e5988a8f2cd2be7',\n",
       "  'lastModified': '2021-05-20T01:28:00.000Z',\n",
       "  'tags': ['pytorch', 'jax', 'bert', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'nboost',\n",
       "  'config': {'model_type': 'bert'},\n",
       "  'id': 'nboost/pt-tinybert-msmarco',\n",
       "  'downloads': 3724,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment': {'modelId': 'CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment',\n",
       "  'sha': 'b0df630d23e5aa5f8a326017605337f6d0863ecd',\n",
       "  'lastModified': '2021-10-17T11:15:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'ar',\n",
       "   'arxiv:2103.06678',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'CAMeL-Lab',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'CAMeL-Lab/bert-base-arabic-camelbert-da-sentiment',\n",
       "  'downloads': 3718,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'أنا بخير'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['ar'],\n",
       "   'license': 'apache-2.0',\n",
       "   'widget': [{'text': 'أنا بخير'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'microsoft/deberta-base-mnli': {'modelId': 'microsoft/deberta-base-mnli',\n",
       "  'sha': 'a80a6eb013898011540b19bf1f64e21eb61e53d6',\n",
       "  'lastModified': '2021-12-09T13:36:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'rust',\n",
       "   'deberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'arxiv:2006.03654',\n",
       "   'transformers',\n",
       "   'deberta-v1',\n",
       "   'deberta-mnli',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'microsoft',\n",
       "  'config': {'architectures': ['DebertaForSequenceClassification'],\n",
       "   'model_type': 'deberta'},\n",
       "  'id': 'microsoft/deberta-base-mnli',\n",
       "  'downloads': 3712,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': '[CLS] I love you. [SEP] I like you. [SEP]'}],\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'tags': ['deberta-v1', 'deberta-mnli'],\n",
       "   'tasks': 'mnli',\n",
       "   'thumbnail': 'https://huggingface.co/front/thumbnails/microsoft.png',\n",
       "   'license': 'mit',\n",
       "   'widget': [{'text': '[CLS] I love you. [SEP] I like you. [SEP]'}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'TransQuest/monotransquest-da-multilingual': {'modelId': 'TransQuest/monotransquest-da-multilingual',\n",
       "  'sha': 'cd947f301588992a749d22fc867e535bc9cb1703',\n",
       "  'lastModified': '2021-06-03T19:06:25.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xlm-roberta',\n",
       "   'text-classification',\n",
       "   'multilingual-multilingual',\n",
       "   'transformers',\n",
       "   'Quality Estimation',\n",
       "   'monotransquest',\n",
       "   'DA',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'TransQuest',\n",
       "  'config': {'architectures': ['XLMRobertaForSequenceClassification'],\n",
       "   'model_type': 'xlm-roberta'},\n",
       "  'id': 'TransQuest/monotransquest-da-multilingual',\n",
       "  'downloads': 3696,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'multilingual-multilingual',\n",
       "   'tags': ['Quality Estimation', 'monotransquest', 'DA'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'alvaroalon2/biobert_diseases_ner': {'modelId': 'alvaroalon2/biobert_diseases_ner',\n",
       "  'sha': 'ce0fd86ac9e145d1a6ca3455219843e0a855471f',\n",
       "  'lastModified': '2021-07-07T12:35:55.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'bert',\n",
       "   'token-classification',\n",
       "   'English',\n",
       "   'dataset:BC5CDR-diseases',\n",
       "   'dataset:ncbi_disease',\n",
       "   'transformers',\n",
       "   'NER',\n",
       "   'Biomedical',\n",
       "   'Diseases',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'alvaroalon2',\n",
       "  'config': {'architectures': ['BertForTokenClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'alvaroalon2/biobert_diseases_ner',\n",
       "  'downloads': 3696,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'likes': 6,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'English',\n",
       "   'license': 'apache-2.0',\n",
       "   'tags': ['token-classification', 'NER', 'Biomedical', 'Diseases'],\n",
       "   'datasets': ['BC5CDR-diseases', 'ncbi_disease']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cross-encoder/nli-roberta-base': {'modelId': 'cross-encoder/nli-roberta-base',\n",
       "  'sha': '1c9dadfb1d7bcaac49176fd3a5de914f6ae2bd42',\n",
       "  'lastModified': '2021-08-05T08:41:05.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'dataset:multi_nli',\n",
       "   'dataset:snli',\n",
       "   'transformers',\n",
       "   'roberta-base',\n",
       "   'license:apache-2.0',\n",
       "   'zero-shot-classification'],\n",
       "  'pipeline_tag': 'zero-shot-classification',\n",
       "  'private': False,\n",
       "  'author': 'cross-encoder',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cross-encoder/nli-roberta-base',\n",
       "  'downloads': 3686,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       "    'candidate_labels': 'urgent, not urgent, phone, tablet, computer',\n",
       "    'multi_class': True},\n",
       "   {'text': 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n",
       "    'candidate_labels': 'mobile, website, billing, account access',\n",
       "    'multi_class': False},\n",
       "   {'text': 'A new model offers an explanation for how the Galilean satellites formed around the solar system’s largest world. Konstantin Batygin did not set out to solve one of the solar system’s most puzzling mysteries when he went for a run up a hill in Nice, France. Dr. Batygin, a Caltech researcher, best known for his contributions to the search for the solar system’s missing “Planet Nine,” spotted a beer bottle. At a steep, 20 degree grade, he wondered why it wasn’t rolling down the hill. He realized there was a breeze at his back holding the bottle in place. Then he had a thought that would only pop into the mind of a theoretical astrophysicist: “Oh! This is how Europa formed.” Europa is one of Jupiter’s four large Galilean moons. And in a paper published Monday in the Astrophysical Journal, Dr. Batygin and a co-author, Alessandro Morbidelli, a planetary scientist at the Côte d’Azur Observatory in France, present a theory explaining how some moons form around gas giants like Jupiter and Saturn, suggesting that millimeter-sized grains of hail produced during the solar system’s formation became trapped around these massive worlds, taking shape one at a time into the potentially habitable moons we know today.',\n",
       "    'candidate_labels': 'space & cosmos, scientific discovery, microbiology, robots, archeology',\n",
       "    'multi_class': True}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'pipeline_tag': 'zero-shot-classification',\n",
       "   'tags': ['roberta-base'],\n",
       "   'datasets': ['multi_nli', 'snli'],\n",
       "   'metrics': ['accuracy'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'studio-ousia/luke-large': {'modelId': 'studio-ousia/luke-large',\n",
       "  'sha': '0729d044dfe301d9ecabc222d60633f92ac450eb',\n",
       "  'lastModified': '2022-04-13T09:06:10.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'luke',\n",
       "   'fill-mask',\n",
       "   'en',\n",
       "   'arxiv:1906.08237',\n",
       "   'arxiv:1903.07785',\n",
       "   'arxiv:2002.01808',\n",
       "   'transformers',\n",
       "   'named entity recognition',\n",
       "   'entity typing',\n",
       "   'relation classification',\n",
       "   'question answering',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'studio-ousia',\n",
       "  'config': {'architectures': ['LukeForMaskedLM'], 'model_type': 'luke'},\n",
       "  'id': 'studio-ousia/luke-large',\n",
       "  'downloads': 3686,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'Paris is the <mask> of France.'},\n",
       "   {'text': 'The goal of life is <mask>.'}],\n",
       "  'likes': 0,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'en',\n",
       "   'thumbnail': 'https://github.com/studio-ousia/luke/raw/master/resources/luke_logo.png',\n",
       "   'tags': ['luke',\n",
       "    'named entity recognition',\n",
       "    'entity typing',\n",
       "    'relation classification',\n",
       "    'question answering'],\n",
       "   'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'osanseviero/dalle-mini-fork': {'modelId': 'osanseviero/dalle-mini-fork',\n",
       "  'sha': '18bdfd0e6c1c68a01666e9517d67875be0180dd0',\n",
       "  'lastModified': '2021-08-17T10:30:17.000Z',\n",
       "  'tags': ['jax',\n",
       "   'bart',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'generic',\n",
       "   'text-to-image'],\n",
       "  'pipeline_tag': 'text-to-image',\n",
       "  'private': False,\n",
       "  'author': 'osanseviero',\n",
       "  'config': {'architectures': ['omFlaxBartForConditionalGeneration'],\n",
       "   'model_type': 'bart',\n",
       "   'task_specific_params': {'summarization': {'early_stopping': True,\n",
       "     'length_penalty': 2,\n",
       "     'max_length': 142,\n",
       "     'min_length': 56,\n",
       "     'no_repeat_ngram_size': 3,\n",
       "     'num_beams': 4}}},\n",
       "  'id': 'osanseviero/dalle-mini-fork',\n",
       "  'downloads': 3681,\n",
       "  'library_name': 'generic',\n",
       "  'mask_token': '<mask>',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'library_name': 'generic',\n",
       "   'language': ['en'],\n",
       "   'pipeline_tag': 'text-to-image'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'HooshvareLab/bert-fa-zwnj-base': {'modelId': 'HooshvareLab/bert-fa-zwnj-base',\n",
       "  'sha': '3880fac085e1a338e9564907cba0adeb9e14bc72',\n",
       "  'lastModified': '2021-05-18T21:05:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'bert',\n",
       "   'fill-mask',\n",
       "   'fa',\n",
       "   'arxiv:2005.12515',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'fill-mask',\n",
       "  'private': False,\n",
       "  'author': 'HooshvareLab',\n",
       "  'config': {'architectures': ['BertForMaskedLM'], 'model_type': 'bert'},\n",
       "  'id': 'HooshvareLab/bert-fa-zwnj-base',\n",
       "  'downloads': 3672,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'زندگی یک سوال است و این که چگونه [MASK] کنیم پاسخ این سوال!'},\n",
       "   {'text': 'زندگی از مرگ پرسید: چرا همه من را [MASK] دارند اما از تو متنفرند؟'}],\n",
       "  'likes': 3,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': 'fa', 'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForMaskedLM',\n",
       "   'pipeline_tag': 'fill-mask',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'cardiffnlp/twitter-roberta-base-stance-climate': {'modelId': 'cardiffnlp/twitter-roberta-base-stance-climate',\n",
       "  'sha': 'a2802f328ed851a97696c724f46e75773c34e098',\n",
       "  'lastModified': '2021-05-20T15:10:09.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tf',\n",
       "   'jax',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'cardiffnlp',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'cardiffnlp/twitter-roberta-base-stance-climate',\n",
       "  'downloads': 3671,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/bert_ft_qqp-0': {'modelId': 'Jeevesh8/bert_ft_qqp-0',\n",
       "  'sha': '17645bf0b6f171d517ac3e9a13f50eb1908b5b4d',\n",
       "  'lastModified': '2022-05-07T12:10:57.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/bert_ft_qqp-0',\n",
       "  'downloads': 3665,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'finiteautomata/bertweet-base-emotion-analysis': {'modelId': 'finiteautomata/bertweet-base-emotion-analysis',\n",
       "  'sha': '64046df9cc41eab40e1ecde7d2b7fb42b971be5b',\n",
       "  'lastModified': '2021-12-10T13:28:56.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'roberta',\n",
       "   'text-classification',\n",
       "   'en',\n",
       "   'arxiv:2106.09462',\n",
       "   'transformers',\n",
       "   'emotion-analysis'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'finiteautomata',\n",
       "  'config': {'architectures': ['RobertaForSequenceClassification'],\n",
       "   'model_type': 'roberta'},\n",
       "  'id': 'finiteautomata/bertweet-base-emotion-analysis',\n",
       "  'downloads': 3660,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '<mask>',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'likes': 4,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en'], 'tags': ['emotion-analysis']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'ckiplab/albert-base-chinese-ner': {'modelId': 'ckiplab/albert-base-chinese-ner',\n",
       "  'sha': 'ef1b19225fc53f10fafd8bfdefb41ac2f2f4e177',\n",
       "  'lastModified': '2022-05-10T03:28:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'albert',\n",
       "   'token-classification',\n",
       "   'zh',\n",
       "   'transformers',\n",
       "   'license:gpl-3.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'ckiplab',\n",
       "  'config': {'architectures': ['AlbertForTokenClassification'],\n",
       "   'model_type': 'albert'},\n",
       "  'id': 'ckiplab/albert-base-chinese-ner',\n",
       "  'downloads': 3638,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': '我叫沃尔夫冈，我住在柏林。'},\n",
       "   {'text': '我叫萨拉，我住在伦敦。'},\n",
       "   {'text': '我叫克拉拉，我住在加州伯克利。'}],\n",
       "  'likes': 0,\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['zh'],\n",
       "   'thumbnail': 'https://ckip.iis.sinica.edu.tw/files/ckip_logo.png',\n",
       "   'tags': ['pytorch', 'token-classification', 'albert', 'zh'],\n",
       "   'license': 'gpl-3.0'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'elastic/distilbert-base-cased-finetuned-conll03-english': {'modelId': 'elastic/distilbert-base-cased-finetuned-conll03-english',\n",
       "  'sha': '3043a315aae69b6e2f88056b23100e144791ac99',\n",
       "  'lastModified': '2022-06-24T09:30:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'distilbert',\n",
       "   'token-classification',\n",
       "   'en',\n",
       "   'dataset:conll2003',\n",
       "   'transformers',\n",
       "   'license:apache-2.0',\n",
       "   'model-index',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'token-classification',\n",
       "  'private': False,\n",
       "  'author': 'elastic',\n",
       "  'config': {'architectures': ['DistilBertForTokenClassification'],\n",
       "   'model_type': 'distilbert'},\n",
       "  'id': 'elastic/distilbert-base-cased-finetuned-conll03-english',\n",
       "  'downloads': 3620,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'My name is Wolfgang and I live in Berlin'},\n",
       "   {'text': 'My name is Sarah and I live in London'},\n",
       "   {'text': 'My name is Clara and I live in Berkeley, California.'}],\n",
       "  'likes': 5,\n",
       "  'model-index': [{'name': 'elastic/distilbert-base-cased-finetuned-conll03-english',\n",
       "    'results': [{'task': {'type': 'token-classification',\n",
       "       'name': 'Token Classification'},\n",
       "      'dataset': {'name': 'conll2003',\n",
       "       'type': 'conll2003',\n",
       "       'config': 'conll2003',\n",
       "       'split': 'validation'},\n",
       "      'metrics': [{'name': 'Accuracy',\n",
       "        'type': 'accuracy',\n",
       "        'value': 0.9834432212868665,\n",
       "        'verified': True},\n",
       "       {'name': 'Precision',\n",
       "        'type': 'precision',\n",
       "        'value': 0.9857564461012737,\n",
       "        'verified': True},\n",
       "       {'name': 'Recall',\n",
       "        'type': 'recall',\n",
       "        'value': 0.9882123948925569,\n",
       "        'verified': True},\n",
       "       {'name': 'F1',\n",
       "        'type': 'f1',\n",
       "        'value': 0.9869828926905132,\n",
       "        'verified': True},\n",
       "       {'name': 'loss',\n",
       "        'type': 'loss',\n",
       "        'value': 0.07748260349035263,\n",
       "        'verified': True}]}]}],\n",
       "  'cardData': {'language': 'en',\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['conll2003'],\n",
       "   'model-index': [{'name': 'elastic/distilbert-base-cased-finetuned-conll03-english',\n",
       "     'results': [{'task': {'type': 'token-classification',\n",
       "        'name': 'Token Classification'},\n",
       "       'dataset': {'name': 'conll2003',\n",
       "        'type': 'conll2003',\n",
       "        'config': 'conll2003',\n",
       "        'split': 'validation'},\n",
       "       'metrics': [{'name': 'Accuracy',\n",
       "         'type': 'accuracy',\n",
       "         'value': 0.9834432212868665,\n",
       "         'verified': True},\n",
       "        {'name': 'Precision',\n",
       "         'type': 'precision',\n",
       "         'value': 0.9857564461012737,\n",
       "         'verified': True},\n",
       "        {'name': 'Recall',\n",
       "         'type': 'recall',\n",
       "         'value': 0.9882123948925569,\n",
       "         'verified': True},\n",
       "        {'name': 'F1',\n",
       "         'type': 'f1',\n",
       "         'value': 0.9869828926905132,\n",
       "         'verified': True},\n",
       "        {'name': 'loss',\n",
       "         'type': 'loss',\n",
       "         'value': 0.07748260349035263,\n",
       "         'verified': True}]}]}]},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForTokenClassification',\n",
       "   'pipeline_tag': 'token-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'stas/tiny-wmt19-en-de': {'modelId': 'stas/tiny-wmt19-en-de',\n",
       "  'sha': '18ca8fc156edb91968dd4d70e33fbe5989d04368',\n",
       "  'lastModified': '2021-05-03T01:48:44.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'fsmt',\n",
       "   'text2text-generation',\n",
       "   'en',\n",
       "   'de',\n",
       "   'dataset:wmt19',\n",
       "   'transformers',\n",
       "   'wmt19',\n",
       "   'testing',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible'],\n",
       "  'pipeline_tag': 'text2text-generation',\n",
       "  'private': False,\n",
       "  'author': 'stas',\n",
       "  'config': {'architectures': ['FSMTForConditionalGeneration'],\n",
       "   'model_type': 'fsmt'},\n",
       "  'id': 'stas/tiny-wmt19-en-de',\n",
       "  'downloads': 3620,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'language': ['en', 'de'],\n",
       "   'thumbnail': None,\n",
       "   'tags': ['wmt19', 'testing'],\n",
       "   'license': 'apache-2.0',\n",
       "   'datasets': ['wmt19'],\n",
       "   'metrics': ['bleu']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSeq2SeqLM',\n",
       "   'pipeline_tag': 'text2text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/bert-small-mm_retrieval-passage_encoder': {'modelId': 'deepset/bert-small-mm_retrieval-passage_encoder',\n",
       "  'sha': 'c764744512975bd3823f689601ab0e388a29c366',\n",
       "  'lastModified': '2021-10-19T16:14:29.000Z',\n",
       "  'tags': ['pytorch', 'dpr', 'transformers'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['DPRContextEncoder'], 'model_type': 'dpr'},\n",
       "  'id': 'deepset/bert-small-mm_retrieval-passage_encoder',\n",
       "  'downloads': 3618,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'DPRContextEncoder',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'deepset/bert-small-mm_retrieval-question_encoder': {'modelId': 'deepset/bert-small-mm_retrieval-question_encoder',\n",
       "  'sha': 'a34edf571667cc1ba38cec55c56f2905f13336a2',\n",
       "  'lastModified': '2021-10-19T15:51:37.000Z',\n",
       "  'tags': ['pytorch', 'dpr', 'feature-extraction', 'transformers'],\n",
       "  'pipeline_tag': 'feature-extraction',\n",
       "  'private': False,\n",
       "  'author': 'deepset',\n",
       "  'config': {'architectures': ['DPRQuestionEncoder'], 'model_type': 'dpr'},\n",
       "  'id': 'deepset/bert-small-mm_retrieval-question_encoder',\n",
       "  'downloads': 3618,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModel',\n",
       "   'pipeline_tag': 'feature-extraction',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'facebook/incoder-1B': {'modelId': 'facebook/incoder-1B',\n",
       "  'sha': '01f46041ac45a5a1ac9a60875189c15209d2fee9',\n",
       "  'lastModified': '2022-05-31T16:56:08.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'xglm',\n",
       "   'text-generation',\n",
       "   'arxiv:2204.05999',\n",
       "   'transformers',\n",
       "   'code',\n",
       "   'python',\n",
       "   'javascript',\n",
       "   'license:cc-by-nc-4.0'],\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'private': False,\n",
       "  'author': 'facebook',\n",
       "  'config': {'architectures': ['XGLMForCausalLM'], 'model_type': 'xglm'},\n",
       "  'id': 'facebook/incoder-1B',\n",
       "  'downloads': 3587,\n",
       "  'library_name': 'transformers',\n",
       "  'widgetData': [{'text': 'My name is Julien and I like to'},\n",
       "   {'text': 'My name is Thomas and my main'},\n",
       "   {'text': 'My name is Mariama, my favorite'},\n",
       "   {'text': 'My name is Clara and I am'},\n",
       "   {'text': 'My name is Lewis and I like to'},\n",
       "   {'text': 'My name is Merve and my favorite'},\n",
       "   {'text': 'My name is Teven and I am'},\n",
       "   {'text': 'Once upon a time,'}],\n",
       "  'likes': 12,\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'cc-by-nc-4.0',\n",
       "   'tags': ['code', 'python', 'javascript']},\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForCausalLM',\n",
       "   'pipeline_tag': 'text-generation',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'pyannote/speaker-segmentation': {'modelId': 'pyannote/speaker-segmentation',\n",
       "  'sha': '3e45863bb592c3fa20db5fd25c1d297acd1ce9d0',\n",
       "  'lastModified': '2022-03-23T09:23:09.000Z',\n",
       "  'tags': ['dataset:ami',\n",
       "   'dataset:dihard',\n",
       "   'dataset:voxconverse',\n",
       "   'pyannote-audio',\n",
       "   'pyannote',\n",
       "   'pyannote-audio-pipeline',\n",
       "   'audio',\n",
       "   'voice',\n",
       "   'speech',\n",
       "   'speaker',\n",
       "   'speaker-segmentation',\n",
       "   'speaker-diarization',\n",
       "   'speaker-change-detection',\n",
       "   'voice-activity-detection',\n",
       "   'overlapped-speech-detection',\n",
       "   'automatic-speech-recognition',\n",
       "   'license:mit'],\n",
       "  'pipeline_tag': 'automatic-speech-recognition',\n",
       "  'private': False,\n",
       "  'author': 'pyannote',\n",
       "  'config': None,\n",
       "  'id': 'pyannote/speaker-segmentation',\n",
       "  'downloads': 3575,\n",
       "  'library_name': 'pyannote-audio',\n",
       "  'likes': 1,\n",
       "  'model-index': None,\n",
       "  'cardData': {'tags': ['pyannote',\n",
       "    'pyannote-audio',\n",
       "    'pyannote-audio-pipeline',\n",
       "    'audio',\n",
       "    'voice',\n",
       "    'speech',\n",
       "    'speaker',\n",
       "    'speaker-segmentation',\n",
       "    'speaker-diarization',\n",
       "    'speaker-change-detection',\n",
       "    'voice-activity-detection',\n",
       "    'overlapped-speech-detection',\n",
       "    'automatic-speech-recognition'],\n",
       "   'datasets': ['ami', 'dihard', 'voxconverse'],\n",
       "   'license': 'mit'},\n",
       "  'transformersInfo': {'auto_model': 'AutoModel'}},\n",
       " 'dandelin/vilt-b32-finetuned-coco': {'modelId': 'dandelin/vilt-b32-finetuned-coco',\n",
       "  'sha': '2f3f7f3f62a3f4f429c26309256485bbd9b0e40a',\n",
       "  'lastModified': '2022-01-23T09:45:24.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'vilt',\n",
       "   'arxiv:2102.03334',\n",
       "   'transformers',\n",
       "   'license:apache-2.0'],\n",
       "  'pipeline_tag': None,\n",
       "  'private': False,\n",
       "  'author': 'dandelin',\n",
       "  'config': {'architectures': ['ViltForImageAndTextRetrieval'],\n",
       "   'model_type': 'vilt'},\n",
       "  'id': 'dandelin/vilt-b32-finetuned-coco',\n",
       "  'downloads': 3565,\n",
       "  'library_name': 'transformers',\n",
       "  'model-index': None,\n",
       "  'cardData': {'license': 'apache-2.0'},\n",
       "  'transformersInfo': {'auto_model': 'ViltForImageAndTextRetrieval',\n",
       "   'processor': 'AutoProcessor'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-44': {'modelId': 'Jeevesh8/init_bert_ft_qqp-44',\n",
       "  'sha': 'e91e1d668d5362aab64271c3c2ff620674b9cecf',\n",
       "  'lastModified': '2022-06-02T12:39:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-44',\n",
       "  'downloads': 3563,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-48': {'modelId': 'Jeevesh8/init_bert_ft_qqp-48',\n",
       "  'sha': 'e8b3d062302ce6d91c785b14b848974ecb336fab',\n",
       "  'lastModified': '2022-06-02T12:39:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-48',\n",
       "  'downloads': 3563,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-52': {'modelId': 'Jeevesh8/init_bert_ft_qqp-52',\n",
       "  'sha': '038c33de491e995aa97d15abd229aa8bc1177e63',\n",
       "  'lastModified': '2022-06-02T12:39:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-52',\n",
       "  'downloads': 3562,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-40': {'modelId': 'Jeevesh8/init_bert_ft_qqp-40',\n",
       "  'sha': '9d032ef7b58b123d2763e8789ec1f424a804d181',\n",
       "  'lastModified': '2022-06-02T12:39:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-40',\n",
       "  'downloads': 3561,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-1': {'modelId': 'Jeevesh8/init_bert_ft_qqp-1',\n",
       "  'sha': 'f7a2fb5ce719ac903ffd3b5b500262046abb1319',\n",
       "  'lastModified': '2022-06-02T12:37:58.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-1',\n",
       "  'downloads': 3560,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-0': {'modelId': 'Jeevesh8/init_bert_ft_qqp-0',\n",
       "  'sha': '347f6a3eeb6992f6d9cc1647834c293e9934248c',\n",
       "  'lastModified': '2022-06-02T12:37:36.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-0',\n",
       "  'downloads': 3560,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-58': {'modelId': 'Jeevesh8/init_bert_ft_qqp-58',\n",
       "  'sha': 'adb129bcb03fe426a47c15db5171aac20fa634f8',\n",
       "  'lastModified': '2022-06-02T12:39:36.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-58',\n",
       "  'downloads': 3560,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-51': {'modelId': 'Jeevesh8/init_bert_ft_qqp-51',\n",
       "  'sha': '5336d616552fb1360f6e1ef4421fad176c787a95',\n",
       "  'lastModified': '2022-06-02T12:39:27.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-51',\n",
       "  'downloads': 3560,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-22': {'modelId': 'Jeevesh8/init_bert_ft_qqp-22',\n",
       "  'sha': 'cad4666abbe0793ea90ad26a0a72c38ea5678ad9',\n",
       "  'lastModified': '2022-06-02T12:40:51.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-22',\n",
       "  'downloads': 3559,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-25': {'modelId': 'Jeevesh8/init_bert_ft_qqp-25',\n",
       "  'sha': '411d83e71e6ab008f1a37692975ac1fd9477b2b6',\n",
       "  'lastModified': '2022-06-02T12:39:39.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-25',\n",
       "  'downloads': 3559,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-53': {'modelId': 'Jeevesh8/init_bert_ft_qqp-53',\n",
       "  'sha': '7f3a8e70c3a817a9793e402af7a6fa4cdc6d1cf4',\n",
       "  'lastModified': '2022-06-02T12:39:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-53',\n",
       "  'downloads': 3559,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-56': {'modelId': 'Jeevesh8/init_bert_ft_qqp-56',\n",
       "  'sha': '5d6a7eb9d10e20374bfc95d431e63ef6d99965c1',\n",
       "  'lastModified': '2022-06-02T12:39:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-56',\n",
       "  'downloads': 3559,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/bert_ft_qqp-99': {'modelId': 'Jeevesh8/bert_ft_qqp-99',\n",
       "  'sha': 'e429b7cd0c1dd70376f8dc03dfa8b773fda50395',\n",
       "  'lastModified': '2022-05-09T13:43:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/bert_ft_qqp-99',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-6': {'modelId': 'Jeevesh8/init_bert_ft_qqp-6',\n",
       "  'sha': '0c42c612266c732b13e08c8829450408fef0324a',\n",
       "  'lastModified': '2022-06-02T12:37:38.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-6',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-16': {'modelId': 'Jeevesh8/init_bert_ft_qqp-16',\n",
       "  'sha': 'dd2887d6ba950e43507ef5733e649a19c8ef8e36',\n",
       "  'lastModified': '2022-06-02T12:41:45.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-16',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-15': {'modelId': 'Jeevesh8/init_bert_ft_qqp-15',\n",
       "  'sha': 'eef52e925b9ff308fc3724a434031a7173eb5ccf',\n",
       "  'lastModified': '2022-06-02T12:41:48.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-15',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-14': {'modelId': 'Jeevesh8/init_bert_ft_qqp-14',\n",
       "  'sha': '8a10dcb1ef0483531a42defd28531e94f2480e34',\n",
       "  'lastModified': '2022-06-02T12:39:12.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-14',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-21': {'modelId': 'Jeevesh8/init_bert_ft_qqp-21',\n",
       "  'sha': '5de6a21340b75619113f3242c83d775732609067',\n",
       "  'lastModified': '2022-06-02T12:39:52.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-21',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-17': {'modelId': 'Jeevesh8/init_bert_ft_qqp-17',\n",
       "  'sha': 'a4ac389a3423dba0036456b1051d98542e97af54',\n",
       "  'lastModified': '2022-06-02T12:40:00.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-17',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-27': {'modelId': 'Jeevesh8/init_bert_ft_qqp-27',\n",
       "  'sha': '62a06d82e4a48fd86ee4a565a1511b7c7e2126c8',\n",
       "  'lastModified': '2022-06-02T12:39:37.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-27',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-59': {'modelId': 'Jeevesh8/init_bert_ft_qqp-59',\n",
       "  'sha': '96d09ede67b2103851c3990670febdba6ff1ba80',\n",
       "  'lastModified': '2022-06-02T12:39:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-59',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-60': {'modelId': 'Jeevesh8/init_bert_ft_qqp-60',\n",
       "  'sha': '60010aa746dbfd973a9c10ca03c27d0639783804',\n",
       "  'lastModified': '2022-06-02T12:39:35.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-60',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-57': {'modelId': 'Jeevesh8/init_bert_ft_qqp-57',\n",
       "  'sha': '0efe5b17b3a8f1795dca2ead725c1e9f9d1cee36',\n",
       "  'lastModified': '2022-06-02T12:39:34.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-57',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-47': {'modelId': 'Jeevesh8/init_bert_ft_qqp-47',\n",
       "  'sha': '8513a6f7da0321943894907cf82c223ebbfa8041',\n",
       "  'lastModified': '2022-06-02T12:39:28.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-47',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-55': {'modelId': 'Jeevesh8/init_bert_ft_qqp-55',\n",
       "  'sha': '01eed3cc7e735fa2f9accc6d28c3092ed3c50222',\n",
       "  'lastModified': '2022-06-02T12:41:40.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-55',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-37': {'modelId': 'Jeevesh8/init_bert_ft_qqp-37',\n",
       "  'sha': 'cd796932624b2969733f3319b77c85bfb50fc808',\n",
       "  'lastModified': '2022-06-02T12:39:59.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-37',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-36': {'modelId': 'Jeevesh8/init_bert_ft_qqp-36',\n",
       "  'sha': '490a3538c45e526812d876156703ea6680d71a66',\n",
       "  'lastModified': '2022-06-02T12:40:02.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-36',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-69': {'modelId': 'Jeevesh8/init_bert_ft_qqp-69',\n",
       "  'sha': '11b27b2e536206739dc19c848cd2d1daf4869c22',\n",
       "  'lastModified': '2022-06-02T12:40:32.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-69',\n",
       "  'downloads': 3558,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/bert_ft_qqp-92': {'modelId': 'Jeevesh8/bert_ft_qqp-92',\n",
       "  'sha': '4660394cec78461b551822615881290dc63f45a0',\n",
       "  'lastModified': '2022-05-09T13:25:31.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/bert_ft_qqp-92',\n",
       "  'downloads': 3557,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-3': {'modelId': 'Jeevesh8/init_bert_ft_qqp-3',\n",
       "  'sha': '9d948e08358914a74874f4eb5cf3c57a71e94045',\n",
       "  'lastModified': '2022-06-02T12:37:47.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-3',\n",
       "  'downloads': 3557,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-4': {'modelId': 'Jeevesh8/init_bert_ft_qqp-4',\n",
       "  'sha': 'c94e68de0e093b7aee98c4bc134f6eb76a0c6504',\n",
       "  'lastModified': '2022-06-02T12:37:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-4',\n",
       "  'downloads': 3557,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-5': {'modelId': 'Jeevesh8/init_bert_ft_qqp-5',\n",
       "  'sha': '578cc4e2c37914baa3ac728fc4234ff1c129ec47',\n",
       "  'lastModified': '2022-06-02T12:37:42.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-5',\n",
       "  'downloads': 3557,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-7': {'modelId': 'Jeevesh8/init_bert_ft_qqp-7',\n",
       "  'sha': '933e4d15c36040fd937caeea8723820bbaeca1ab',\n",
       "  'lastModified': '2022-06-02T12:38:15.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-7',\n",
       "  'downloads': 3557,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-10': {'modelId': 'Jeevesh8/init_bert_ft_qqp-10',\n",
       "  'sha': '85dcd3d2c38a5e71b605474e285b9bbb1b121c9d',\n",
       "  'lastModified': '2022-06-02T12:37:54.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-10',\n",
       "  'downloads': 3557,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " 'Jeevesh8/init_bert_ft_qqp-9': {'modelId': 'Jeevesh8/init_bert_ft_qqp-9',\n",
       "  'sha': '3738e5bf4ff0956b53bb322dda7014c693ecdd8a',\n",
       "  'lastModified': '2022-06-02T12:37:46.000Z',\n",
       "  'tags': ['pytorch',\n",
       "   'tensorboard',\n",
       "   'bert',\n",
       "   'text-classification',\n",
       "   'transformers'],\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'private': False,\n",
       "  'author': 'Jeevesh8',\n",
       "  'config': {'architectures': ['BertForSequenceClassification'],\n",
       "   'model_type': 'bert'},\n",
       "  'id': 'Jeevesh8/init_bert_ft_qqp-9',\n",
       "  'downloads': 3557,\n",
       "  'library_name': 'transformers',\n",
       "  'mask_token': '[MASK]',\n",
       "  'widgetData': [{'text': 'I like you. I love you'}],\n",
       "  'model-index': None,\n",
       "  'transformersInfo': {'auto_model': 'AutoModelForSequenceClassification',\n",
       "   'pipeline_tag': 'text-classification',\n",
       "   'processor': 'AutoTokenizer'}},\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"modelinfo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m {m[\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m]: m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m data}\n",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m {m[\u001b[39m\"\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m\"\u001b[39;49m]: m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m data}\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "data = {m[\"id\"]: m for m in data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ICSE-AE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f978456172793b1e5202b1e32483855cb2f0db51d84bae17525fe5e1390177d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
